[
  {
    "objectID": "posts/BPProject/BPProject.html",
    "href": "posts/BPProject/BPProject.html",
    "title": "Project Blog Post: Skin Cancer Classification with LogisticRegression and CNN",
    "section": "",
    "text": "Project Blog Post: Skin Cancer Classification with LogisticRegression and CNN\n\nAbstract\nSkin cancer is the most common form of cancer. According to the Victoria department of health, over 95% of skin cancers that are dected early can be successfully treated. This means that early detection is a crucial step in the treatment process of skin cancer. In our analyses, we aim to identify different types of skin conditions with image classification techniques. Using convolutional neural networks (CNN) and logistic regression (LR) models, we obtain a 75% accuracy rate for classification, where melanocytic nevi (NV) and melanoma (MEL) are the most successfully classified forms of skin condition.\nLink to GitHub Repository: https://github.com/ShadowedSpace/cancer\n\n\nIntroduction\nIn 1994, Binder et al. trained a neural network that successfully differentiated between melanomas, the leading cause of skin cancer deaths, and melanocytic nevus, which are generally harmless skin lesions commonly known as moles or birth marks. In 2018, a challenge hosted by the International Skin Imaging Collaboration (ISIC) lay out the task of detection and classification of skin lesions and diseases. 900 parties registered to download the data for the challenge and hundreds of groups submitted novel evaluation techniques to automate the process of diagnosing skin lesions and diseases. A study conducted on the ISIC challenge revealed that the best classification models still failed to properly classify on average over 10% of dermoscopic images and had varying abilities to generalize (Coedlla et al., 2019). As it turns out, Binder et al.’s study (1994) trained their neural network on 200 images. The dataset used by the ISIC challenge is the world’s largest public repository of dermoscopic images of skin and contains 10015 dermoscopic images. First released in 2018 by Philipp Tschandl et al., the HAM10000 (Humans Against Machines) is a novel dataset with the aim of improving the process of automated skin lesion diagnosis, as most existing dermoscopic image datasets are either small in size or lacking diversity.\nAccording to a recent study by Tandon et al. (2024), the most successful deep learning model to implement for the automation of cancer diagnoses is the convolutional neural network (CNN). CNNs are a type of neural network that are particularly effective for identifying patterns in images and audio. CNNs are best used when there are large amounts of data to train the model on (MathWorks). Before the release of the HAM10000 dataset, the diversity and size of dermascopic images was limited, which also limited the progression of automated skin lesion detection (Tschandl et al. 2024). Binder et al.(1994) creates a model from a much smaller sample size and is limited in the diversity of skin lesions it can detect. As revealed by the 2018 ISIC challenge, automated healthcare diagnoses are still limited in accuracy and generalizability.\nIn our project, we apply our knowledge of Convolutional Neural Networks (CNN) and other machine learning techniques to the HAM10000 dataset with the goal of classifying types of skin lesions. Accurate and early skin lesion diagnosis is crucial to a successful recovery when it comes to skin cancer. Melanoma, a type of skin cancer, is quick to spread to other organs, and is the leading cause of deaths due to skin cancer. But when detected early enough, before the melanoma has a chance to spread, melanoma has a five-year survival rate of 98%. Precancerous or skin cancers spots can be spotted by the naked eye. According to the Fred Hutch Center, the ABCDE Guice is a way to visually detect abnormalities in skin lesions:\nA for asymmetry- early melanomas are asymmetrical while moles are symmetrical\nB for border- early melanomas tend to have uneven borders\nC for color- early melanomas are often a vareity of shades of brown while common moles tend to be one shade\nD for diameter- the diameter of melanomas are typically larger than that of a mole\nE for evolution- changes in a skin lesion may indicate skin cancer\nAs seen in Figure 1, the majority of skin lesions in the HAM10000 dataset are melanocytic nevi (NV), and the second most common skin lesions are melanomas (MEL).\nWe use CNN and logistic regression models to classify types of skin lesions identified in the HAM10000 dataset. Using the logistic regression models on non-image variables provided in the dataset, we achieve a testing accuracy of 70%. Including images when training the model does not significantly increase the accuracy rate. When using the CNN method, we apply class weights as the dataset is highly imbalance towards NV diagnoses. This results in a model with 75% testing accuracy.\nWe also separate skin lesion types into ‘malignant’ and ‘benign’ categories. When training to classify more dangerous skin lesions from harmless skin lesions, the weighted logistic regression model is able to correctly identify 57% of dermoscopic images showing malignant skin lesions. Compared to the CNN model of simply classifying skin lesions, where malignant skin lesions are correctly identified 3% to 16% of the time, grouping the malignant skin lesions for classification results in a much improved model for correctly identifying malignant skin lesions.\nWhile the imbalanced dataset made the methods we used more difficult to train to achieve higher accuracy than baseline, weighing these models achieves an accuracy of classification of 75%, which is about 10 percentage points greater than the baseline accuracy. Additionally, our findings from malignant skin lesion identification reveal that combining malignant skin lesion categories for classification significantly improves the model performance.\n\n\nValues Statement\nWe created this model as a way to explore how technologies like convolutional neural networks could be used on a real-world problem. In the state it is in now, we do not recommend using our model as a true diagnosis tool. However, if we were to continue to improve this model to higher accuracy, there is a world where community members could upload photos of skin lesions and receive a percentage risk score that their skin lesion might benefit from analysis from a professional. That being said, an image would need to be high-quality and under professional lighting to match the image style of the images in HAM10000.\nOne of the greatest challenges in the healthcare industry is access. Geographical “deserts” exist all across the country where population healthcare needs are unmet partially or totally due to lack of adequate access (Brinzac et al). TeleHealth and WebHealth solutions are aiding these communities; a skin lesion classification algorithm could be used in such services.\nThe HAM10000 dataset was created to address the issue of small size and lack of diversity in publicly available skin cancer datasets. That being said, the images were collected from the Austrian and Australian population, consisting of predominantly white individuals. We have not tested how our algorithm’s results when applied to skin lesions on darker skin tones, but it would likely underperform.\nSpeak with just about anyone and it’s likely that they or somebody they love has been touched by cancer. This is certainly true for the four of us, especially Zoe who received a cancer diagnosis last spring. We combined Liz’s interest in applying machine learning to improve health outcomes with Zoe’s specific interest in cancer, and settled on the HAM10000 dataset because of its wide use.\nIt is our hope that the world would be a more joyful place with the implementation of this algorithm. Because of the crucial role of early recognition in skin cancer diagnosis, we hope that this model could allow people to more readily determine if they should get a skin lesion evaluated by a medical professional.\n\n\nMaterials and Methods\n\nData\nFor this project, we used the publicly available HAM10000 dataset, found on the Harvard Dataverse (Tschandl, 2023). It includes 10015 dermatoscopic images of skin lesions which were collected over a period of 20 years at two different sites, one in Austria and the other in Australia (Tschandl et al. 2018).\nAlong with the image data, we used the provided metadata file that contained information such as lesion_id, image_id, dx, dx_type, age, sex, localization, dataset. Some lesions were photographed multiple times, and dx refers to the diagnosis. One of the first challenges was converting the images into 2D numpy arrays and adding them into the data frame with the dx information (the target we were attempting to predict).\nThe limitation of this data were immediately obvious. This includes bias in the type of skin lesions represented in the data, in addition to bias of the demographics and skin types of patients with lesions represented in the data. As mentioned above, the data was collected in Australia and Austria, meaning that darker skin tones were not represented. This is particularly important to consider in a data set being used for machine learning, because if the machine doesn’t learn on a fair data set, there is likely to be a trickle down effect into the health care system, further systematizing racial injustices in patient treatment.\n\n\nFigure 1: Frequency of Skin Lesion Categories\n\n\n\nFrequency Bar Graph\n\n\nThis figure shows the frequency of diagnoses present in the data set. Clearly, the nv dx dominates the space, bringing up questions about how well a model trained on this data will classify any non-nv diagnoses. This issue will be further discussed below in the approach section.\n\nApproach\nThe features of our data used to make predictions were the images, converted into 32x32x3 numpy arrays. The goal was to predict the column of the metadata table labeled dx, the diagnosis corresponding to each lesion.\nWe subset our data into two sets, 80% for the training set and 20% for the test set. This train-test split provided a large enough sample to properly train the model, while also leaving enough data unseen to evaluate whether overfitting occurred.\nTo start out, we used a the sci kit learn LogisticRegression. This was mostly a preliminary effort to eliminate any bugs and see how we were doing in comparison to the base accuracy. After this, we moved on to our main model, which was a convolutional neural network (CNN). This decision was primarily based off previously published papers showing that this method was the most successful. We attempted several variations, each with and without data augmentation. First, we tried a minimally connected linear model. Then we added additional ReLU and Conv2D layers to see if the additional layers would improve the accuracy. We also attempted a model with added dropout layers, and a dropout probability of 25%. We started to be suspicious of the results when a model recommended by a published paper was not yielding good accuracy. After investigating this issue, we realized that because of the high prevalence of the dx nv in our dataset, the model was always choosing this as the correct answer. To fix this, we implemented class weights and transfer learning. Transfer learning proved more successful, yielding our best accuracy of around 75%.\nOriginally, we trained our models on 10 or 20 epochs. Once we isolated the best models, we spent the time to do a 100 epoch training period. All training was performed on our personal laptops.\nOur models were evaluated on accuracy of the validation set after extensive training. Confusion matrices were used extensively to investigate in which categories the misclassifications were occurring. As mentioned above, the training set contained 80% of the data (~8000 images) while the validation set was comprised of 20% (~2000 images).\n\n\n\n\nResults\nOur best models were the product of data augmentation, class weights, and transfer learning. First, we trained an ImageNet model, as designed by Stanford researchers in 2009. Training for 100 epochs took about 240 minutes but produced an accuracy of about 70% on a validation set. The accuracy steadily increased over the epochs:\n\n\n\nTrainingGraph\n\n\nTake a look at the confusion matrix:\n\n\n\nConfusionMatrix\n\n\nOur model is best at predicting nv, or melanocytic nevi. Unfortunately, this is a benign skin lesion. It is the majority class in our model, explaining why it is predicted most often. That being said, we see better accuracy with melanoma and benign keratosis than previous models.\nWith the intention of increasing classification accuracy for melanoma, the most dangerous skin cancer, we utilized mobilenet_v2, Google’s transfer learning model from 2007. By adjusting the class weights to target the minority classes more aggressively, we were able to produce the following results on validation data.\n\n\n\nConfusionMatrix\n\n\nWe see increased accuracy for melanoma, at the expense of melanocytic nevi.\n\n\nConcluding Discussion\nYour conclusion is the right time to assess:\nIn what ways did our project work? Did we meet the goals that we set at the beginning of the project? How do our results compare to the results of others who have also studied similar problems? If we had more time, data, or computational resources, what might we do differently in order to improve further?\n\n\nGroup Contribution Statement\nOur group was formed because the topics proposed by Liz and Zoe had similarities regarding identification in the health space. Once we decided to look into cancer diagnoses, Zoe presented several dataset options to the group. We decided to focus on skin cancer identification and looked into datasets as a group, ultimately landing on the HAM10000 dataset. In doing the analyses, we decided to split the workload so that Liz and Zoe work primarily on the building the convolutional neural network, while Julia and Breanna worked on improving the logistic regression models. For the final blog post, Breanna wrote the abstract, introduction and group contribution statement. Julia summarized the concluding thoughts from the project. Lize wrote the values statement and worked on the results section jointly with Zoe. Zoe worked with Liz on the results and wrote the data/methods section.\n\n\nPersonal Reflection\nAt the very end of your blog post, in a few paragraphs, respond to the following questions:\nWhat did you learn from the process of researching, implementing, and communicating about your project? How do you feel about what you achieved? Did meet your initial goals? Did you exceed them or fall short? In what ways? In what ways will you carry the experience of working on this project into your next courses, career stages, or personal life?\n\n\nSources\nBinder, M., A. Steiner, M. Schwarz, S. Knollmayer, K. Wolff, and H. Pehamberger. 1994. “Application of an Artificial Neural Network in Epiluminescence Microscopy Pattern Analysis of Pigmented Skin Lesions: A Pilot Study.” British Journal of Dermatology 130 (4): 460–65. https://doi.org/10.1111/j.1365-2133.1994.tb03378.x.\nCodella, Noel, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, et al. 2019. “Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the International Skin Imaging Collaboration (ISIC).” arXiv. https://doi.org/10.48550/arXiv.1902.03368.\nServices, Department of Health & Human. n.d. “Melanoma.” Department of Health & Human Services. Accessed May 13, 2024. http://www.betterhealth.vic.gov.au/health/conditionsandtreatments/melanoma.\n“Skin Cancer Early Detection.” n.d. Fred Hutch. Accessed May 13, 2024. https://www.fredhutch.org/en/patient-care/prevention/skin-cancer-early-detection.html.\nTandon, Ritu, Shweta Agrawal, Narendra Pal Singh Rathore, Abhinava K. Mishra, and Sanjiv Kumar Jain. 2024. “A Systematic Review on Deep Learning-Based Automated Cancer Diagnosis Models.” Journal of Cellular and Molecular Medicine 28 (6): e18144. https://doi.org/10.1111/jcmm.18144.\nTschandl, Philipp. 2023. “The HAM10000 Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions.” Harvard Dataverse. https://doi.org/10.7910/DVN/DBW86T.\nTschandl, Philipp, Cliff Rosendahl, and Harald Kittler. 2018. “The HAM10000 Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions.” Scientific Data 5 (1): 180161. https://doi.org/10.1038/sdata.2018.161.\n“What Is a Convolutional Neural Network? | 3 Things You Need to Know.” n.d. Accessed May 13, 2024. https://www.mathworks.com/discovery/convolutional-neural-network.html.\nBrinzac, Monica, Kuhlmann, Ellen, Dussault, Gilles. “Defining medical deserts – an international consensus-building exercise.” PubMed, National Center for Biotechnology Information. https://pubmed.ncbi.nlm.nih.gov/37421651/#:~:text=Results%3A%20The%20agreed%20definition%20highlight."
  },
  {
    "objectID": "posts/BP7/BP7.html",
    "href": "posts/BP7/BP7.html",
    "title": "Blog Post: Logistic Regression",
    "section": "",
    "text": "First of all, here is a link to my LogisticRegression source code: https://github.com/ShadowedSpace/ShadowedSpace.github.io/blob/main/posts/BP7/LogisticRegression.py\nIn this blog post, I aim to perform several experiments on the above posted logistic regression source code. The first experiment will be vanilla gradient descent, to get a better understanding of whether my source code works, and what it means for it to work. Then, I will implement momentum, and compare the efficiency of the first two experiments. Finally, I will do an experiment to show the danger of overfitting and that it doesn’t generalize well.\n\n%load_ext autoreload\n%autoreload 2\nfrom LogisticRegression import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\n\n# generates data for classification problem\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nw = torch.linspace(-1, 1, X.shape[1])\nw_prev = torch.linspace(-1, 1, X.shape[1])\n\n\n# plots data and boundary decision based on w\n# only works for dimension = 2\n\ndef plot_data_and_boundary(X, y, w, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the data\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    colors = [\"red\", \"blue\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = colors[i], cmap = \"BrBG\", marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n# calculates the accuracy of a model\n\ndef accuracy(y_pred, y_true):\n    correct = (y_pred == y_true).sum()\n    total = y_true.shape[0]\n    print(correct)\n    return correct / total\n\n\n\nThis first experiment is to implement “vanilla gradient descent.” Its main purpose is to ensure that the LogisticRegression and GradientDescentOptimizer classes are properly implemented. The intent is to show that w looks visually correct, and the loss decreases monotonically.\n\n# vanilla gradient descent\n\nLR = LogisticRegression(w)\nopt = GradientDescentOptimizer(LR, w, w_prev)\n\nlosses = []\n\nfor _ in range(100):\n    losses.append(LR.loss(X, y))\n    opt.step(X, y, alpha = 1.5, beta = 0)\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nOk nice! It looks like the losses indeed converge, so that’s exciting to see. It looks like it takes around 70 steps to even out.\n\nplot_data_and_boundary(X, y, LR.w)\n\n\n\n\n\n\n\n\nThat’s a solid decision boundary. Probably pretty close to what my human eye could make out as the best linear decision boundary.\n\n# Predict the labels\ny_pred = LR.predict(X)\n\n# Calculate the accuracy\nacc = accuracy(y_pred, y)\nprint(f'Accuracy: {acc}')\n\ntensor(273)\nAccuracy: 0.9100000262260437\n\n\n\n\n\nThe second experiment is to better learn how momentum impacts logistic regression. With a proper selection of beta, it should converge even faster than vanilla gradient descent.\n\n# momentum\n\nw2 = torch.linspace(-1, 1, X.shape[1])\nw_prev2 = torch.linspace(-1, 1, X.shape[1])\n\nLR2 = LogisticRegression(w)\nopt2 = GradientDescentOptimizer(LR2, w2, w_prev2)\n\nlosses_mom = []\n\nfor _ in range(50):\n    losses_mom.append(LR2.loss(X, y))\n    opt2.step(X, y, alpha = 0.45, beta = 0.9)\n\nplt.plot(losses_mom, label = \"momentum\")\nplt.plot(losses, label = \"vanilla\")\nplt.xlim(0, 50)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nWoah - LR with momentum is totally worth the extra term. This converged after only about 15 steps (in comparison to ~70 without momentum).\n\n# Predict the labels\ny_pred2 = LR2.predict(X)\n\n# Calculate the accuracy\nacc = accuracy(y_pred2, y)\nprint(f'Accuracy: {acc}')\n\ntensor(273)\nAccuracy: 0.9100000262260437\n\n\n\nplot_data_and_boundary(X, y, LR2.w)\n\n\n\n\n\n\n\n\n\n\n\nThe third and final experiment is to look into the effects of overfitting. To do this, I will generate two data sets through exactly the same method, and fit the training set on an absurdly high number of dimensions to see what impact this has on the test set. I would expecet the accuracy to decrease noticeably.\n\n# overfitting\n\nX_train, y_train = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\nX_val, y_val = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\n\nw3 = torch.linspace(-1, 1, X_train.shape[1])\nw_prev3 = torch.linspace(-1, 1, X_train.shape[1])\n\nLR3 = LogisticRegression(w3)\nopt = GradientDescentOptimizer(LR3, w3, w_prev3)\n\nlosses = []\nlosses_val = []\n\nfor _ in range(500):\n    losses.append(LR3.loss(X_train, y_train))\n    losses_val.append(LR3.loss(X_val, y_val))\n    opt.step(X_train, y_train, alpha = 0.15, beta = 0.2)\n\nplt.plot(losses, label = \"training\")\nplt.plot(losses_val, label = \"validation\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nHere, I plot the losses of the train set (blue) and the losses of the validation set (orange) as the logistic regression algorithm steps. As predicted, because of the high number of dimensions, while the losses for the train set converge rather quickly, while the losses of the validation set do not converge.\n\n# Predict the labels\ny_pred_train = LR3.predict(X_train)\n\n# Calculate the accuracy\nacc = accuracy(y_pred_train, y_train)\nprint(f'Accuracy: {acc}')\n\ntensor(75)\nAccuracy: 1.0\n\n\nAn astounding accuracy of 100% on the training test. I sure would hope so with more dimensions than data points.\n\n# Predict the labels\ny_pred_test = LR3.predict(X_test)\n\n# Calculate the accuracy\nacc = accuracy(y_pred_test, y_test)\nprint(f'Accuracy: {acc}')\n\ntensor(67)\nAccuracy: 0.8933333158493042\n\n\nLooks like we (successfully) overfit from the less than satisfactory 92% accuracy on the test set. While more dimensions than necessary can get an impressive result on the training set, it doesn’t generalize so well.\n\n\n\n\nWell, I agree with Phil — that was a pretty cool blog post. I definitely struggled some of the more challenging bits (especially plots and vectorized operations), but looking back at my accomplishment, I am proud of the learning I did. I have a much better understanding of how logistic regression works as a classifier, and how to show this in the form of loss graphs and data visualization. I also learned about the use of a validation set to track loss on a “test” set without touching the test set, which is a pretty neat comment. My first experiment (vanilla gradient descent) showed that the loss monotonically decreases. The momentum experiment showed that logistic regression can be further optimized. Finally, the overfitting experiment showed that too many dimensions can decrease accuracy when generalizing to unknown data."
  },
  {
    "objectID": "posts/BP7/BP7.html#introduction",
    "href": "posts/BP7/BP7.html#introduction",
    "title": "Blog Post: Logistic Regression",
    "section": "",
    "text": "First of all, here is a link to my LogisticRegression source code: https://github.com/ShadowedSpace/ShadowedSpace.github.io/blob/main/posts/BP7/LogisticRegression.py\nIn this blog post, I aim to perform several experiments on the above posted logistic regression source code. The first experiment will be vanilla gradient descent, to get a better understanding of whether my source code works, and what it means for it to work. Then, I will implement momentum, and compare the efficiency of the first two experiments. Finally, I will do an experiment to show the danger of overfitting and that it doesn’t generalize well.\n\n%load_ext autoreload\n%autoreload 2\nfrom LogisticRegression import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\n\n# generates data for classification problem\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nw = torch.linspace(-1, 1, X.shape[1])\nw_prev = torch.linspace(-1, 1, X.shape[1])\n\n\n# plots data and boundary decision based on w\n# only works for dimension = 2\n\ndef plot_data_and_boundary(X, y, w, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the data\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    colors = [\"red\", \"blue\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = colors[i], cmap = \"BrBG\", marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n# calculates the accuracy of a model\n\ndef accuracy(y_pred, y_true):\n    correct = (y_pred == y_true).sum()\n    total = y_true.shape[0]\n    print(correct)\n    return correct / total\n\n\n\nThis first experiment is to implement “vanilla gradient descent.” Its main purpose is to ensure that the LogisticRegression and GradientDescentOptimizer classes are properly implemented. The intent is to show that w looks visually correct, and the loss decreases monotonically.\n\n# vanilla gradient descent\n\nLR = LogisticRegression(w)\nopt = GradientDescentOptimizer(LR, w, w_prev)\n\nlosses = []\n\nfor _ in range(100):\n    losses.append(LR.loss(X, y))\n    opt.step(X, y, alpha = 1.5, beta = 0)\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nOk nice! It looks like the losses indeed converge, so that’s exciting to see. It looks like it takes around 70 steps to even out.\n\nplot_data_and_boundary(X, y, LR.w)\n\n\n\n\n\n\n\n\nThat’s a solid decision boundary. Probably pretty close to what my human eye could make out as the best linear decision boundary.\n\n# Predict the labels\ny_pred = LR.predict(X)\n\n# Calculate the accuracy\nacc = accuracy(y_pred, y)\nprint(f'Accuracy: {acc}')\n\ntensor(273)\nAccuracy: 0.9100000262260437\n\n\n\n\n\nThe second experiment is to better learn how momentum impacts logistic regression. With a proper selection of beta, it should converge even faster than vanilla gradient descent.\n\n# momentum\n\nw2 = torch.linspace(-1, 1, X.shape[1])\nw_prev2 = torch.linspace(-1, 1, X.shape[1])\n\nLR2 = LogisticRegression(w)\nopt2 = GradientDescentOptimizer(LR2, w2, w_prev2)\n\nlosses_mom = []\n\nfor _ in range(50):\n    losses_mom.append(LR2.loss(X, y))\n    opt2.step(X, y, alpha = 0.45, beta = 0.9)\n\nplt.plot(losses_mom, label = \"momentum\")\nplt.plot(losses, label = \"vanilla\")\nplt.xlim(0, 50)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nWoah - LR with momentum is totally worth the extra term. This converged after only about 15 steps (in comparison to ~70 without momentum).\n\n# Predict the labels\ny_pred2 = LR2.predict(X)\n\n# Calculate the accuracy\nacc = accuracy(y_pred2, y)\nprint(f'Accuracy: {acc}')\n\ntensor(273)\nAccuracy: 0.9100000262260437\n\n\n\nplot_data_and_boundary(X, y, LR2.w)\n\n\n\n\n\n\n\n\n\n\n\nThe third and final experiment is to look into the effects of overfitting. To do this, I will generate two data sets through exactly the same method, and fit the training set on an absurdly high number of dimensions to see what impact this has on the test set. I would expecet the accuracy to decrease noticeably.\n\n# overfitting\n\nX_train, y_train = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\nX_val, y_val = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\n\nw3 = torch.linspace(-1, 1, X_train.shape[1])\nw_prev3 = torch.linspace(-1, 1, X_train.shape[1])\n\nLR3 = LogisticRegression(w3)\nopt = GradientDescentOptimizer(LR3, w3, w_prev3)\n\nlosses = []\nlosses_val = []\n\nfor _ in range(500):\n    losses.append(LR3.loss(X_train, y_train))\n    losses_val.append(LR3.loss(X_val, y_val))\n    opt.step(X_train, y_train, alpha = 0.15, beta = 0.2)\n\nplt.plot(losses, label = \"training\")\nplt.plot(losses_val, label = \"validation\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nHere, I plot the losses of the train set (blue) and the losses of the validation set (orange) as the logistic regression algorithm steps. As predicted, because of the high number of dimensions, while the losses for the train set converge rather quickly, while the losses of the validation set do not converge.\n\n# Predict the labels\ny_pred_train = LR3.predict(X_train)\n\n# Calculate the accuracy\nacc = accuracy(y_pred_train, y_train)\nprint(f'Accuracy: {acc}')\n\ntensor(75)\nAccuracy: 1.0\n\n\nAn astounding accuracy of 100% on the training test. I sure would hope so with more dimensions than data points.\n\n# Predict the labels\ny_pred_test = LR3.predict(X_test)\n\n# Calculate the accuracy\nacc = accuracy(y_pred_test, y_test)\nprint(f'Accuracy: {acc}')\n\ntensor(67)\nAccuracy: 0.8933333158493042\n\n\nLooks like we (successfully) overfit from the less than satisfactory 92% accuracy on the test set. While more dimensions than necessary can get an impressive result on the training set, it doesn’t generalize so well."
  },
  {
    "objectID": "posts/BP7/BP7.html#conclusion",
    "href": "posts/BP7/BP7.html#conclusion",
    "title": "Blog Post: Logistic Regression",
    "section": "",
    "text": "Well, I agree with Phil — that was a pretty cool blog post. I definitely struggled some of the more challenging bits (especially plots and vectorized operations), but looking back at my accomplishment, I am proud of the learning I did. I have a much better understanding of how logistic regression works as a classifier, and how to show this in the form of loss graphs and data visualization. I also learned about the use of a validation set to track loss on a “test” set without touching the test set, which is a pretty neat comment. My first experiment (vanilla gradient descent) showed that the loss monotonically decreases. The momentum experiment showed that logistic regression can be further optimized. Finally, the overfitting experiment showed that too many dimensions can decrease accuracy when generalizing to unknown data."
  },
  {
    "objectID": "posts/BP4/BP4.html",
    "href": "posts/BP4/BP4.html",
    "title": "Blog Post: Women in Data Science",
    "section": "",
    "text": "Blog Post: Women in Data Science\nIn this blog post, I will first seek to better understand the under-representation of women in engineering and the computing sciences, the harms that this causes for everyone, why this happens, and some steps towards improving it. Then, I will discuss the Women in Data Science Conference at Middlebury, one such step taken to help give women a sense of belonging in fields that are more stereotypically dominated by men. Each speaker shared important lessons about their time in data science, and how data science is used in the real world. Then I will summarize my learnings from the readings regarding women in data science, attending the conference, and writing this blog post!\nThere are a few reasons why it is problematic that women are underrepresented in engineering and computing sciences. First off, and potentially most obviously, this is a problem for women. When women cannot work in fields, their needs may be overlooked by their male counterparts. However, this under-representation is not just a problem for women - its a problem for everyone. Research has found that innovation soars with greater diversity in the work force. Further research has shown that low-performing men are frequently hired over high-performing women. This combination of factors leaves a large talent pool untapped, and many breakthroughs on the bench. Increased diversity has also been found to increase productivity.\nMany fields that have found themselves under-representing women historically have made huge gains with respect to representation since the 2000s. Computing represents the unique exception to the rule - while almost 40% of the field was women in 1985, that percent has declined continuously and in 2013, women only made up 18% of the workforce. This percent is reminiscent of the 1970s. One reason that could be behind this is the narrow focus of many engineering jobs; there is rarely a drive to discuss and consider social and ethical responsibilities of ones’ job. As women have a higher preference for attaining a clear social purpose, this leads to imbalance. Other issues include isolation, stereotyping, and challenges with work life balance that are culturally expected of many women like marriage and children. Additionally, women are statistically more likely to be the victim of sexual assault.\nDespite these challenges, progress is being made towards change. One idea is to include components of ethical and community values in both college courses and the work place. This can help women feel as if they are making a contribution to society. Feeling welcome and wanted can improve motivation, perseverance, and commitment to computing, boosting interest, and retention of women in the field. Anti-harassment and anti-assault policies and trainings can feel safe in the workplace. Introducing women to computing and engineering from a young age and exposing students to female and non-binary role models who have successful careers can provide a sense of belonging. One example of that is the Women in Data Science conference hosted annually at Middlebury College.\nThe conference opened with Professor Amy Yuen’s lightning talk. As a political science professor, one of her recent areas of study has been representation in the UN Security council - more specifically whether the representation was equal. She explained how it seemed unlikely; the council consists of only 15 member countries at a time. Moreover, 1/3 of these countries always remain on the council with veto privileges while the other 10 seats are campaigned for by countries based on region. Professor Yuen determined that a successful member would have a high output discovered that wealthy countries were not necessarily more productive when on the council. Rather, sponsorship was a significant influencing factor. Somewhat surprisingly, she also determined that the council has somewhat equal representation among the seats that are campaigned for.\nThe keynote presenter, Professor Sarah Brown, spoke about the ethical implications of involving machine learning in our daily life. As usage of such algorithms increases, so does the need to ensure that they are capable of fairly analyzing data. To frame her talk, she shared three keys, or epiphanies that she has had, and how they apply to data science. Each key related understanding the context of data to its proper usage.\nThe first key was that Professor Brown discovered was that context is necessary to understand primary sources. The following example in particular resonated with me. Her project involved involved diagnosing patients with PTSD. The formerly used method applied a threshold, below which patients were no longer assessed. Professor Brown, after gaining a better understanding of how the data was interpreted, was able to make a minor adjustment which drastically decreased misclassification of patients who had PTSD as not having PTSD. As someone who is deeply interested in the healthcare system, how access can be increased, and the role data science is and will play in it, I found this insightful.\nContinuing on, she shared her discovery that disciplines are communities. This was an essential learning; most people who work with data science are, well, computer scientists, and don’t have the expertise to interpret the data in context. Involving others from the community, and even other communities from relevant fields can lessen bias and bolster information gained from the results.\nThe final key was to meet people where they are. While she was on the board of the National Society for Black Engineers, she discovered a startling breakdown of how information was passed on. The national board received positive feedback from individual chapter heads, the policies still weren’t being implemented. Eventually, they realized that student organizations didn’t have the resources or motivation to make these policies happen. This translated to machine learning when Professor Brown noticed that once an algorithm is accurate, producers will hesitate to make it fair, not sure if it is worth the trade-off in accuracy. Her proposed solution was to indicate whether a model was fair before fitting it, to reduce hesitance to make a model fair.\nThe second lightning talk was shared by Professor Jessica L’Roe highlighted the importance of context in research. She went into detail about how she collected quantitative and qualitative data during her work with deforestation directly from local communities that were impacted. By doing so, she discovered that the tree planting was carried out primarily by foreigners, and they were not planting local species of trees. As a result of the increased interest in tree planting by foreigners, much of the farming land was being purchased, and mothers local to the area had begun prioritizing education over agriculture as a future for their children due to concerns of insufficient land to be farmed. Without gathering data from locals directly impacted, Professor L’Roe never would have discovered the subtle intricacies of her data, and how it was changing life for future generations of locals.\nProfessor Biester (one of our own Middlebury Computer Science Professors!) shared a talk about her research on mental health and social media presence. To collect data, she took on the impressive feat of scraping a massive amount of data from reddit. Then, she searched the data for instances of specific first person declarations of having been diagnosed with depression. The assumption that makes this work is that although a few users may lie, the percentage of users who claim to have been diagnosed and but have not been is negligible compared to those who claim to have been diagnosed and actually have been diagnosed, and vice versa for those who did not claim to be diagnosed. I learned that looking at the data and thinking about what must be accounted for, and what factors are negligible is an important step in cleaning and preparing the data.\nFirst and foremost, writing this blog post forced me to sit with how tall of a mountain we will have to climb to have equal representation, opportunity, and treatment of women working in STEM. I was previously aware of the under-representation of women in these fields, I didn’t quite realize how deep the roots were of this issue, or how difficult it would be to unravel them. From stereotype threat to a sense of belonging, it is challenging to be a successful woman in STEM. However, I was pleased to see the deep thought that went into some steps that have been taken (by, for example, Harvey Mudd College), and even just an increased awareness that this is, indeed an issue (thanks to big tech companies for publishing percentages of women in the workforce). While this mountain might be a tall one, it’s definitely worth climbing. I hope to learn more about steps I can take to reduce all kinds of bias in algorithms that I implement by considering what biases I have beforehand and asking experts for help to reduce them."
  },
  {
    "objectID": "posts/BP2/BP2.html",
    "href": "posts/BP2/BP2.html",
    "title": "Blog Post: ‘Optimal’ Decision Making",
    "section": "",
    "text": "In this blog post, I aim to analyze data regarding the likelihood that the bank will grant a loan based on a group of personal attributes. First, I will find explore the data set to observe trends, and then I will determine the best attributes for successfully predicted whether a borrower will default on a loan. Then, I will build a model and determine an ideal threshold before analyzing the test data. Finally, I will observe the accuracy and profit with which my model performs on the test data from the point of view of the bank and the borrower.\nOnce again, to begin, the training data must be accessed.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nHere’s a look at the data:\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nTime to clean the data and take another look. I won’t one hot encode the columns yet, because I’m going to use some of them to explore the data set.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nle.fit(df_train['loan_status'])\n\ndef prepare_data(df):\n    df = df.drop(['loan_grade'], axis=1)\n    df = df.dropna()\n    y = le.fit_transform(df['loan_status'])\n    df = df.drop(['loan_status'], axis = 1)\n\n    return df, y\n\nX_train, y_train = prepare_data(df_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\n11750\n13.47\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\n10000\n7.51\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\n1325\n12.87\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\n15000\n9.63\n0.28\nN\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\n5500\n14.91\n0.25\nN\n2\n\n\n\n\n\n\n\n\n\nLet’s get familiar with some of the attributes and decide which ones might be helpful to predict whether someone defaulted on a loan or not.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nx = X_train['person_home_ownership']\ny = X_train['loan_percent_income']\n\nax = sns.barplot(data=X_train, x = 'person_home_ownership', y = 'loan_percent_income', hue = y_train.astype(str))\n\nh,l = ax.get_legend_handles_labels()\nl = [\"Repaid Loan\", \"Defaulted\"]\nax.legend(h, l)\n\nax.set_title(\"Percent Income vs. Home Ownership\")\nax.set_xlabel(\"Home Ownership\")\nax.set_ylabel(\"Loan Percent of Income\")\n\nplt.show()\n\n\n\n\n\n\n\n\nLooking at different types of home ownership, it looks like the percent income of the loan plays a really big role in whether or not it gets defaulted.\n\nx = X_train['loan_percent_income']\ny = X_train['loan_int_rate']\n\nax = sns.scatterplot(data=X_train, x = 'loan_percent_income', y = 'loan_int_rate', hue = y_train.astype(str), alpha = 0.5)\n\nh,l = ax.get_legend_handles_labels()\nl = [\"Repaid Loan\", \"Defaulted\"]\nax.legend(h, l)\n\nax.set_title(\"Interest Rate vs. Percent Income\")\nax.set_xlabel(\"Loan Percent of Income\")\nax.set_ylabel(\"Loan Interest Rate\")\n\nplt.show()\n\n\n\n\n\n\n\n\nWell, unsurprisingly, it looks like the chance that a loan is fully repaid is increased as the interest rate falls, and as the percentage of a person’s income falls.\n\nX_train.groupby([y_train, 'cb_person_default_on_file'])[['person_emp_length', 'cb_person_cred_hist_length']].mean()\n\n\n\n\n\n\n\n\n\nperson_emp_length\ncb_person_cred_hist_length\n\n\n\ncb_person_default_on_file\n\n\n\n\n\n\n0\nN\n4.987118\n5.809296\n\n\nY\n4.782471\n5.928938\n\n\n1\nN\n4.161868\n5.603114\n\n\nY\n4.166886\n5.735217\n\n\n\n\n\n\n\nPeople who repaid the loan fully have generally been employed for a bit longer. I was surprised by how little difference it made to have a previous default on file. Also, while credit history length decreased slightly for loans that were defaulted, it didn’t seem to be by a significant amount.\n\n\n\nNow that I’m ready to start modeling, I’ll go ahead an one hot encode any qualitative columns.\n\nX_train = pd.get_dummies(X_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\nI’m going to try recursive feature elimination to assign weights to different features in the training data and select the best ones.\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nestimator = LogisticRegression()\n\ncols = RFE(estimator, n_features_to_select=3, step=1).fit(X_train, y_train)\ncols_rfe = cols.get_feature_names_out()\ncols_rfe\n\narray(['person_income', 'loan_amnt', 'loan_int_rate'], dtype=object)\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\nlr_rfe = LogisticRegression().fit(X_train[cols_rfe], y_train)\n\ncv_scores_rfe = cross_val_score(lr_rfe, X_train[cols_rfe], y_train, cv=5).mean()\ncv_scores_rfe\n\n0.8117172718507574\n\n\nRFE uses recursion consider smaller and smaller sets of features and selects the best (3 in this case) for a model. Based on this model, the best features are income, loan amount, and loan interest rate, and the model has an accuracy of about 81%. I’m going to try a few other models before I choose what features to use for my final model.\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\n\ncols = SelectKBest(mutual_info_classif, k = 3)\ncols.fit_transform(X_train, y_train)\ncols_skb = cols.get_feature_names_out()\ncols_skb\n\narray(['person_income', 'loan_int_rate', 'loan_percent_income'],\n      dtype=object)\n\n\n\nlr_skb = LogisticRegression().fit(X_train[cols_skb], y_train)\n\ncv_scores_skb = cross_val_score(lr_skb, X_train[cols_skb], y_train, cv=5).mean()\ncv_scores_skb\n\n0.8021563455835603\n\n\nSelectKBest uses statistical tests to rank features based on their relationship with the outcome variables. It selected income, interest rate, and loan percent income as the best features with an accuracy of 80% (just a bit lower than RFE). I note that 2/3 of these features agree with RFE.\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.0001, penalty=\"l1\", dual=False).fit(X_train, y_train)\nmodel = SelectFromModel(lsvc, prefit=True).fit(X_train, y_train)\nx_new = model.transform(X_train)\ncols_svc = model.get_feature_names_out()\ncols_svc\n\narray(['person_age', 'loan_amnt', 'loan_int_rate'], dtype=object)\n\n\n\nlr_svc = LogisticRegression().fit(X_train[cols_svc], y_train)\n\ncv_scores_svc = cross_val_score(lr_svc, X_train[cols_svc], y_train, cv=5).mean()\ncv_scores_svc\n\n0.7949095723125648\n\n\nLinearSVC uses a random number generator to select features - this makes sense as we’re seeing less feature overlap, and slightly lower accuracy. Since it looks like RFE worked the best to select features so I’ll stick with person_income, loan_int_rate, and loan_percent_income as the attributes for the remainder of the project.\n\nw = lr_rfe.coef_.T\nw\n\narray([[-4.05735976e-05],\n       [ 1.06558819e-04],\n       [ 9.49045880e-08]])\n\n\n\ndef calc_score(X, w):\n    return X@w\n\n\nX_train['score'] = calc_score(X_train[cols_rfe], w)\n\n\n\n\n\nimport numpy as np\n\n# create columns for the profit of the bank if the loan is not defaulted and the cost if it is\nX_train['profit'] = X_train[\"loan_amnt\"]*(1 + 0.25*X_train[\"loan_int_rate\"] * 0.01)**10 - X_train[\"loan_amnt\"]\nX_train['cost'] = X_train[\"loan_amnt\"]*(1 + 0.25*X_train[\"loan_int_rate\"] * 0.01)*3 - 1.7*X_train[\"loan_amnt\"]\n\nprofit = []\nacc = []\n\nfor t in np.linspace(-2, 0, 41):\n    #if score &lt;= t, they get the loan\n    y_pred = X_train['score'] &gt; t\n\n    # when y_pred and y_train equal one, the bank correctly predicted a default and profit is 0\n    # when y_pred and y_train equal zero, the bank correctly predicted a repaid loan and profit +X_train['profit']\n    # when y_pred = 1, y_train = 0 the bank predicted a default and did not give the loan, so profit is 0\n    # when y_pred = 0, y_train = 1 the bank incorrectly predicted the loan would be repaid, so profit is -X_train['cost']\n\n    profit.append((((y_pred != y_train) * (y_pred == 0) * (-X_train['cost'])) + ((y_pred == y_train) * (y_pred == 0) * X_train['profit'])).mean())\n    acc.append((y_pred == y_train).mean())\n\n# making a plot of the threshold data from above loop\nplt.plot(np.linspace(-2, 0, 41), profit)\nplt.title(\"Profit vs. Threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit per Borrower\")\nplt.show()\n\nt_best = -2 + np.argmax(np.array(profit)) * 0.05\nprint(f\"A threshold of {t_best:.2f} gives the maximum profit of ${np.array(profit).max():.2f} per borrower and an accuracy of {acc[np.argmax(np.array(profit))]*100:.0f}%.\")\n\n\n\n\n\n\n\n\nA threshold of -0.70 gives the maximum profit of $608.31 per borrower and an accuracy of 77%.\n\n\nLooking at the plot, we see that from the smallest threshold of -2, the profit rises slowly until it reaches a peak of just over $600 at t = -0.70 and then plummets steeply.\n\n\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\n\nX_test, y_test = prepare_data(df_test)\n\nX_test['score'] = calc_score(X_test[cols_rfe], w)\nX_test['profit'] = X_test[\"loan_amnt\"]*(1 + 0.25*X_test[\"loan_int_rate\"] * 0.01)**10 - X_test[\"loan_amnt\"]\nX_test['cost'] = X_test[\"loan_amnt\"]*(1 + 0.25*X_test[\"loan_int_rate\"] * 0.01)*3 - 1.7*X_test[\"loan_amnt\"]\n\n\ny_pred_test = X_test['score'] &gt; t_best\navg_test_profit = ((((y_pred_test != y_test) * (y_pred_test == 0) * (-X_test['cost'])) + ((y_pred_test == y_test) * (y_pred_test == 0) * X_test['profit'])).mean())\ntest_acc = ((y_pred_test == y_test).mean())\n\nprint(f\"A threshold of {t_best:.2f} on the test data set gives an average profit of ${avg_test_profit:.2f} per borrower and an accuracy of {test_acc*100:.0f}%.\")\n\nA threshold of -0.70 on the test data set gives an average profit of $472.68 per borrower and an accuracy of 77%.\n\n\nHonestly, I feel pretty good about this. The expected profit for the test set is a bit over $150 less per person than that of the training data set. Additionally, the accuracy is exactly the same - yay!\n\n\n\n\nages = X_test['person_age'].unique()\nages.sort()\n\npred_d_rates = []\nactual_d_rates = []\n\nfor a in ages:\n    pred_d_rates.append((y_pred_test * (a == X_test['person_age'])).mean() * 100)\n    actual_d_rates.append((y_test * (a == X_test['person_age'])).mean() * 100)\n\nfig, ax = plt.subplots()\nax.plot(ages, pred_d_rates, label=\"Predicted\")\nax.plot(ages, actual_d_rates, label='Actual')\nplt.ylabel(\"Percent Default Rate\")\nplt.xlabel(\"Age\")\nax.legend()\n\n\n\n\n\n\n\n\nIt looks like the bank predicts a much higher rate of default for younger folks. This has a particularly strong impact on people in their 20s, but also seems to have some impact on people in their 30. As seen by the visual above, this does correlate reasonably well with an increased chance of an actual default among these ages.\n\nX_test['y_test'] = y_test\nX_test['y_pred_test'] = y_pred_test\n\nmean_values = X_test.groupby('loan_intent').agg({'y_test': 'mean', 'y_pred_test': 'mean'})\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nbar_width = 0.35\nindex = np.arange(len(mean_values))\n\nax.bar(index, mean_values['y_test'], bar_width, label='Actual')\nax.bar(index + bar_width, mean_values['y_pred_test'], bar_width, label='Predicted')\n\nax.set_xlabel('Loan Intent')\nax.set_ylabel('Mean Values')\nax.set_title('Mean of y_test and y_pred_test Values for Each Loan Intent')\nax.set_xticks(index + bar_width / 2)\nax.set_xticklabels(mean_values.index)\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the actual and predicted mean rates of default based on loan intent. The higher a column is means the higher the chance of default, or the more data points where a default occurred (blue) or was predicted (orange), as defaulting is represented by 1 in this data set. We can see that medical loans have the highest predicted rate of default among all types of loan intent, which means medical loans are the least likely to be granted of any type of loan, or in other words, the most difficult to attain. In this category, its just a bit of an over-prediction of the actual rate of default. On the other hand, the rate of default for business ventures and education loans is steeply over predicted by the model compared to the actual rate.\n\nX_test['inc_percentile'] = round(X_test['person_income'].rank(pct = True), 2) * 100\ngrouped_data = X_test.groupby(['inc_percentile'])['y_pred_test'].mean()\ngrouped_data = pd.DataFrame(grouped_data)\n\nax = sns.scatterplot(data=grouped_data, x='y_pred_test', y='inc_percentile')\nax.set(xlabel = \"Predicted Default Rate\")\nax.set(ylabel = \"Income Percentile\")\n\n\n\n\n\n\n\n\nThis is… well, more or less the trend I expected. As the income percentile increases, so does the chance that the bank grants them a loan. This is seen by the more or less inverse relationship on the plot above.\nAs mentioned earlier, it getting a medical loan can be challenging. In my opinion (supported by the middle view of fairness of Barocas, Hardt, and Narayanan), this is unfair. As they stated, “this view holds that the decision makers have an obligation to avoid perpetuating injustice.” Often times, individuals are born with medical conditions. This is completely out of their control, and doesn’t necessarily guarantee that they possess the economic capital to pay for continuous treatment. In fact, I’d assume that there is a higher chance of being born into a high-risk medical group in lower income brackets, and by this logic, the middle view of fairness is blatantly violated by the difficulty to access medical loans regardless of whether they are paid back.\nIn this blog post, I learned a lot about how difficult it can be to manipulate data with groupby functions to create effective plots. Keeping the number of points to a minimum can help make a clear point and avoid diluting with substantial amounts of noise. The technique I used was splitting data up into percentiles. Then I explored the data, and with recursive feature elimination to determined which features to model (income, loan amount, and loan interest rate) and based off this model, I found a good threshold. After running the model on the test set, I observed that younger folks have more difficulty getting a loan (which makes some sense given their increased risk of default), that medical loans are the most difficult to get, and that business venture and education loans are the categories in which the bank most over-predicts the chance of default. Also, a higher income is a strong contributing factor to being granted a loan."
  },
  {
    "objectID": "posts/BP2/BP2.html#evaluating-the-model-from-the-banks-perspective",
    "href": "posts/BP2/BP2.html#evaluating-the-model-from-the-banks-perspective",
    "title": "Blog Post: ‘Optimal’ Decision Making",
    "section": "",
    "text": "url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\n\nX_test, y_test = prepare_data(df_test)\n\nX_test['score'] = calc_score(X_test[cols_rfe], w)\nX_test['profit'] = X_test[\"loan_amnt\"]*(1 + 0.25*X_test[\"loan_int_rate\"] * 0.01)**10 - X_test[\"loan_amnt\"]\nX_test['cost'] = X_test[\"loan_amnt\"]*(1 + 0.25*X_test[\"loan_int_rate\"] * 0.01)*3 - 1.7*X_test[\"loan_amnt\"]\n\n\ny_pred_test = X_test['score'] &gt; t_best\navg_test_profit = ((((y_pred_test != y_test) * (y_pred_test == 0) * (-X_test['cost'])) + ((y_pred_test == y_test) * (y_pred_test == 0) * X_test['profit'])).mean())\ntest_acc = ((y_pred_test == y_test).mean())\n\nprint(f\"A threshold of {t_best:.2f} on the test data set gives an average profit of ${avg_test_profit:.2f} per borrower and an accuracy of {test_acc*100:.0f}%.\")\n\nA threshold of -0.70 on the test data set gives an average profit of $472.68 per borrower and an accuracy of 77%.\n\n\nHonestly, I feel pretty good about this. The expected profit for the test set is a bit over $150 less per person than that of the training data set. Additionally, the accuracy is exactly the same - yay!"
  },
  {
    "objectID": "posts/BP2/BP2.html#and-the-borrowers-perspective",
    "href": "posts/BP2/BP2.html#and-the-borrowers-perspective",
    "title": "Blog Post: ‘Optimal’ Decision Making",
    "section": "",
    "text": "ages = X_test['person_age'].unique()\nages.sort()\n\npred_d_rates = []\nactual_d_rates = []\n\nfor a in ages:\n    pred_d_rates.append((y_pred_test * (a == X_test['person_age'])).mean() * 100)\n    actual_d_rates.append((y_test * (a == X_test['person_age'])).mean() * 100)\n\nfig, ax = plt.subplots()\nax.plot(ages, pred_d_rates, label=\"Predicted\")\nax.plot(ages, actual_d_rates, label='Actual')\nplt.ylabel(\"Percent Default Rate\")\nplt.xlabel(\"Age\")\nax.legend()\n\n\n\n\n\n\n\n\nIt looks like the bank predicts a much higher rate of default for younger folks. This has a particularly strong impact on people in their 20s, but also seems to have some impact on people in their 30. As seen by the visual above, this does correlate reasonably well with an increased chance of an actual default among these ages.\n\nX_test['y_test'] = y_test\nX_test['y_pred_test'] = y_pred_test\n\nmean_values = X_test.groupby('loan_intent').agg({'y_test': 'mean', 'y_pred_test': 'mean'})\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nbar_width = 0.35\nindex = np.arange(len(mean_values))\n\nax.bar(index, mean_values['y_test'], bar_width, label='Actual')\nax.bar(index + bar_width, mean_values['y_pred_test'], bar_width, label='Predicted')\n\nax.set_xlabel('Loan Intent')\nax.set_ylabel('Mean Values')\nax.set_title('Mean of y_test and y_pred_test Values for Each Loan Intent')\nax.set_xticks(index + bar_width / 2)\nax.set_xticklabels(mean_values.index)\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the actual and predicted mean rates of default based on loan intent. The higher a column is means the higher the chance of default, or the more data points where a default occurred (blue) or was predicted (orange), as defaulting is represented by 1 in this data set. We can see that medical loans have the highest predicted rate of default among all types of loan intent, which means medical loans are the least likely to be granted of any type of loan, or in other words, the most difficult to attain. In this category, its just a bit of an over-prediction of the actual rate of default. On the other hand, the rate of default for business ventures and education loans is steeply over predicted by the model compared to the actual rate.\n\nX_test['inc_percentile'] = round(X_test['person_income'].rank(pct = True), 2) * 100\ngrouped_data = X_test.groupby(['inc_percentile'])['y_pred_test'].mean()\ngrouped_data = pd.DataFrame(grouped_data)\n\nax = sns.scatterplot(data=grouped_data, x='y_pred_test', y='inc_percentile')\nax.set(xlabel = \"Predicted Default Rate\")\nax.set(ylabel = \"Income Percentile\")\n\n\n\n\n\n\n\n\nThis is… well, more or less the trend I expected. As the income percentile increases, so does the chance that the bank grants them a loan. This is seen by the more or less inverse relationship on the plot above.\nAs mentioned earlier, it getting a medical loan can be challenging. In my opinion (supported by the middle view of fairness of Barocas, Hardt, and Narayanan), this is unfair. As they stated, “this view holds that the decision makers have an obligation to avoid perpetuating injustice.” Often times, individuals are born with medical conditions. This is completely out of their control, and doesn’t necessarily guarantee that they possess the economic capital to pay for continuous treatment. In fact, I’d assume that there is a higher chance of being born into a high-risk medical group in lower income brackets, and by this logic, the middle view of fairness is blatantly violated by the difficulty to access medical loans regardless of whether they are paid back.\nIn this blog post, I learned a lot about how difficult it can be to manipulate data with groupby functions to create effective plots. Keeping the number of points to a minimum can help make a clear point and avoid diluting with substantial amounts of noise. The technique I used was splitting data up into percentiles. Then I explored the data, and with recursive feature elimination to determined which features to model (income, loan amount, and loan interest rate) and based off this model, I found a good threshold. After running the model on the test set, I observed that younger folks have more difficulty getting a loan (which makes some sense given their increased risk of default), that medical loans are the most difficult to get, and that business venture and education loans are the categories in which the bank most over-predicts the chance of default. Also, a higher income is a strong contributing factor to being granted a loan."
  },
  {
    "objectID": "posts/BP1/BP1.html",
    "href": "posts/BP1/BP1.html",
    "title": "Blog Post: Classifying Palmer Penguins",
    "section": "",
    "text": "Blog Post: Classifying Palmer Penguins\nIn this blog post, I aim to gain familiarity with pandas tools used for machine learning. I will be exploring a data set about penguins, and attempting to classify them by species based on individual quantitative and qualitative attributes. First, I will take a look at the data and clean it so that it is ready for use. Then I will get familiar with the data through graphical representations, in an effort to make the best decision about which features to use in modelling. Using a reproducible process, I will determine which 3 attributes (1 qualitative and 2 quantitative) should be used for the model. Cross-validation on the training data will be tested with several different models to determine the best model before moving on to the test data. Once the best model is determined, it will be run on the test data, with a goal of 100% accuracy of classification. To analyze the work, I will plot the species regions for both test and training data, and look at a confusion matrix before summing up my findings.\nBefore beginning analysis, the training data must be accessed.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nBelow is a sample of what it looks like:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nClearly, this is a LOT of data. Furthermore, not all of it is super useful. For example, the studyName and Sample Number columns aren’t meaningful to this analysis. Some other columns, like Sex and Island need to be reformatted to allow for analysis. Now, I will clean the data.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.fit_transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\n\n\nExplore\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = X_train['Flipper Length (mm)']\ny = X_train['Body Mass (g)']\n\nax = sns.scatterplot(data = X_train, x = 'Flipper Length (mm)', y = 'Body Mass (g)', hue = y_train)\n\nh,l = ax.get_legend_handles_labels()\n\nl = [\"Gentoo Penguin\", \"Chinstrap Penguin\", \"Adelie Penguin\"]\n\nax.legend(h, l)\nplt.show()\n\n\n\n\n\n\n\n\nTo be honest, I started out with some quantitative attributes that seemed interesting. Tossing them onto a graph organized by species seems to be a good way to get a feel of whether they could successfully be used as classifiers. While the combination of body mass and flipper length looks like it may be able to identify (2) penguins, it would be extremely difficult to differentiate penguins of species (0) and (1). Thus, I determined I should continue searching for more ideal attributes prior to modeling.\n\nx = X_train['Delta 15 N (o/oo)']\ny = X_train['Delta 13 C (o/oo)']\n\nax = sns.scatterplot(data=X_train, x = 'Delta 15 N (o/oo)', y = 'Delta 13 C (o/oo)', hue = y_train)\n\nh,l = ax.get_legend_handles_labels()\n\nl = [\"Gentoo Penguin\", \"Chinstrap Penguin\", \"Adelie Penguin\"]\n\nax.legend(h, l)\nplt.show()\n\n\n\n\n\n\n\n\nI threw a few new quantitative attributes on a graph to get an idea of how they compared. Once again, looking at the graph, I don’t have super high hopes that these would be successful as classifiers. It does appear that (1) penguins have a higher Delta 13 C (o/oo) than other species. (0) penguins overlapped with (1) penguins heavily for Delta 15 N (o/oo). (0) penguins also overlap heavily for Delta 13 C (o/oo) with (2) penguins, leading to a prediction that classification would be difficult.\n\nX_train.groupby(y_train)[['Island_Biscoe', 'Island_Dream', 'Island_Torgersen']].mean()\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\n\n\n\n\n0\n0.305556\n0.37963\n0.314815\n\n\n1\n0.000000\n1.00000\n0.000000\n\n\n2\n1.000000\n0.00000\n0.000000\n\n\n\n\n\n\n\nHere, I felt like I hit the jackpot. It seems that (1) penguins were only found on Island_Dream, and (2) pengiuns were only found on Island_Biscoe. Although (0) penguins are spread equally among all three islands, this simplifies the problem significantly. However, it is still important to run some tests and be aware of any possible overfitting.\nWith a few interesting graphics, and a summary table, it’s time to start modeling.\n\n\nModeling\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\n\nx_new = SelectKBest(mutual_info_classif, k = 3)\nx_new.fit_transform(X_train, y_train)\nx_new.get_feature_names_out()\n\narray(['Culmen Depth (mm)', 'Flipper Length (mm)', 'Delta 13 C (o/oo)'],\n      dtype=object)\n\n\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.001, penalty=\"l1\", dual=False).fit(X_train, y_train)\nmodel = SelectFromModel(lsvc, prefit=True).fit(X_train, y_train)\nx_new = model.transform(X_train)\nmodel.get_feature_names_out()\n\nc:\\Users\\Zoe Greenwald\\anaconda3\\envs\\ml-0451-2\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\narray(['Culmen Length (mm)', 'Flipper Length (mm)', 'Body Mass (g)'],\n      dtype=object)\n\n\nAfter trying two methods of features selection and getting only quantitative features, I’m going to give the brute force method a shot.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nqual_cols = [\"Island\", \"Clutch Completion\", \"Stage_adult\", \"Sex\"]\nquant_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nbest_score = 0.0\nbest_cols = []\n\nfor qual in qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression(solver='lbfgs', max_iter = 10000)\n    LR.fit(X_train[cols], y_train)\n    score = LR.score(X_train[cols], y_train)\n\n    if score &gt; best_score:\n      best_score = score\n      best_cols = cols\n\n# puts the qualitative columns at the end \nbest_cols = best_cols[::-1]\n\nprint(best_cols)\nprint(best_score)\n\n['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Torgersen', 'Island_Dream', 'Island_Biscoe']\n0.99609375\n\n\nSuccess! I may have forced it by looking only at groups of 3 that included 1 qualitative column, but I’ve managed to reproducibly find the columns I want to use for my modelling. Culmen depth, culmen length, and island seem to have a high success rate for classifying the three penguin species.\n\ncv_scores_LR = cross_val_score(LR, X_train[best_cols], y_train, cv=5).mean()\nprint(cv_scores_LR)\n\n0.9883107088989442\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\ndepths = [*range(3, 20)]\ncv_scores_dtc = []\n\nfor val in depths:\n    dtc = DecisionTreeClassifier(criterion=\"gini\", max_depth=val).fit(X_train[best_cols], y_train)\n    cv_scores_dtc.append(cross_val_score(dtc, X_train[best_cols], y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores_dtc)\nbest_depth = depths[np.argmax(cv_scores_dtc)]\nprint(best_score)\nprint(best_depth)\n\n0.9765460030165913\n7\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrees = [*range(10, 100)]\ncv_scores_rf = []\n\nfor val in trees:\n    rf = RandomForestClassifier(n_estimators = val).fit(X_train[best_cols], y_train)\n    cv_scores_rf.append(cross_val_score(rf, X_train[best_cols], y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores_rf)\nbest_depth = trees[np.argmax(cv_scores_rf)]\nprint(best_score)\nprint(best_depth)\n\n0.9843891402714933\n11\n\n\n\n\nTesting\nLooking at the scores of the best fits of all the tested models, it looks like LR pulls ahead by just a fraction of a percent, so I’ll go ahead and try that on the test data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR.fit(X_train[best_cols], y_train)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nWe did it! With enough modeling and praying, we got 100% accuracy on the test data! Woohoo!!!\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"upper right\", bbox_to_anchor = (2.5, 0.75))\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[best_cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\nTo finish up, lets take a look at the confusion matrix to see what kind of mistakes our model made (even though our model didn’t make any mistakes on the test data).\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[best_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]], dtype=int64)\n\n\nGiven that our model had 100% accuracy on the test set, this was the confusion matrix I was expecting. The only non-zero numbers are along the diagonal, signifying that every penguin was correctly classified as its own species.\nTo close out this first blog post, I will first discuss my results, and then my learnings. I found that I was able to use machine learning on training data to prepare an group of features and model to successfully classify penguins into their respective species group. The optimal features ended up being culmen length, culmen depth, and what island the penguin was found on, and I determined the ideal model to be linear regression after using cross-validation on several different models before applying the algorithm to the test data. I was excited to meet Phil’s request for 100% accuracy on the test data. Some further analysis was done to compare the decision regions and their boundaries between the train and test data sets. I learned a lot about pandas feature selections and models in this assignment, along with the basic flow of using machine learning in a data science project."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Some new text here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog Post: Classifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 11: Deep Music Genre Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: ‘Optimal’ Decision Making\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Replication Study\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Women in Data Science\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Perceptron\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Newton’s Method for Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Blog Post: Skin Cancer Classification with LogisticRegression and CNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/BP11/BP11.html",
    "href": "posts/BP11/BP11.html",
    "title": "Blog Post 11: Deep Music Genre Classification",
    "section": "",
    "text": "Blog Post 11: Deep Music Genre Classification\nWell, I’ve been hoping to do a deep learning blog post all semester, so I’m really excited to give this one a shot! The first step will be to properly format the data, and then perform text vectorization, since lyrics are words, which cannot function as features. Then, I will collate batches. Finally, I will be prepared to create a model and evaluate its accuracy. I will create 3 models, one learning from just the lyrics, one learning from just the features, and one learning from both. Once the models have successfully performed better than the base rate, I will compare the accuracy across the three models to see which one would be the most promising to continue to work with.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy'] \n\nAs our target variable, I’ll be converting the genre column into numbers so that a neural net can deal with it.\n\nprint(df[\"genre\"].unique())\n\ngenres = {'pop': 0, 'country': 1, 'blues': 2, 'jazz': 3, 'reggae': 4, 'rock': 5, 'hip hop': 6}\ndf[\"genre\"] = df[\"genre\"].apply(genres.get)\n\n# check to make sure all the genres were successfully converted to numbers\nprint(df[\"genre\"].unique())\n\ndf.head(3)\n\n['pop' 'country' 'blues' 'jazz' 'reggae' 'rock' 'hip hop']\n[0 1 2 3 4 5 6]\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\n0\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\n0\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\n0\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n\n\n3 rows × 31 columns\n\n\n\n\ndf.groupby(\"genre\").size() / len(df)\n\ngenre\n0    0.248202\n1    0.191915\n2    0.162273\n3    0.135521\n4    0.088045\n5    0.142182\n6    0.031862\ndtype: float64\n\n\nLooks like if we guess genre 0 (pop) every time, we will get a base accuracy of 24.8%. Let’s see if we an do better than this. Since we’re going to start out using just the lyrics for classification, I’m going to go ahead and vectorize those.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n    \n    # returns the target, lyrics, and features separately\n    def __getitem__(self, index):\n        target = self.df['genre'].iloc[index]\n        lyrics = self.df['lyrics'].iloc[index]\n        features = self.df[engineered_features].iloc[index]\n        return target, lyrics, features\n\n    def __len__(self):\n        return len(self.df)                \n\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_val = train_test_split(df, shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data   = TextDataFromDF(df_val)\n\nHere’s a sample of what one piece of data looks like:\n\ntrain_data[193]\n\n(4,\n 'ballistic affair ballistic affair lick chalice cook ital play football cricket brother true rest jungle block rema fight gainst brother right sister live yeah throw throw knife unite live fear ballistic affair true black outta east rest come best ballistic affair tell brother ballistic affair ballistic affair tell sister ballistic affair throw throw knife unite live fear ballistic affair true black outta east rest come best ballistic affair tell brother ballistic affair ballistic affair tell sister ballistic affair ballistic affair ballistic affair throw throw knife unite throw throw knife unite hear brother',\n dating                      0.001144\n violence                    0.172638\n world/life                  0.181385\n night/time                  0.001144\n shake the audience          0.024042\n family/gospel               0.183652\n romantic                    0.001144\n communication               0.092167\n obscene                     0.331242\n music                       0.001144\n movement/places             0.001144\n light/visual perceptions    0.001144\n family/spiritual            0.001144\n like/girls                  0.001144\n sadness                     0.001144\n feelings                    0.001144\n danceability                0.893859\n loudness                    0.423173\n acousticness                0.426706\n instrumentalness            0.000052\n valence                     0.862943\n energy                      0.293271\n Name: 21143, dtype: float64)\n\n\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\ntokenized = tokenizer(train_data[193][1])\n\n\ndef yield_tokens(data_iter):\n    for target, text, _ in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"&lt;unk&gt;\"], min_freq = 50)\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\n\nimport torch\n\nnum_tokens = len(vocab.get_itos())\nmax_len = 30\n\n# converts text (like lyrics!) into numbers (which a neural net can use!)\ndef text_pipeline(x):\n    tokens = vocab(tokenizer(x))\n    y = torch.zeros(max_len, dtype=torch.int64) + num_tokens\n    if len(tokens) &gt; max_len:\n        tokens = tokens[0:max_len]\n    y[0:len(tokens)] = torch.tensor(tokens,dtype=torch.int64)\n    return y\n\nlabel_pipeline = lambda x: int(x)\n\n\nimport numpy as np\n\ndef collate_batch(batch):\n    target_label_list = []\n    lyric_text_list = []\n    features_list = []  \n\n    for (_targets, _lyrics, _features) in batch:\n        # process targets\n        target_label_list.append(label_pipeline(_targets))\n\n        # process lyrics\n        processed_lyrics = text_pipeline(_lyrics)\n        lyric_text_list.append(processed_lyrics)\n\n        # process features\n        features_list.append(_features.to_numpy())\n\n    # turn it all into tensors\n    target_label_list = torch.tensor(target_label_list, dtype=(torch.int64))\n    lyric_text_list = torch.stack(lyric_text_list)\n    features_list = torch.tensor((features_list), dtype=torch.float64)\n\n    return target_label_list, lyric_text_list, features_list\n\n\n#train set and validation set\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\n\n\nnext(iter(train_loader))\n\nC:\\Users\\Zoe Greenwald\\AppData\\Local\\Temp\\ipykernel_22856\\3029876209.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n  features_list = torch.tensor((features_list), dtype=torch.float64)\n\n\n(tensor([2, 3, 0, 0, 3, 0, 0, 5]),\n tensor([[  18,   18,   49,   49,   49,   49,  127,  178,   18,  108,   49,   49,\n            49,   49,  655,    6,   77,    6,    6,   24,   18,   49,   49,   49,\n            49,   49,   18,   18, 2911, 2911],\n         [  21,    5,  334,    5,   21,    5,  334, 2911, 2911, 2911, 2911, 2911,\n          2911, 2911, 2911, 2911, 2911, 2911, 2911, 2911, 2911, 2911, 2911, 2911,\n          2911, 2911, 2911, 2911, 2911, 2911],\n         [ 666,   28,  666,   29,  247, 1755,  151,   14,  629,  255,   41,    7,\n           255,   41,    7,  171,  141,    1,   29,    1,  666,  108,   14,  629,\n           255,   41,    7,  255,   41,    7],\n         [1259, 2404,  598,  474,   48, 2759,  462, 2404,   50,  457,    0,  283,\n           224,  387,  472,   17,  790,  433,  677,  801,    0,  339,   31,   37,\n            31,    0,   76,   27,  239,   18],\n         [ 519,  276,  394,  519,  904,    6,  519,    0,  932,    0,  460, 2584,\n          1003,  550,    0,    1, 1545,  121,   18,  519,    0,  394,  519,  877,\n            32,  990,  292, 1035,   11, 1109],\n         [ 248,  229,  428,  206,   49, 1104,  892,   17,   10,   91,    0,   10,\n             1, 1266,  493,   34,   26,  167,   98,    5,   90,  309,   20,  175,\n             3,    4,  352,   28,  289,  214],\n         [  25,  456,  456,  456,    6,    4,  456,   20,    6,  131,  456,  182,\n           154,    6, 2353,   55,   52,   28,   25,  456,  456,  456,    6,    4,\n           456,   20,    6,  131,  456,  182],\n         [ 155,    6,  120,  473,   16,   16,    1,    1,    1,   48,   23,   16,\n            90,   16,  152,   39,   74,    0,  136,   16,   16,   48,   23,   16,\n            90,   16, 2911, 2911, 2911, 2911]]),\n tensor([[1.8149e-03, 1.8149e-03, 4.4747e-01, 1.8149e-03, 1.8149e-03, 1.8149e-03,\n          1.8149e-03, 3.2864e-01, 1.8149e-03, 1.8149e-03, 1.8149e-03, 1.8149e-03,\n          1.8149e-03, 1.8149e-03, 1.9484e-01, 1.8149e-03, 2.4727e-01, 6.6808e-01,\n          9.7189e-01, 5.5061e-06, 2.7453e-01, 2.9527e-01],\n         [6.5789e-03, 6.5789e-03, 6.5789e-03, 2.4702e-01, 6.5789e-03, 6.5789e-03,\n          6.5789e-03, 2.6824e-01, 6.5789e-03, 6.5789e-03, 6.5789e-03, 6.5789e-03,\n          6.5789e-03, 6.5789e-03, 6.5789e-03, 3.7947e-01, 7.6389e-01, 4.4422e-01,\n          9.2269e-01, 8.4008e-01, 6.3829e-01, 2.5824e-01],\n         [1.1962e-03, 4.8448e-02, 1.1962e-03, 5.5294e-02, 1.1962e-03, 1.1962e-03,\n          1.1962e-03, 1.9763e-01, 1.1962e-03, 1.1962e-03, 1.1962e-03, 1.1962e-03,\n          1.1962e-03, 1.1962e-03, 6.0631e-01, 1.1962e-03, 4.3789e-01, 8.2653e-01,\n          1.1344e-02, 4.7166e-03, 4.9093e-01, 9.7898e-01],\n         [7.9745e-04, 7.9745e-04, 3.3189e-02, 7.9745e-04, 7.9745e-04, 1.3136e-01,\n          4.1652e-01, 7.9745e-04, 7.9745e-04, 6.9069e-02, 7.9745e-04, 2.3674e-01,\n          7.9745e-04, 7.9745e-04, 7.9745e-04, 7.9745e-04, 3.8048e-01, 5.7737e-01,\n          7.9418e-01, 1.5182e-06, 3.3533e-01, 3.1329e-01],\n         [1.4620e-03, 5.4061e-01, 1.4620e-03, 6.6774e-02, 1.4620e-03, 1.4620e-03,\n          1.4620e-03, 1.4620e-03, 1.9349e-01, 1.4620e-03, 1.4620e-03, 1.4620e-03,\n          1.4620e-03, 9.5524e-02, 8.3130e-02, 1.4620e-03, 5.2345e-01, 5.7729e-01,\n          7.8414e-01, 0.0000e+00, 5.9398e-01, 3.6034e-01],\n         [7.4000e-02, 8.6958e-02, 4.0347e-01, 1.0971e-01, 1.0741e-03, 1.0741e-03,\n          1.0741e-03, 1.0742e-03, 1.1433e-01, 3.5531e-02, 1.1523e-01, 1.0741e-03,\n          2.3605e-02, 1.0741e-03, 1.0741e-03, 2.6443e-02, 4.6388e-01, 6.6413e-01,\n          2.1385e-01, 7.8543e-05, 4.4456e-01, 3.9738e-01],\n         [3.3501e-02, 1.3850e-03, 1.3850e-03, 1.3850e-03, 2.8108e-02, 1.3850e-03,\n          1.3850e-03, 1.3850e-03, 1.3850e-03, 1.3850e-03, 1.3850e-03, 1.3850e-03,\n          1.3850e-03, 1.3850e-03, 9.1623e-01, 1.3850e-03, 2.4402e-01, 6.3190e-01,\n          8.0422e-01, 1.8117e-05, 4.1467e-01, 3.9337e-01],\n         [2.0243e-03, 4.6580e-02, 7.6299e-01, 2.0243e-03, 2.0243e-03, 2.0243e-03,\n          2.0243e-03, 7.0791e-02, 2.0243e-03, 2.0243e-03, 2.0243e-03, 2.0243e-03,\n          2.0243e-03, 2.0243e-03, 5.0386e-02, 2.0243e-03, 3.6315e-01, 5.2188e-01,\n          2.7410e-01, 1.0931e-04, 1.7045e-01, 3.6835e-01]], dtype=torch.float64))\n\n\nOk - happy to see that everything was successfully vectorized. Now we can go for model building. #### Experiment 1: Just the Lyrics Now that I’ve vectorized all the lyrics, its time to do some deep learning. I’m going to start out by using a model that learns from only the lyrics.\n\nfrom torch import nn\n\nclass LyricsNet(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, max_len, num_class, dropout_p):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc = nn.Linear(embedding_dim, num_class)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        x = x.mean(axis = 1)\n        x = self.fc(x)\n        x = self.dropout(x)\n        return(x)\n\n\nvocab_size = len(vocab)\nembedding_dim = 3\nnum_class = 7\ndropout_p = 0.2\nmax_len = 150\nlyrics_model = LyricsNet(vocab_size, embedding_dim, max_len, num_class, dropout_p)\n\noptimizer = torch.optim.Adam(lyrics_model.parameters(), lr = 0.1)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nimport time\n\ndef train(model, dataloader, d_type):\n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n\n    for idx, (label, text, features) in enumerate(dataloader):\n        # zero gradients\n        optimizer.zero_grad()\n\n        if d_type == 'lyrics':\n            input_data = text\n        elif d_type == 'features':\n            input_data = features\n        else:  \n            # both lyrics and features\n            input_data = (text.int(), features.float())\n\n        # form prediction on batch\n        predicted_label = model(input_data)\n\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, label)\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n\n        # for printing accuracy\n        total_acc   += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n        \n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n    \ndef evaluate(model, dataloader, d_type):\n\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (label, text, features) in enumerate(dataloader):\n\n            if d_type == 'lyrics':\n                input_data = text\n            elif d_type == 'features':\n                input_data = features\n            else:  \n                # both lyrics and features\n                input_data = (text.int(), features.float())\n\n            predicted_label = model(input_data)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc/total_count\n\n\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    train(lyrics_model, train_loader, 'lyrics')\n\n| epoch   1 | train accuracy    0.267 | time: 42.29s\n| epoch   2 | train accuracy    0.310 | time: 41.89s\n| epoch   3 | train accuracy    0.327 | time: 42.09s\n| epoch   4 | train accuracy    0.335 | time: 42.10s\n| epoch   5 | train accuracy    0.343 | time: 42.28s\n| epoch   6 | train accuracy    0.345 | time: 42.29s\n| epoch   7 | train accuracy    0.349 | time: 42.23s\n| epoch   8 | train accuracy    0.353 | time: 42.07s\n| epoch   9 | train accuracy    0.353 | time: 42.22s\n| epoch  10 | train accuracy    0.351 | time: 42.10s\n\n\nAlright nice - solidly 10% above the base rate. We take what we can get.\n\nevaluate(lyrics_model, val_loader, 'lyrics')\n\n0.29427312775330394\n\n\nAnd we’re passing on the validation data too! Looks like there is definitely some merit to categorizing music into genres based on lyrics. Let’s check out whether this applies to the engineered features as well. #### Experiment 2: Engineered Features Now we’re going to give it a shot with just the engineered features. It’ll be interesting to compare the two results.\n\nimport torch.nn as nn\n\nclass FeaturesNet(nn.Module):\n    def __init__(self, input_size, num_class):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Linear(16, num_class),\n            nn.Softmax(dim = 1)\n        )\n\n\n    def forward(self, x):\n        x = x.float()\n        x = torch.flatten(x, 1)\n        x = self.model(x)\n        return x\n\n\ninput_size = 22\nfeatures_model = FeaturesNet(input_size, num_class)\n\noptimizer = torch.optim.Adam(features_model.parameters(), lr = 0.0001)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    train(features_model, train_loader, 'features')\n\n| epoch   1 | train accuracy    0.268 | time: 45.91s\n| epoch   2 | train accuracy    0.315 | time: 46.12s\n| epoch   3 | train accuracy    0.340 | time: 46.45s\n| epoch   4 | train accuracy    0.343 | time: 46.13s\n| epoch   5 | train accuracy    0.345 | time: 46.19s\n| epoch   6 | train accuracy    0.347 | time: 46.20s\n| epoch   7 | train accuracy    0.348 | time: 46.38s\n| epoch   8 | train accuracy    0.350 | time: 46.12s\n| epoch   9 | train accuracy    0.350 | time: 46.20s\n| epoch  10 | train accuracy    0.350 | time: 46.43s\n\n\nWow, almost exactly the same as lyrics! Still 10% better than the base rate.\n\nevaluate(features_model, val_loader, 'features')\n\n0.34149779735682817\n\n\nIt looks like it held up a lot better on the validation set, maintaining abut 10% better than the base rate, so maybe using the engineered features can help avoid overfitting.\n\nExperiment 3: Lyrics and Features\nLet’s see what happens if we combine both the lyrics and the features in one neural net.\n\nclass CombinedNet(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, input_size, num_class):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.text_fc = nn.Linear(embedding_dim, 64)\n\n        self.feature_fc = nn.Linear(input_size, 64)\n\n        self.combine_fc = nn.Linear(9664, 32)\n        self.output_fc = nn.Linear(32, num_class)\n    \n    def forward(self, x):\n        x_1, x_2 = x\n\n        # text pipeline: try embedding! \n        x_1 = self.embedding(x_1)\n        x_1 = x_1.float()\n        x_1 = self.text_fc(x_1)\n        x_1 = torch.flatten(x_1, 1)\n\n        # engineered features: fully-connected Linear layers are fine\n        x_2 = self.feature_fc(x_2)\n\n        # ensure that both x_1 and x_2 are 2-d tensors, flattening if necessary\n        # then, combine them with: \n        combo = torch.cat((x_1, x_2), 1)\n\n        # pass x through a couple more fully-connected layers and return output\n        combo = self.combine_fc(combo)\n        output = self.output_fc(combo)\n\n        return output\n\n\ninput_size = 22\nvocab_size = len(vocab)\nembedding_dim = 3\nnum_class = 7\ncombined_model = CombinedNet(vocab_size, embedding_dim, input_size, num_class)\n\noptimizer = torch.optim.Adam(combined_model.parameters(), lr = 0.0001)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    train(combined_model, train_loader, 'both')\n\n| epoch   1 | train accuracy    0.246 | time: 50.05s\n| epoch   2 | train accuracy    0.301 | time: 48.55s\n| epoch   3 | train accuracy    0.327 | time: 51.65s\n| epoch   4 | train accuracy    0.344 | time: 51.23s\n| epoch   5 | train accuracy    0.355 | time: 48.90s\n| epoch   6 | train accuracy    0.364 | time: 50.54s\n| epoch   7 | train accuracy    0.371 | time: 54.80s\n| epoch   8 | train accuracy    0.373 | time: 55.53s\n| epoch   9 | train accuracy    0.380 | time: 52.73s\n| epoch  10 | train accuracy    0.381 | time: 54.73s\n\n\nAlright - 14% better than the base rate. That’s the best accuracy we’ve seen yet.\n\nevaluate(combined_model, val_loader, 'both')\n\n0.35577092511013214\n\n\nA quick reminder that the base rate here is 24.8%, which means… we’re doing awesome! 10% over the base rate is definitely a success, that means we can do substantially better than if we were to guess the most frequently occurring category every time. Although it was only a bit better than the models just using the lyrics or the model just using the features (~1% on the validation set and 3% on the training set), I think it makes sense that performance was overall improved. #### Some interesting visualizations\n\nfrom matplotlib import pyplot as plt \n\ndanceability_over_time = df.groupby('release_date')['danceability'].mean()\nplt.plot(danceability_over_time.index, danceability_over_time.values)\nplt.xlabel('Year')\nplt.ylabel('Average Predicted Danceability')\nplt.title('Predicted Danceability of Pop Music Over Time')\nplt.show()\n\n\n\n\n\n\n\n\nLooks like music started out fairly danceable, suffered a danceability crisis in the mid 50’s and since then has been improving steadily, to reaching the current maximum of 62% in 2020.\n\nimport matplotlib.pyplot as plt\n\nreverse_genres = {v: k for k, v in genres.items()}\n# Reverse the mapping in the 'genre' column\ndf['genre'] = df['genre'].map(reverse_genres)\n\ngenre_analysis = df.groupby('genre')[['sadness', 'energy']].mean()\nprint(genre_analysis)\n\n# Plot average sadness scores\nplt.figure(figsize=(10, 5))\ngenre_analysis['sadness'].sort_values().plot(kind='bar')\nplt.xlabel('Genre')\nplt.ylabel('Average Sadness')\nplt.title('Average Sadness by Genre')\nplt.show()\n\n# Plot average energy scores\nplt.figure(figsize=(10, 5))\ngenre_analysis['energy'].sort_values().plot(kind='bar')\nplt.xlabel('Genre')\nplt.ylabel('Average Energy')\nplt.title('Average Energy by Genre')\nplt.show()\n\n          sadness    energy\ngenre                      \nblues    0.113511  0.581534\ncountry  0.165922  0.466350\nhip hop  0.036589  0.703236\njazz     0.124067  0.463430\npop      0.142083  0.601097\nreggae   0.078312  0.589931\nrock     0.133539  0.700954\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterestingly enough, blues is actually ranked on the lower end for sadness. Country takes the win on that one. For energy, rock and hip-hop at the upper end of the scale totally makes sense.\nWell, that’s the deep learning blog post! This was something I was really excited about learning since last winter when I wasn’t able to take the deep learning course. I’m very pleased I’ve finally had the chance to learn a bit about it. I’ve certainly enjoyed the absolute time sink of watching the epochs go by with my fingers crossed. I was a bit surprised that the features model performed better on the validation set than the lyrics model did - spotify clearly knew what it was doing by calculating those features. It did make sense that the combined model slightly outperformed either individual model on both the training set and the validation set, as it combined learning from the lyrics and features model. Finally, to speak about the interesting figures above, I was honestly not so surprised that the danceability of music has been increasing (the youth is a little obsessed with dancing). However, I was surprised to see how low blues ranked in terms of sadness. Same with hip-hop; although it might not be sadness in a spotify ranked way, I feel like there are often pretty dark themes in hip-hop.\nI definitely hope to learn more about deep learning in the future, and who knows, maybe one day I will finally make that deep learning model for chess."
  },
  {
    "objectID": "posts/BP3/BP3.html",
    "href": "posts/BP3/BP3.html",
    "title": "Blog Post: Replication Study",
    "section": "",
    "text": "In this blog post, I seek to replicate the findings of Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” They give into a study of whether medical data used to qualify high-risk patients for additional automatic support from the health care system is biased. I am to reproduce several of their figures showing the discrepancies between black and white patients with regards to who qualifies as “high-risk” thus being considered or automatically entered into the aforementioned program, and costs between black and white patients for the same risk-score as well as number of chronic conditions. Finally, I will perform a linear regression to determine how much less the average black patient incurs in health costs, and discuss why this is problematic, and which principle of fairness is violated.\n\n\n\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\ndf_imp = df[[\"risk_score_t\", \"cost_t\", \"race\", \"gagne_sum_t\"]].copy()\ndf_imp.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n\n\n1\n7.677934\n2600.0\nwhite\n3\n\n\n2\n0.407678\n500.0\nwhite\n0\n\n\n3\n0.798369\n1300.0\nwhite\n0\n\n\n4\n17.513165\n1100.0\nwhite\n1\n\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_imp['risk_percentile'] = round(df['risk_score_t'].rank(pct = True), 2) * 100\n\ngrouped_data = df_imp.groupby(['risk_percentile', 'race'])['gagne_sum_t'].mean()\n\ngrouped_data = pd.DataFrame(grouped_data)\n\nsns.scatterplot(data=grouped_data, x='gagne_sum_t', y='risk_percentile', hue='race')\n\nplt.xlabel('Mean Number of Chronic Illnesses')\nplt.ylabel('Percentile Risk Score')\nplt.title('Mean number of chronic illnesses versus algorithm predicted risk by race')\n\nplt.legend(title='Race')\nplt.show()\n\n\n\n\n\n\n\n\nThis plot begins to unravel some underlying racial bias that could be present in the algorithm. As seen in the figure, the orange dots representing white patients lie on a higher curve of percentile risk score. This suggests that less sick white patients are more likely to be referred to a high-risk program than sicker black patients.\n\n\n\n\nfig, axes = plt.subplots(1, 2, sharey = True)\n\ngrouped_cost_data = pd.DataFrame(df_imp.groupby(['risk_percentile', 'race'])['cost_t'].mean())\ngrouped_illness_data = pd.DataFrame(df_imp.groupby(['gagne_sum_t', 'race'])['cost_t'].mean())\n\nsns.scatterplot(ax = axes[0], data=grouped_cost_data, x='risk_percentile', y='cost_t', hue='race')\nsns.scatterplot(ax = axes[1], data=grouped_illness_data, x='gagne_sum_t', y='cost_t', hue='race')\n\naxes[0].set_ylabel('Total Medical Expenditure')\naxes[0].set_xlabel('Percentile Risk Score')\naxes[1].set_xlabel('Number of Chronic Illnesses')\n\nplt.ylim(600, 120000)\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\nFrom this chart, we can see that holding either percentile risk score or number of chronic illnesses constant, black patients have lower costs than white patients. Further to the right on each graph, the amount of data available drops off steeply, which explains the more erratic correlation.\n\n\n\n\n\n\ngreater_than_5 = round((df_imp['gagne_sum_t'] &gt;= 5).mean() * 100, 2)\ngreater_than_5\n\n7.0\n\n\nGiven that 93% of the patients in this data set have 5 or less chronic conditions, it seems reasonable to focus on them, at least as a starting point. It is still important to analyze trends for patients with more chronic illnesses, but in the scope of this assignment, it could downplay trends, and without sufficient data, could display incorrect trends.\n\nimport numpy as np\n\ndf_not_0 = df_imp.drop(df_imp[df_imp['cost_t'] == 0].index)\ndf_not_0['log_cost'] = np.log(df_not_0['cost_t'])\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\nrisk_percentile\nlog_cost\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n35.0\n7.090077\n\n\n1\n7.677934\n2600.0\nwhite\n3\n86.0\n7.863267\n\n\n2\n0.407678\n500.0\nwhite\n0\n4.0\n6.214608\n\n\n3\n0.798369\n1300.0\nwhite\n0\n11.0\n7.170120\n\n\n4\n17.513165\n1100.0\nwhite\n1\n98.0\n7.003065\n\n\n\n\n\n\n\n\ndf_not_0 = pd.get_dummies(df_not_0)\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_black\nrace_white\n\n\n\n\n0\n1.987430\n1200.0\n0\n35.0\n7.090077\n0\n1\n\n\n1\n7.677934\n2600.0\n3\n86.0\n7.863267\n0\n1\n\n\n2\n0.407678\n500.0\n0\n4.0\n6.214608\n0\n1\n\n\n3\n0.798369\n1300.0\n0\n11.0\n7.170120\n0\n1\n\n\n4\n17.513165\n1100.0\n1\n98.0\n7.003065\n0\n1\n\n\n\n\n\n\n\n\nX_train = df_not_0[['gagne_sum_t', 'race_black']].rename(columns={'race_black' : 'race'})\ny_train = df_not_0[['log_cost']]\nX_train.head(10)\n\n\n\n\n\n\n\n\ngagne_sum_t\nrace\n\n\n\n\n0\n0\n0\n\n\n1\n3\n0\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n1\n0\n\n\n5\n1\n0\n\n\n6\n1\n0\n\n\n7\n0\n0\n\n\n8\n1\n1\n\n\n9\n0\n0\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nLR = LinearRegression()\nLR.fit(X_train, y_train)\n\n# adds {degree} polynomial features to {X}\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nNow, I will add a variable number of features to X_train and see which produces the best accuracy with cross-validation.\n\ncv_scores = []\n\n# I will check how adding up to 10 additional polynomial features impacts the score\nfor i in range(0, 10):\n    X_deg = add_polynomial_features(X_train, i)\n    cv_scores.append(cross_val_score(LR.fit(X_deg, y_train), X_deg, y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores)\nbest_deg = np.argmax(cv_scores)\n\nprint(best_score)\nprint(best_deg)\n\n0.14820529981684197\n9\n\n\nIt looks like a polynomial with degree 9 was most successful with this data, so I will fit one additional linear regression model with this degree before proceeding.\n\nbest = add_polynomial_features(X_train, best_deg)\n\nLR.fit(best, y_train)\nw = LR.coef_\nprint(w)\n\n[[ 3.92483807e-01 -2.67357533e-01  3.92483807e-01 -5.63208481e-01\n   2.48313944e-01 -5.61461922e-02  7.08781442e-03 -5.05267129e-04\n   1.89805627e-05 -2.91722116e-07]]\n\n\n\n# as the coefficients are in the order of the data input\n# the second weight should correspond to the race column\n# this can be used to calculate the estimate cost incurred by black patients as a percent of that incurred by white patients\nb_coef = w[0, 1]\nb_coef\n\n-0.26735753310360466\n\n\n\ncost_incurred = np.exp(b_coef)\ncost_incurred\n\n0.7653993669279184\n\n\nSo, it looks like black patients, on average, pay about 3/4 of what white patients pay for health care. This is more or less following the argument of Obermeyer et al. (2019). I note that my percentage shows significantly larger disparity than what was found in the paper, but but show that black patients generate lower costs than white patients.\nIn this blog post, I found that preparing, processing, and cleaning data can take a lot of work to achieve the desired result. From the recreated figure 1, I found that on average, white patients are rated as having a higher risk score than black patients with the same amount of chronic illnesses. This puts black patients at risk because they are less likely to be referred to a high-risk program that can provide additional health care support. In figure 3, it was clear that, particularly for lower numbers of chronic illnesses (for which there was a lot more data), white patients typically spent more money than black patients. This was further emphasized by the analysis done of the performed linear regression, as described above. In my opinion, separation is the model of fairness that most clearly demonstrates the bias in this data set; the finding that sicker black patients don’t qualify as frequently as less sick white patients suggests a higher false negative rate for black patients. This discrepancy in error rates leads to more black patients not receiving additional care, despite being qualified for it, and can lead to them falling through holes in the health care system without necessary support."
  },
  {
    "objectID": "posts/BP3/BP3.html#blog-post-replication-study",
    "href": "posts/BP3/BP3.html#blog-post-replication-study",
    "title": "Blog Post: Replication Study",
    "section": "",
    "text": "In this blog post, I seek to replicate the findings of Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” They give into a study of whether medical data used to qualify high-risk patients for additional automatic support from the health care system is biased. I am to reproduce several of their figures showing the discrepancies between black and white patients with regards to who qualifies as “high-risk” thus being considered or automatically entered into the aforementioned program, and costs between black and white patients for the same risk-score as well as number of chronic conditions. Finally, I will perform a linear regression to determine how much less the average black patient incurs in health costs, and discuss why this is problematic, and which principle of fairness is violated.\n\n\n\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\ndf_imp = df[[\"risk_score_t\", \"cost_t\", \"race\", \"gagne_sum_t\"]].copy()\ndf_imp.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n\n\n1\n7.677934\n2600.0\nwhite\n3\n\n\n2\n0.407678\n500.0\nwhite\n0\n\n\n3\n0.798369\n1300.0\nwhite\n0\n\n\n4\n17.513165\n1100.0\nwhite\n1\n\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_imp['risk_percentile'] = round(df['risk_score_t'].rank(pct = True), 2) * 100\n\ngrouped_data = df_imp.groupby(['risk_percentile', 'race'])['gagne_sum_t'].mean()\n\ngrouped_data = pd.DataFrame(grouped_data)\n\nsns.scatterplot(data=grouped_data, x='gagne_sum_t', y='risk_percentile', hue='race')\n\nplt.xlabel('Mean Number of Chronic Illnesses')\nplt.ylabel('Percentile Risk Score')\nplt.title('Mean number of chronic illnesses versus algorithm predicted risk by race')\n\nplt.legend(title='Race')\nplt.show()\n\n\n\n\n\n\n\n\nThis plot begins to unravel some underlying racial bias that could be present in the algorithm. As seen in the figure, the orange dots representing white patients lie on a higher curve of percentile risk score. This suggests that less sick white patients are more likely to be referred to a high-risk program than sicker black patients.\n\n\n\n\nfig, axes = plt.subplots(1, 2, sharey = True)\n\ngrouped_cost_data = pd.DataFrame(df_imp.groupby(['risk_percentile', 'race'])['cost_t'].mean())\ngrouped_illness_data = pd.DataFrame(df_imp.groupby(['gagne_sum_t', 'race'])['cost_t'].mean())\n\nsns.scatterplot(ax = axes[0], data=grouped_cost_data, x='risk_percentile', y='cost_t', hue='race')\nsns.scatterplot(ax = axes[1], data=grouped_illness_data, x='gagne_sum_t', y='cost_t', hue='race')\n\naxes[0].set_ylabel('Total Medical Expenditure')\naxes[0].set_xlabel('Percentile Risk Score')\naxes[1].set_xlabel('Number of Chronic Illnesses')\n\nplt.ylim(600, 120000)\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\nFrom this chart, we can see that holding either percentile risk score or number of chronic illnesses constant, black patients have lower costs than white patients. Further to the right on each graph, the amount of data available drops off steeply, which explains the more erratic correlation.\n\n\n\n\n\n\ngreater_than_5 = round((df_imp['gagne_sum_t'] &gt;= 5).mean() * 100, 2)\ngreater_than_5\n\n7.0\n\n\nGiven that 93% of the patients in this data set have 5 or less chronic conditions, it seems reasonable to focus on them, at least as a starting point. It is still important to analyze trends for patients with more chronic illnesses, but in the scope of this assignment, it could downplay trends, and without sufficient data, could display incorrect trends.\n\nimport numpy as np\n\ndf_not_0 = df_imp.drop(df_imp[df_imp['cost_t'] == 0].index)\ndf_not_0['log_cost'] = np.log(df_not_0['cost_t'])\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\nrisk_percentile\nlog_cost\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n35.0\n7.090077\n\n\n1\n7.677934\n2600.0\nwhite\n3\n86.0\n7.863267\n\n\n2\n0.407678\n500.0\nwhite\n0\n4.0\n6.214608\n\n\n3\n0.798369\n1300.0\nwhite\n0\n11.0\n7.170120\n\n\n4\n17.513165\n1100.0\nwhite\n1\n98.0\n7.003065\n\n\n\n\n\n\n\n\ndf_not_0 = pd.get_dummies(df_not_0)\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_black\nrace_white\n\n\n\n\n0\n1.987430\n1200.0\n0\n35.0\n7.090077\n0\n1\n\n\n1\n7.677934\n2600.0\n3\n86.0\n7.863267\n0\n1\n\n\n2\n0.407678\n500.0\n0\n4.0\n6.214608\n0\n1\n\n\n3\n0.798369\n1300.0\n0\n11.0\n7.170120\n0\n1\n\n\n4\n17.513165\n1100.0\n1\n98.0\n7.003065\n0\n1\n\n\n\n\n\n\n\n\nX_train = df_not_0[['gagne_sum_t', 'race_black']].rename(columns={'race_black' : 'race'})\ny_train = df_not_0[['log_cost']]\nX_train.head(10)\n\n\n\n\n\n\n\n\ngagne_sum_t\nrace\n\n\n\n\n0\n0\n0\n\n\n1\n3\n0\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n1\n0\n\n\n5\n1\n0\n\n\n6\n1\n0\n\n\n7\n0\n0\n\n\n8\n1\n1\n\n\n9\n0\n0\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nLR = LinearRegression()\nLR.fit(X_train, y_train)\n\n# adds {degree} polynomial features to {X}\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nNow, I will add a variable number of features to X_train and see which produces the best accuracy with cross-validation.\n\ncv_scores = []\n\n# I will check how adding up to 10 additional polynomial features impacts the score\nfor i in range(0, 10):\n    X_deg = add_polynomial_features(X_train, i)\n    cv_scores.append(cross_val_score(LR.fit(X_deg, y_train), X_deg, y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores)\nbest_deg = np.argmax(cv_scores)\n\nprint(best_score)\nprint(best_deg)\n\n0.14820529981684197\n9\n\n\nIt looks like a polynomial with degree 9 was most successful with this data, so I will fit one additional linear regression model with this degree before proceeding.\n\nbest = add_polynomial_features(X_train, best_deg)\n\nLR.fit(best, y_train)\nw = LR.coef_\nprint(w)\n\n[[ 3.92483807e-01 -2.67357533e-01  3.92483807e-01 -5.63208481e-01\n   2.48313944e-01 -5.61461922e-02  7.08781442e-03 -5.05267129e-04\n   1.89805627e-05 -2.91722116e-07]]\n\n\n\n# as the coefficients are in the order of the data input\n# the second weight should correspond to the race column\n# this can be used to calculate the estimate cost incurred by black patients as a percent of that incurred by white patients\nb_coef = w[0, 1]\nb_coef\n\n-0.26735753310360466\n\n\n\ncost_incurred = np.exp(b_coef)\ncost_incurred\n\n0.7653993669279184\n\n\nSo, it looks like black patients, on average, pay about 3/4 of what white patients pay for health care. This is more or less following the argument of Obermeyer et al. (2019). I note that my percentage shows significantly larger disparity than what was found in the paper, but but show that black patients generate lower costs than white patients.\nIn this blog post, I found that preparing, processing, and cleaning data can take a lot of work to achieve the desired result. From the recreated figure 1, I found that on average, white patients are rated as having a higher risk score than black patients with the same amount of chronic illnesses. This puts black patients at risk because they are less likely to be referred to a high-risk program that can provide additional health care support. In figure 3, it was clear that, particularly for lower numbers of chronic illnesses (for which there was a lot more data), white patients typically spent more money than black patients. This was further emphasized by the analysis done of the performed linear regression, as described above. In my opinion, separation is the model of fairness that most clearly demonstrates the bias in this data set; the finding that sicker black patients don’t qualify as frequently as less sick white patients suggests a higher false negative rate for black patients. This discrepancy in error rates leads to more black patients not receiving additional care, despite being qualified for it, and can lead to them falling through holes in the health care system without necessary support."
  },
  {
    "objectID": "posts/BP6/BP6.html",
    "href": "posts/BP6/BP6.html",
    "title": "Blog Post: Perceptron",
    "section": "",
    "text": "Blog Post: Perceptron\nIn this blog post, I will explore the perceptron algorithm, one of the oldest classification algorithms. First, I will implement the perceptron algorithm, and then I will conduct a couple of experiments to ensure that it is working properly. The first experiment will confirm that the algorithm will converge to a loss of 0 on linearly separable data. The next experiment will show that given non-linearly separable data, the algorithm will continue iterating until it reaches a maximum number of iterations without ever converging. The final experiment will demonstrate that the perceptron can work in more than 2 dimensions. After that, I will implement minibatch perceptron. Before concluding, I will summarize my findings.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptron_mini import PerceptronMini, PerceptronOptimizerMini\n\nImportError: cannot import name 'PerceptronMini' from 'perceptron_mini' (c:\\Users\\Zoe Greenwald\\ShadowedSpace.github.io\\posts\\BP6\\perceptron_mini.py)\n\n\n\nimport torch\nfrom matplotlib import pyplot as plt\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, w, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, [0, 0, 0], ax)\n\n\n\n\n\n\n\n\n\nExperiment 1\nIn this experiment, I will show that given linearly separable data, the perceptron algorithm will converge to a value of w that separates the data.\n\n# instantiate a model and an optimizer\np = Perceptron() \np.w = torch.Tensor([1, 1, 1])\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nlosses = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    losses.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, p.w, ax)\n\n\n\n\n\n\n\n\nOk nice - I can see that the w (represented by the black line) successfully separates all of the yellow data points from all of the green data points. Looks like this experiment was successful.\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nAnd it took 70 iterations to converge. A little unlucky given that there are only 50 data points. ## Experiment 2 In this second experiment, I will show that when the data is not linearly separable, the perceptron algorithm will continue iterating until it reaches a provided maximum number of iterations (for example, through a for loop). It will never reach 0, as this isn’t possible for data that isn’t linearly separable, and, as such, will continue iterating until told to stop.\n\nX, y = perceptron_data(n_points = 50, noise = 0.75)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, [0, 0, 0], ax)\n\n\n\n\n\n\n\n\nAlright - we increased the noise to get some good data here. We can see visually that the data is not linearly separable, so it should serve for this experiment.\n\n# instantiate a model and an optimizer\np = Perceptron() \np.w = torch.Tensor([1, 1, 1])\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nlosses = []\n\nn = X.size()[0]\n\nfor i in range(1000): # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    losses.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, p.w, ax)\n\n\n\n\n\n\n\n\nThe algorithm did a pretty good job separating most of the points, but as expected, there are some outliers of each group on the wrong side of w.\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nAs can be seen from the plot of the loss, it was successfully shown that convergence will not occur, and instead the algorithm will continue to iterate until it hits the maximum value of iterations. ## Experiment 3 In this experiment, I will explore how the perceptron performs on high dimension data sets.\n\nX, y = perceptron_data(n_points = 300, noise = 0.9, p_dims=10)\n\n# instantiate a model and an optimizer\np = Perceptron() \np.w = torch.rand(X.size()[1])\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nlosses = []\n\nn = X.size()[0]\n\nfor i in range(1000): # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    losses.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\n\nX, y = perceptron_data(n_points = 300, noise = 0.9, p_dims=10)\n\n# instantiate a model and an optimizer\np = Perceptron() \np.w = torch.rand(X.size()[1])\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nlosses = []\n\nn = X.size()[0]\n\nfor i in range(1000): # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    losses.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nMy guess would be that this data isn’t linearly separable due to the lack of convergence. The plot aligns more closely with that of the second experiment. However, I was interested to observe that the loss seemed to fluctuate a lot less with higher dimensions.\nIn this blog post, I explored the perceptron algorithm to separate data. One important note about this algorithm is that if the data is not linearly separable, it will never converge! This means it is important to put a cap on the number of times it is run if you aren’t sure the data is linearly separable. In terms of implementing the Perceptron class, the gradient function, Perceptron.grad, is incredibly important. It first computes the scores for the input data X, and then calculates exp, an expression to indicate for which instances the model made incorrect predictions. Finally, it calculates the gradient of the loss function, multiplying by X and y to decide which direction the weights must be updated to improve. Finally, it returns the mean of the gradient over the 0th dimension. The runtime complexity of a single iteration of the perceptron algorithm is O(p) where p is the number of features. This comes from the gradient function. Following this time complexity, it seems that the runtime of perceptron relies on the number of features p, but not the number of data points n."
  },
  {
    "objectID": "posts/BP8/BP8.html",
    "href": "posts/BP8/BP8.html",
    "title": "Blog Post: Newton’s Method for Logistic Regression",
    "section": "",
    "text": "First of all, here is a link to my LogisticRegression source code with NewtonOptimizer: https://github.com/ShadowedSpace/ShadowedSpace.github.io/blob/main/posts/BP8/LogisticRegression.py\nLogistic regression was so fun I just had to try implementing it with another method! In this blog post, I will start by implementing Newton’s Method, a second-order optimization technique that computes the second derivatives of the loss function. This primarily involves calculating the Hessian Matrix of second derivatives of the loss. Once I’ve done this, I will perform experiments to show that: 1) Newton’s Method does converge 2) Newton’s Method can converge faster than gradient descent 3) Newton’s Method fails to converge if alpha is too high Finally, I will do an analysis of the time complexity of Newton’s Method versus Gradient Descent.\n\n%load_ext autoreload\n%autoreload 2\nfrom LogisticRegression import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# generates data for classification problem\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\nX, y = classification_data(noise = 1)\nw = torch.linspace(-1, 1, X.shape[1])\n\n\n# plots data and boundary decision based on w\n# only works for dimension = 2\n\ndef plot_data_and_boundary(X, y, w, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the data\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    colors = [\"red\", \"blue\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = colors[i], cmap = \"BrBG\", marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\n\n# calculates the accuracy of a model\n\ndef accuracy(y_pred, y_true):\n    correct = (y_pred == y_true).sum()\n    total = y_true.shape[0]\n    print(correct)\n    return correct / total\n\n\nLR = LogisticRegression(w)\nopt = NewtonOptimizer(LR, w)\n\nlosses = []\n\nfor _ in range(100):\n    losses.append(LR.loss(X, y))\n    opt.step(X, y, alpha = 200)\n\n\n\nThis experiment will do two things. First, it will show that Newton’s Method can converge. It will also show that Newton’s method can converge faster than Gradient Descent with the proper parameters.\n\nw = torch.linspace(-1, 1, X.shape[1])\nw_prev = torch.linspace(-1, 1, X.shape[1])\n\nLR_grad = LogisticRegression(w)\nopt = GradientDescentOptimizer(LR_grad, w, w_prev)\n\nlosses_grad = []\n\nfor _ in range(100):\n    losses_grad.append(LR_grad.loss(X, y))\n    opt.step(X, y, alpha = 0.05, beta = 0.9)\n\nplt.plot(losses, label=\"Newton's Method\")\nplt.plot(losses_grad, label = \"Gradient Descent\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nAwesome! As we can see here, the blue plot of the losses of Newton’s Method converges practically instantly - definitely much faster than the orange losses of Gradient Descent with Momentum.\n\n\n\nHere, I’m aiming to show that Newton’s Method converging is contingent on a suitably small alpha.\n\nLR_broken = LogisticRegression(w)\nopt = NewtonOptimizer(LR_broken, w)\n\nlosses = []\n\nfor _ in range(6):\n    losses.append(LR_broken.loss(X, y))\n    opt.step(X, y, alpha = 341)\n\n\nplt.plot(losses, label=\"Newton's Method\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nAnd… that doesn’t look so great. Fortunately, in this case, not so great is exactly what we were hoping for. This plot of oscillating losses shows that Newton’s Method can fail to converge if the value of alpha is too high.\n\n\n\nLet p be the number of features, and let the cost of computing the loss be c computational units, the gradient be 2c computational units, the hessian be pc computational units. Suppose also that it costs \\(k_1p^\\gamma\\) to invert a \\(pxp\\) matrix, and \\(k_2p^2\\) to do the matrix-vector multiplication required by Newton’s Method.\nComputational cost of Gradient Descent: - computing the loss, (c computational units) - computing the gradient (2c computational units) - multiply by the number of steps for convergence, \\(t_{gd}\\)\n\\[\nT(GradientDescent) = O(t_{gd} 3c)\n\\]\nComputational cost of Newton’s Method: - everything required by gradient descent - computing the hessian (pc computational units) - inverting a \\(pxp\\) matrix (\\(k_1p^\\gamma\\) computational units) - matrix-vector operation (\\(k_2p^2\\))\n\\[\nT(NewtonOptimizer) = O(t_{nm} (c(3 + p) + k_1p^\\gamma + k_2p^2))\n\\]\nSo… \\(t_{gd}\\) needs to be a lot smaller than \\(t_{nm}\\) for Newton’s Method to be worthwhile. As p increases, the \\(k_1p^\\gamma\\) and \\(k_2p^2\\) terms will grow overwhelmingly, so the fast convergence likely isn’t worth it for large values of p.\n\n\n\n\nAnother cool blog post! I really liked how this one built off of the Logistic Regression blog post, and I think it boosted my understanding of both (probably through the excruciatingly frustrating debugging process). The practice with vectorized operations was helpful, and I enjoyed the comparison between the two methods of Logistic Regression (both in terms of seeing Newton’s Method converge faster in Experiment 1, and then looking at the limited cases in which this actually happens via the time complexity analysis)."
  },
  {
    "objectID": "posts/BP8/BP8.html#introduction",
    "href": "posts/BP8/BP8.html#introduction",
    "title": "Blog Post: Newton’s Method for Logistic Regression",
    "section": "",
    "text": "First of all, here is a link to my LogisticRegression source code with NewtonOptimizer: https://github.com/ShadowedSpace/ShadowedSpace.github.io/blob/main/posts/BP8/LogisticRegression.py\nLogistic regression was so fun I just had to try implementing it with another method! In this blog post, I will start by implementing Newton’s Method, a second-order optimization technique that computes the second derivatives of the loss function. This primarily involves calculating the Hessian Matrix of second derivatives of the loss. Once I’ve done this, I will perform experiments to show that: 1) Newton’s Method does converge 2) Newton’s Method can converge faster than gradient descent 3) Newton’s Method fails to converge if alpha is too high Finally, I will do an analysis of the time complexity of Newton’s Method versus Gradient Descent.\n\n%load_ext autoreload\n%autoreload 2\nfrom LogisticRegression import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# generates data for classification problem\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\nX, y = classification_data(noise = 1)\nw = torch.linspace(-1, 1, X.shape[1])\n\n\n# plots data and boundary decision based on w\n# only works for dimension = 2\n\ndef plot_data_and_boundary(X, y, w, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the data\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    colors = [\"red\", \"blue\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = colors[i], cmap = \"BrBG\", marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\n\n# calculates the accuracy of a model\n\ndef accuracy(y_pred, y_true):\n    correct = (y_pred == y_true).sum()\n    total = y_true.shape[0]\n    print(correct)\n    return correct / total\n\n\nLR = LogisticRegression(w)\nopt = NewtonOptimizer(LR, w)\n\nlosses = []\n\nfor _ in range(100):\n    losses.append(LR.loss(X, y))\n    opt.step(X, y, alpha = 200)\n\n\n\nThis experiment will do two things. First, it will show that Newton’s Method can converge. It will also show that Newton’s method can converge faster than Gradient Descent with the proper parameters.\n\nw = torch.linspace(-1, 1, X.shape[1])\nw_prev = torch.linspace(-1, 1, X.shape[1])\n\nLR_grad = LogisticRegression(w)\nopt = GradientDescentOptimizer(LR_grad, w, w_prev)\n\nlosses_grad = []\n\nfor _ in range(100):\n    losses_grad.append(LR_grad.loss(X, y))\n    opt.step(X, y, alpha = 0.05, beta = 0.9)\n\nplt.plot(losses, label=\"Newton's Method\")\nplt.plot(losses_grad, label = \"Gradient Descent\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nAwesome! As we can see here, the blue plot of the losses of Newton’s Method converges practically instantly - definitely much faster than the orange losses of Gradient Descent with Momentum.\n\n\n\nHere, I’m aiming to show that Newton’s Method converging is contingent on a suitably small alpha.\n\nLR_broken = LogisticRegression(w)\nopt = NewtonOptimizer(LR_broken, w)\n\nlosses = []\n\nfor _ in range(6):\n    losses.append(LR_broken.loss(X, y))\n    opt.step(X, y, alpha = 341)\n\n\nplt.plot(losses, label=\"Newton's Method\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nAnd… that doesn’t look so great. Fortunately, in this case, not so great is exactly what we were hoping for. This plot of oscillating losses shows that Newton’s Method can fail to converge if the value of alpha is too high.\n\n\n\nLet p be the number of features, and let the cost of computing the loss be c computational units, the gradient be 2c computational units, the hessian be pc computational units. Suppose also that it costs \\(k_1p^\\gamma\\) to invert a \\(pxp\\) matrix, and \\(k_2p^2\\) to do the matrix-vector multiplication required by Newton’s Method.\nComputational cost of Gradient Descent: - computing the loss, (c computational units) - computing the gradient (2c computational units) - multiply by the number of steps for convergence, \\(t_{gd}\\)\n\\[\nT(GradientDescent) = O(t_{gd} 3c)\n\\]\nComputational cost of Newton’s Method: - everything required by gradient descent - computing the hessian (pc computational units) - inverting a \\(pxp\\) matrix (\\(k_1p^\\gamma\\) computational units) - matrix-vector operation (\\(k_2p^2\\))\n\\[\nT(NewtonOptimizer) = O(t_{nm} (c(3 + p) + k_1p^\\gamma + k_2p^2))\n\\]\nSo… \\(t_{gd}\\) needs to be a lot smaller than \\(t_{nm}\\) for Newton’s Method to be worthwhile. As p increases, the \\(k_1p^\\gamma\\) and \\(k_2p^2\\) terms will grow overwhelmingly, so the fast convergence likely isn’t worth it for large values of p."
  },
  {
    "objectID": "posts/BP8/BP8.html#conclusion",
    "href": "posts/BP8/BP8.html#conclusion",
    "title": "Blog Post: Newton’s Method for Logistic Regression",
    "section": "",
    "text": "Another cool blog post! I really liked how this one built off of the Logistic Regression blog post, and I think it boosted my understanding of both (probably through the excruciatingly frustrating debugging process). The practice with vectorized operations was helpful, and I enjoyed the comparison between the two methods of Logistic Regression (both in terms of seeing Newton’s Method converge faster in Experiment 1, and then looking at the limited cases in which this actually happens via the time complexity analysis)."
  }
]