[
  {
    "objectID": "posts/BP8/BP8.html",
    "href": "posts/BP8/BP8.html",
    "title": "Blog Post: Newton’s Method for Logistic Regression",
    "section": "",
    "text": "First of all, here is a link to my LogisticRegression source code with NewtonOptimizer: https://github.com/ShadowedSpace/ShadowedSpace.github.io/blob/main/posts/BP8/LogisticRegression.py\nLogistic regression was so fun I just had to try implementing it with another method! In this blog post, I will start by implementing Newton’s Method, a second-order optimization technique that computes the second derivatives of the loss function. This primarily involves calculating the Hessian Matrix of second derivatives of the loss. Once I’ve done this, I will perform experiments to show that: 1) Newton’s Method does converge 2) Newton’s Method can converge faster than gradient descent 3) Newton’s Method fails to converge if alpha is too high Finally, I will do an analysis of the time complexity of Newton’s Method versus Gradient Descent.\n\n%load_ext autoreload\n%autoreload 2\nfrom LogisticRegression import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# generates data for classification problem\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\nX, y = classification_data(noise = 1)\nw = torch.linspace(-1, 1, X.shape[1])\n\n\n# plots data and boundary decision based on w\n# only works for dimension = 2\n\ndef plot_data_and_boundary(X, y, w, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the data\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    colors = [\"red\", \"blue\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = colors[i], cmap = \"BrBG\", marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\n\n# calculates the accuracy of a model\n\ndef accuracy(y_pred, y_true):\n    correct = (y_pred == y_true).sum()\n    total = y_true.shape[0]\n    print(correct)\n    return correct / total\n\n\nLR = LogisticRegression(w)\nopt = NewtonOptimizer(LR, w)\n\nlosses = []\n\nfor _ in range(100):\n    losses.append(LR.loss(X, y))\n    opt.step(X, y, alpha = 200)\n\n\n\nThis experiment will do two things. First, it will show that Newton’s Method can converge. It will also show that Newton’s method can converge faster than Gradient Descent with the proper parameters.\n\nw = torch.linspace(-1, 1, X.shape[1])\nw_prev = torch.linspace(-1, 1, X.shape[1])\n\nLR_grad = LogisticRegression(w)\nopt = GradientDescentOptimizer(LR_grad, w, w_prev)\n\nlosses_grad = []\n\nfor _ in range(100):\n    losses_grad.append(LR_grad.loss(X, y))\n    opt.step(X, y, alpha = 0.05, beta = 0.9)\n\nplt.plot(losses, label=\"Newton's Method\")\nplt.plot(losses_grad, label = \"Gradient Descent\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nAwesome! As we can see here, the blue plot of the losses of Newton’s Method converges practically instantly - definitely much faster than the orange losses of Gradient Descent with Momentum.\n\n\n\nHere, I’m aiming to show that Newton’s Method converging is contingent on a suitably small alpha.\n\nLR_broken = LogisticRegression(w)\nopt = NewtonOptimizer(LR_broken, w)\n\nlosses = []\n\nfor _ in range(6):\n    losses.append(LR_broken.loss(X, y))\n    opt.step(X, y, alpha = 341)\n\n\nplt.plot(losses, label=\"Newton's Method\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nAnd… that doesn’t look so great. Fortunately, in this case, not so great is exactly what we were hoping for. This plot of oscillating losses shows that Newton’s Method can fail to converge if the value of alpha is too high.\n\n\n\nLet p be the number of features, and let the cost of computing the loss be c computational units, the gradient be 2c computational units, the hessian be pc computational units. Suppose also that it costs \\(k_1p^\\gamma\\) to invert a \\(pxp\\) matrix, and \\(k_2p^2\\) to do the matrix-vector multiplication required by Newton’s Method.\nComputational cost of Gradient Descent: - computing the loss, (c computational units) - computing the gradient (2c computational units) - multiply by the number of steps for convergence, \\(t_{gd}\\)\n\\[\nT(GradientDescent) = O(t_{gd} 3c)\n\\]\nComputational cost of Newton’s Method: - everything required by gradient descent - computing the hessian (pc computational units) - inverting a \\(pxp\\) matrix (\\(k_1p^\\gamma\\) computational units) - matrix-vector operation (\\(k_2p^2\\))\n\\[\nT(NewtonOptimizer) = O(t_{nm} (c(3 + p) + k_1p^\\gamma + k_2p^2))\n\\]\nSo… \\(t_{gd}\\) needs to be a lot smaller than \\(t_{nm}\\) for Newton’s Method to be worthwhile. As p increases, the \\(k_1p^\\gamma\\) and \\(k_2p^2\\) terms will grow overwhelmingly, so the fast convergence likely isn’t worth it for large values of p.\n\n\n\n\nAnother cool blog post! I really liked how this one built off of the Logistic Regression blog post, and I think it boosted my understanding of both (probably through the excruciatingly frustrating debugging process). The practice with vectorized operations was helpful, and I enjoyed the comparison between the two methods of Logistic Regression (both in terms of seeing Newton’s Method converge faster in Experiment 1, and then looking at the limited cases in which this actually happens via the time complexity analysis)."
  },
  {
    "objectID": "posts/BP8/BP8.html#introduction",
    "href": "posts/BP8/BP8.html#introduction",
    "title": "Blog Post: Newton’s Method for Logistic Regression",
    "section": "",
    "text": "First of all, here is a link to my LogisticRegression source code with NewtonOptimizer: https://github.com/ShadowedSpace/ShadowedSpace.github.io/blob/main/posts/BP8/LogisticRegression.py\nLogistic regression was so fun I just had to try implementing it with another method! In this blog post, I will start by implementing Newton’s Method, a second-order optimization technique that computes the second derivatives of the loss function. This primarily involves calculating the Hessian Matrix of second derivatives of the loss. Once I’ve done this, I will perform experiments to show that: 1) Newton’s Method does converge 2) Newton’s Method can converge faster than gradient descent 3) Newton’s Method fails to converge if alpha is too high Finally, I will do an analysis of the time complexity of Newton’s Method versus Gradient Descent.\n\n%load_ext autoreload\n%autoreload 2\nfrom LogisticRegression import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n# generates data for classification problem\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\nX, y = classification_data(noise = 1)\nw = torch.linspace(-1, 1, X.shape[1])\n\n\n# plots data and boundary decision based on w\n# only works for dimension = 2\n\ndef plot_data_and_boundary(X, y, w, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the data\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    colors = [\"red\", \"blue\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = colors[i], cmap = \"BrBG\", marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\n\n# calculates the accuracy of a model\n\ndef accuracy(y_pred, y_true):\n    correct = (y_pred == y_true).sum()\n    total = y_true.shape[0]\n    print(correct)\n    return correct / total\n\n\nLR = LogisticRegression(w)\nopt = NewtonOptimizer(LR, w)\n\nlosses = []\n\nfor _ in range(100):\n    losses.append(LR.loss(X, y))\n    opt.step(X, y, alpha = 200)\n\n\n\nThis experiment will do two things. First, it will show that Newton’s Method can converge. It will also show that Newton’s method can converge faster than Gradient Descent with the proper parameters.\n\nw = torch.linspace(-1, 1, X.shape[1])\nw_prev = torch.linspace(-1, 1, X.shape[1])\n\nLR_grad = LogisticRegression(w)\nopt = GradientDescentOptimizer(LR_grad, w, w_prev)\n\nlosses_grad = []\n\nfor _ in range(100):\n    losses_grad.append(LR_grad.loss(X, y))\n    opt.step(X, y, alpha = 0.05, beta = 0.9)\n\nplt.plot(losses, label=\"Newton's Method\")\nplt.plot(losses_grad, label = \"Gradient Descent\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nAwesome! As we can see here, the blue plot of the losses of Newton’s Method converges practically instantly - definitely much faster than the orange losses of Gradient Descent with Momentum.\n\n\n\nHere, I’m aiming to show that Newton’s Method converging is contingent on a suitably small alpha.\n\nLR_broken = LogisticRegression(w)\nopt = NewtonOptimizer(LR_broken, w)\n\nlosses = []\n\nfor _ in range(6):\n    losses.append(LR_broken.loss(X, y))\n    opt.step(X, y, alpha = 341)\n\n\nplt.plot(losses, label=\"Newton's Method\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nAnd… that doesn’t look so great. Fortunately, in this case, not so great is exactly what we were hoping for. This plot of oscillating losses shows that Newton’s Method can fail to converge if the value of alpha is too high.\n\n\n\nLet p be the number of features, and let the cost of computing the loss be c computational units, the gradient be 2c computational units, the hessian be pc computational units. Suppose also that it costs \\(k_1p^\\gamma\\) to invert a \\(pxp\\) matrix, and \\(k_2p^2\\) to do the matrix-vector multiplication required by Newton’s Method.\nComputational cost of Gradient Descent: - computing the loss, (c computational units) - computing the gradient (2c computational units) - multiply by the number of steps for convergence, \\(t_{gd}\\)\n\\[\nT(GradientDescent) = O(t_{gd} 3c)\n\\]\nComputational cost of Newton’s Method: - everything required by gradient descent - computing the hessian (pc computational units) - inverting a \\(pxp\\) matrix (\\(k_1p^\\gamma\\) computational units) - matrix-vector operation (\\(k_2p^2\\))\n\\[\nT(NewtonOptimizer) = O(t_{nm} (c(3 + p) + k_1p^\\gamma + k_2p^2))\n\\]\nSo… \\(t_{gd}\\) needs to be a lot smaller than \\(t_{nm}\\) for Newton’s Method to be worthwhile. As p increases, the \\(k_1p^\\gamma\\) and \\(k_2p^2\\) terms will grow overwhelmingly, so the fast convergence likely isn’t worth it for large values of p."
  },
  {
    "objectID": "posts/BP8/BP8.html#conclusion",
    "href": "posts/BP8/BP8.html#conclusion",
    "title": "Blog Post: Newton’s Method for Logistic Regression",
    "section": "",
    "text": "Another cool blog post! I really liked how this one built off of the Logistic Regression blog post, and I think it boosted my understanding of both (probably through the excruciatingly frustrating debugging process). The practice with vectorized operations was helpful, and I enjoyed the comparison between the two methods of Logistic Regression (both in terms of seeing Newton’s Method converge faster in Experiment 1, and then looking at the limited cases in which this actually happens via the time complexity analysis)."
  },
  {
    "objectID": "posts/BP6/BP6.html",
    "href": "posts/BP6/BP6.html",
    "title": "Blog Post: Perceptron",
    "section": "",
    "text": "Blog Post: Perceptron\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nimport torch\nfrom matplotlib import pyplot as plt\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, w, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\nX, y = perceptron_data()\n\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.3)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, [0, 0, 0], ax)\n\n\n\n\n\n\n\n\n\nExperiment 1\nIn this experiment, I will show that given linearly separable data, the perceptron algorithm will converge to a value of w that separates the data.\n\n# instantiate a model and an optimizer\np = Perceptron() \np.w = torch.Tensor([1, 1, 1])\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nlosses = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    losses.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, p.w, ax)\n\n\n\n\n\n\n\n\nOk nice - I can see that the w (represented by the black line) successfully separates all of the yellow data points from all of the green data points. Looks like this experiment was successful.\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nAnd it took 70 iterations to converge. A little unlucky given that there are only 50 data points. ## Experiment 2 In this second experiment, I will show that when the data is not linearly separable, the perceptron algorithm will continue iterating until it reaches a provided maximum number of iterations (for example, through a for loop). It will never reach 0, as this isn’t possible for data that isn’t linearly separable, and, as such, will continue iterating until told to stop.\n\nX, y = perceptron_data(n_points = 50, noise = 0.75)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, [0, 0, 0], ax)\n\n\n\n\n\n\n\n\nAlright - we increased the noise to get some good data here. We can see visually that the data is not linearly separable, so it should serve for this experiment.\n\n# instantiate a model and an optimizer\np = Perceptron() \np.w = torch.Tensor([1, 1, 1])\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nlosses = []\n\nn = X.size()[0]\n\nfor i in range(1000): # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    losses.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, p.w, ax)\n\n\n\n\n\n\n\n\nThe algorithm did a pretty good job separating most of the points, but as expected, there are some outliers of each group on the wrong side of w.\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nAs can be seen from the plot of the loss, it was successfully shown that convergence will not occur, and instead the algorithm will continue to iterate until it hits the maximum value of iterations. ## Experiment 3 In this experiment, I will explore how the perceptron performs on high dimension data sets.\n\nX, y = perceptron_data(n_points = 300, noise = 0.9, p_dims=10)\n\n# instantiate a model and an optimizer\np = Perceptron() \np.w = torch.rand(X.size()[1])\nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nlosses = []\n\nn = X.size()[0]\n\nfor i in range(1000): # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    losses.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nMy guess would be that this data isn’t linearly separable due to the lack of convergence. The plot aligns more closely with that of the second experiment. However, I was interested to observe that the loss seemed to fluctuate a lot less with higher dimensions.\nIn this blog post, I explored the perceptron algorithm to separate data. One important note about this algorithm is that if the data is not linearly separable, it will never converge! This means it is important to put a cap on the number of times it is run if you aren’t sure the data is linearly separable. In terms of implementing the Perceptron class, the gradient function, Perceptron.grad, is incredibly important. It first computes the scores for the input data X, and then calculates exp, an expression to indicate for which instances the model made incorrect predictions. Finally, it calculates the gradient of the loss function, multiplying by X and y to decide which direction the weights must be updated to improve. Finally, it returns the mean of the gradient over the 0th dimension. The runtime complexity of a single iteration of the perceptron algorithm is O(p) where p is the number of features. This comes from the gradient function. Following this time complexity, it seems that the runtime of perceptron relies on the number of features p, but not the number of data points n."
  },
  {
    "objectID": "posts/BP3/BP3.html",
    "href": "posts/BP3/BP3.html",
    "title": "Blog Post: Replication Study",
    "section": "",
    "text": "In this blog post, I seek to replicate the findings of Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” They give into a study of whether medical data used to qualify high-risk patients for additional automatic support from the health care system is biased. I am to reproduce several of their figures showing the discrepancies between black and white patients with regards to who qualifies as “high-risk” thus being considered or automatically entered into the aforementioned program, and costs between black and white patients for the same risk-score as well as number of chronic conditions. Finally, I will perform a linear regression to determine how much less the average black patient incurs in health costs, and discuss why this is problematic, and which principle of fairness is violated.\n\n\n\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\ndf_imp = df[[\"risk_score_t\", \"cost_t\", \"race\", \"gagne_sum_t\"]].copy()\ndf_imp.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n\n\n1\n7.677934\n2600.0\nwhite\n3\n\n\n2\n0.407678\n500.0\nwhite\n0\n\n\n3\n0.798369\n1300.0\nwhite\n0\n\n\n4\n17.513165\n1100.0\nwhite\n1\n\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_imp['risk_percentile'] = round(df['risk_score_t'].rank(pct = True), 2) * 100\n\ngrouped_data = df_imp.groupby(['risk_percentile', 'race'])['gagne_sum_t'].mean()\n\ngrouped_data = pd.DataFrame(grouped_data)\n\nsns.scatterplot(data=grouped_data, x='gagne_sum_t', y='risk_percentile', hue='race')\n\nplt.xlabel('Mean Number of Chronic Illnesses')\nplt.ylabel('Percentile Risk Score')\nplt.title('Mean number of chronic illnesses versus algorithm predicted risk by race')\n\nplt.legend(title='Race')\nplt.show()\n\n\n\n\n\n\n\n\nThis plot begins to unravel some underlying racial bias that could be present in the algorithm. As seen in the figure, the orange dots representing white patients lie on a higher curve of percentile risk score. This suggests that less sick white patients are more likely to be referred to a high-risk program than sicker black patients.\n\n\n\n\nfig, axes = plt.subplots(1, 2, sharey = True)\n\ngrouped_cost_data = pd.DataFrame(df_imp.groupby(['risk_percentile', 'race'])['cost_t'].mean())\ngrouped_illness_data = pd.DataFrame(df_imp.groupby(['gagne_sum_t', 'race'])['cost_t'].mean())\n\nsns.scatterplot(ax = axes[0], data=grouped_cost_data, x='risk_percentile', y='cost_t', hue='race')\nsns.scatterplot(ax = axes[1], data=grouped_illness_data, x='gagne_sum_t', y='cost_t', hue='race')\n\naxes[0].set_ylabel('Total Medical Expenditure')\naxes[0].set_xlabel('Percentile Risk Score')\naxes[1].set_xlabel('Number of Chronic Illnesses')\n\nplt.ylim(600, 120000)\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\nFrom this chart, we can see that holding either percentile risk score or number of chronic illnesses constant, black patients have lower costs than white patients. Further to the right on each graph, the amount of data available drops off steeply, which explains the more erratic correlation.\n\n\n\n\n\n\ngreater_than_5 = round((df_imp['gagne_sum_t'] &gt;= 5).mean() * 100, 2)\ngreater_than_5\n\n7.0\n\n\nGiven that 93% of the patients in this data set have 5 or less chronic conditions, it seems reasonable to focus on them, at least as a starting point. It is still important to analyze trends for patients with more chronic illnesses, but in the scope of this assignment, it could downplay trends, and without sufficient data, could display incorrect trends.\n\nimport numpy as np\n\ndf_not_0 = df_imp.drop(df_imp[df_imp['cost_t'] == 0].index)\ndf_not_0['log_cost'] = np.log(df_not_0['cost_t'])\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\nrisk_percentile\nlog_cost\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n35.0\n7.090077\n\n\n1\n7.677934\n2600.0\nwhite\n3\n86.0\n7.863267\n\n\n2\n0.407678\n500.0\nwhite\n0\n4.0\n6.214608\n\n\n3\n0.798369\n1300.0\nwhite\n0\n11.0\n7.170120\n\n\n4\n17.513165\n1100.0\nwhite\n1\n98.0\n7.003065\n\n\n\n\n\n\n\n\ndf_not_0 = pd.get_dummies(df_not_0)\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_black\nrace_white\n\n\n\n\n0\n1.987430\n1200.0\n0\n35.0\n7.090077\n0\n1\n\n\n1\n7.677934\n2600.0\n3\n86.0\n7.863267\n0\n1\n\n\n2\n0.407678\n500.0\n0\n4.0\n6.214608\n0\n1\n\n\n3\n0.798369\n1300.0\n0\n11.0\n7.170120\n0\n1\n\n\n4\n17.513165\n1100.0\n1\n98.0\n7.003065\n0\n1\n\n\n\n\n\n\n\n\nX_train = df_not_0[['gagne_sum_t', 'race_black']].rename(columns={'race_black' : 'race'})\ny_train = df_not_0[['log_cost']]\nX_train.head(10)\n\n\n\n\n\n\n\n\ngagne_sum_t\nrace\n\n\n\n\n0\n0\n0\n\n\n1\n3\n0\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n1\n0\n\n\n5\n1\n0\n\n\n6\n1\n0\n\n\n7\n0\n0\n\n\n8\n1\n1\n\n\n9\n0\n0\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nLR = LinearRegression()\nLR.fit(X_train, y_train)\n\n# adds {degree} polynomial features to {X}\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nNow, I will add a variable number of features to X_train and see which produces the best accuracy with cross-validation.\n\ncv_scores = []\n\n# I will check how adding up to 10 additional polynomial features impacts the score\nfor i in range(0, 10):\n    X_deg = add_polynomial_features(X_train, i)\n    cv_scores.append(cross_val_score(LR.fit(X_deg, y_train), X_deg, y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores)\nbest_deg = np.argmax(cv_scores)\n\nprint(best_score)\nprint(best_deg)\n\n0.14820529981684197\n9\n\n\nIt looks like a polynomial with degree 9 was most successful with this data, so I will fit one additional linear regression model with this degree before proceeding.\n\nbest = add_polynomial_features(X_train, best_deg)\n\nLR.fit(best, y_train)\nw = LR.coef_\nprint(w)\n\n[[ 3.92483807e-01 -2.67357533e-01  3.92483807e-01 -5.63208481e-01\n   2.48313944e-01 -5.61461922e-02  7.08781442e-03 -5.05267129e-04\n   1.89805627e-05 -2.91722116e-07]]\n\n\n\n# as the coefficients are in the order of the data input\n# the second weight should correspond to the race column\n# this can be used to calculate the estimate cost incurred by black patients as a percent of that incurred by white patients\nb_coef = w[0, 1]\nb_coef\n\n-0.26735753310360466\n\n\n\ncost_incurred = np.exp(b_coef)\ncost_incurred\n\n0.7653993669279184\n\n\nSo, it looks like black patients, on average, pay about 3/4 of what white patients pay for health care. This is more or less following the argument of Obermeyer et al. (2019). I note that my percentage shows significantly larger disparity than what was found in the paper, but but show that black patients generate lower costs than white patients.\nIn this blog post, I found that preparing, processing, and cleaning data can take a lot of work to achieve the desired result. From the recreated figure 1, I found that on average, white patients are rated as having a higher risk score than black patients with the same amount of chronic illnesses. This puts black patients at risk because they are less likely to be referred to a high-risk program that can provide additional health care support. In figure 3, it was clear that, particularly for lower numbers of chronic illnesses (for which there was a lot more data), white patients typically spent more money than black patients. This was further emphasized by the analysis done of the performed linear regression, as described above. In my opinion, separation is the model of fairness that most clearly demonstrates the bias in this data set; the finding that sicker black patients don’t qualify as frequently as less sick white patients suggests a higher false negative rate for black patients. This discrepancy in error rates leads to more black patients not receiving additional care, despite being qualified for it, and can lead to them falling through holes in the health care system without necessary support."
  },
  {
    "objectID": "posts/BP3/BP3.html#blog-post-replication-study",
    "href": "posts/BP3/BP3.html#blog-post-replication-study",
    "title": "Blog Post: Replication Study",
    "section": "",
    "text": "In this blog post, I seek to replicate the findings of Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” They give into a study of whether medical data used to qualify high-risk patients for additional automatic support from the health care system is biased. I am to reproduce several of their figures showing the discrepancies between black and white patients with regards to who qualifies as “high-risk” thus being considered or automatically entered into the aforementioned program, and costs between black and white patients for the same risk-score as well as number of chronic conditions. Finally, I will perform a linear regression to determine how much less the average black patient incurs in health costs, and discuss why this is problematic, and which principle of fairness is violated.\n\n\n\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\ndf_imp = df[[\"risk_score_t\", \"cost_t\", \"race\", \"gagne_sum_t\"]].copy()\ndf_imp.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n\n\n1\n7.677934\n2600.0\nwhite\n3\n\n\n2\n0.407678\n500.0\nwhite\n0\n\n\n3\n0.798369\n1300.0\nwhite\n0\n\n\n4\n17.513165\n1100.0\nwhite\n1\n\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_imp['risk_percentile'] = round(df['risk_score_t'].rank(pct = True), 2) * 100\n\ngrouped_data = df_imp.groupby(['risk_percentile', 'race'])['gagne_sum_t'].mean()\n\ngrouped_data = pd.DataFrame(grouped_data)\n\nsns.scatterplot(data=grouped_data, x='gagne_sum_t', y='risk_percentile', hue='race')\n\nplt.xlabel('Mean Number of Chronic Illnesses')\nplt.ylabel('Percentile Risk Score')\nplt.title('Mean number of chronic illnesses versus algorithm predicted risk by race')\n\nplt.legend(title='Race')\nplt.show()\n\n\n\n\n\n\n\n\nThis plot begins to unravel some underlying racial bias that could be present in the algorithm. As seen in the figure, the orange dots representing white patients lie on a higher curve of percentile risk score. This suggests that less sick white patients are more likely to be referred to a high-risk program than sicker black patients.\n\n\n\n\nfig, axes = plt.subplots(1, 2, sharey = True)\n\ngrouped_cost_data = pd.DataFrame(df_imp.groupby(['risk_percentile', 'race'])['cost_t'].mean())\ngrouped_illness_data = pd.DataFrame(df_imp.groupby(['gagne_sum_t', 'race'])['cost_t'].mean())\n\nsns.scatterplot(ax = axes[0], data=grouped_cost_data, x='risk_percentile', y='cost_t', hue='race')\nsns.scatterplot(ax = axes[1], data=grouped_illness_data, x='gagne_sum_t', y='cost_t', hue='race')\n\naxes[0].set_ylabel('Total Medical Expenditure')\naxes[0].set_xlabel('Percentile Risk Score')\naxes[1].set_xlabel('Number of Chronic Illnesses')\n\nplt.ylim(600, 120000)\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\nFrom this chart, we can see that holding either percentile risk score or number of chronic illnesses constant, black patients have lower costs than white patients. Further to the right on each graph, the amount of data available drops off steeply, which explains the more erratic correlation.\n\n\n\n\n\n\ngreater_than_5 = round((df_imp['gagne_sum_t'] &gt;= 5).mean() * 100, 2)\ngreater_than_5\n\n7.0\n\n\nGiven that 93% of the patients in this data set have 5 or less chronic conditions, it seems reasonable to focus on them, at least as a starting point. It is still important to analyze trends for patients with more chronic illnesses, but in the scope of this assignment, it could downplay trends, and without sufficient data, could display incorrect trends.\n\nimport numpy as np\n\ndf_not_0 = df_imp.drop(df_imp[df_imp['cost_t'] == 0].index)\ndf_not_0['log_cost'] = np.log(df_not_0['cost_t'])\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\nrisk_percentile\nlog_cost\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n35.0\n7.090077\n\n\n1\n7.677934\n2600.0\nwhite\n3\n86.0\n7.863267\n\n\n2\n0.407678\n500.0\nwhite\n0\n4.0\n6.214608\n\n\n3\n0.798369\n1300.0\nwhite\n0\n11.0\n7.170120\n\n\n4\n17.513165\n1100.0\nwhite\n1\n98.0\n7.003065\n\n\n\n\n\n\n\n\ndf_not_0 = pd.get_dummies(df_not_0)\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_black\nrace_white\n\n\n\n\n0\n1.987430\n1200.0\n0\n35.0\n7.090077\n0\n1\n\n\n1\n7.677934\n2600.0\n3\n86.0\n7.863267\n0\n1\n\n\n2\n0.407678\n500.0\n0\n4.0\n6.214608\n0\n1\n\n\n3\n0.798369\n1300.0\n0\n11.0\n7.170120\n0\n1\n\n\n4\n17.513165\n1100.0\n1\n98.0\n7.003065\n0\n1\n\n\n\n\n\n\n\n\nX_train = df_not_0[['gagne_sum_t', 'race_black']].rename(columns={'race_black' : 'race'})\ny_train = df_not_0[['log_cost']]\nX_train.head(10)\n\n\n\n\n\n\n\n\ngagne_sum_t\nrace\n\n\n\n\n0\n0\n0\n\n\n1\n3\n0\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n1\n0\n\n\n5\n1\n0\n\n\n6\n1\n0\n\n\n7\n0\n0\n\n\n8\n1\n1\n\n\n9\n0\n0\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nLR = LinearRegression()\nLR.fit(X_train, y_train)\n\n# adds {degree} polynomial features to {X}\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nNow, I will add a variable number of features to X_train and see which produces the best accuracy with cross-validation.\n\ncv_scores = []\n\n# I will check how adding up to 10 additional polynomial features impacts the score\nfor i in range(0, 10):\n    X_deg = add_polynomial_features(X_train, i)\n    cv_scores.append(cross_val_score(LR.fit(X_deg, y_train), X_deg, y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores)\nbest_deg = np.argmax(cv_scores)\n\nprint(best_score)\nprint(best_deg)\n\n0.14820529981684197\n9\n\n\nIt looks like a polynomial with degree 9 was most successful with this data, so I will fit one additional linear regression model with this degree before proceeding.\n\nbest = add_polynomial_features(X_train, best_deg)\n\nLR.fit(best, y_train)\nw = LR.coef_\nprint(w)\n\n[[ 3.92483807e-01 -2.67357533e-01  3.92483807e-01 -5.63208481e-01\n   2.48313944e-01 -5.61461922e-02  7.08781442e-03 -5.05267129e-04\n   1.89805627e-05 -2.91722116e-07]]\n\n\n\n# as the coefficients are in the order of the data input\n# the second weight should correspond to the race column\n# this can be used to calculate the estimate cost incurred by black patients as a percent of that incurred by white patients\nb_coef = w[0, 1]\nb_coef\n\n-0.26735753310360466\n\n\n\ncost_incurred = np.exp(b_coef)\ncost_incurred\n\n0.7653993669279184\n\n\nSo, it looks like black patients, on average, pay about 3/4 of what white patients pay for health care. This is more or less following the argument of Obermeyer et al. (2019). I note that my percentage shows significantly larger disparity than what was found in the paper, but but show that black patients generate lower costs than white patients.\nIn this blog post, I found that preparing, processing, and cleaning data can take a lot of work to achieve the desired result. From the recreated figure 1, I found that on average, white patients are rated as having a higher risk score than black patients with the same amount of chronic illnesses. This puts black patients at risk because they are less likely to be referred to a high-risk program that can provide additional health care support. In figure 3, it was clear that, particularly for lower numbers of chronic illnesses (for which there was a lot more data), white patients typically spent more money than black patients. This was further emphasized by the analysis done of the performed linear regression, as described above. In my opinion, separation is the model of fairness that most clearly demonstrates the bias in this data set; the finding that sicker black patients don’t qualify as frequently as less sick white patients suggests a higher false negative rate for black patients. This discrepancy in error rates leads to more black patients not receiving additional care, despite being qualified for it, and can lead to them falling through holes in the health care system without necessary support."
  },
  {
    "objectID": "posts/BP11/BP11.html",
    "href": "posts/BP11/BP11.html",
    "title": "Blog Post 11: Deep Music Genre Classification",
    "section": "",
    "text": "Blog Post 11: Deep Music Genre Classification\nWell, I’ve been hoping to do a deep learning blog post all semester, so I’m really excited to give this one a shot! The first step will be to properly format the data, and then perform text vectorization, since lyrics are words, which cannot function as features. Then, I will collate batches. Finally, I will be prepared to create a model and evaluate its accuracy. I will create 3 models, one learning from just the lyrics, one learning from just the features, and one learning from both. Once the models have successfully performed better than the base rate, I will compare the accuracy across the three models to see which one would be the most promising to continue to work with.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy'] \n\n\nprint(df[\"genre\"].unique())\n\ngenres = {'pop': 0, 'country': 1, 'blues': 2, 'jazz': 3, 'reggae': 4, 'rock': 5, 'hip hop': 6}\ndf[\"genre\"] = df[\"genre\"].apply(genres.get)\n\n# check to make sure all the genres were successfully converted to numbers\nprint(df[\"genre\"].unique())\n\ndf.head(3)\n\n['pop' 'country' 'blues' 'jazz' 'reggae' 'rock' 'hip hop']\n[0 1 2 3 4 5 6]\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\n0\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\n0\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\n0\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n\n\n3 rows × 31 columns\n\n\n\n\ndf.groupby(\"genre\").size() / len(df)\n\ngenre\n0    0.248202\n1    0.191915\n2    0.162273\n3    0.135521\n4    0.088045\n5    0.142182\n6    0.031862\ndtype: float64\n\n\nLooks like if we guess genre 0 (pop) every time, we will get a base accuracy of 24.8%. Let’s see if we an do better than this.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n    \n    def __getitem__(self, index):\n        target = self.df['genre'].iloc[index]\n        lyrics = self.df['lyrics'].iloc[index]\n        features = self.df[engineered_features].iloc[index]\n        return target, lyrics, features\n\n    def __len__(self):\n        return len(self.df)                \n\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_val = train_test_split(df, shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data   = TextDataFromDF(df_val)\n\n\ntrain_data[193]\n\n(2,\n 'walk talk breathe try smile death finger pulse time time rest simple proposal walk line try borrow time moral story limit crew gonna choose tomorrow scene easy turn money green promise future wrong line walk keep take long rainbow morning follow gonna live tomorrow like live today borrow look stay',\n dating                      0.001224\n violence                    0.115240\n world/life                  0.354443\n night/time                  0.170155\n shake the audience          0.001224\n family/gospel               0.001224\n romantic                    0.001224\n communication               0.001224\n obscene                     0.001224\n music                       0.001224\n movement/places             0.255235\n light/visual perceptions    0.001224\n family/spiritual            0.001224\n like/girls                  0.041008\n sadness                     0.001224\n feelings                    0.001224\n danceability                0.352323\n loudness                    0.597134\n acousticness                0.000141\n instrumentalness            0.327935\n valence                     0.704246\n energy                      0.720712\n Name: 13728, dtype: float64)\n\n\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\ntokenized = tokenizer(train_data[193][1])\ntokenized\n\n['walk',\n 'talk',\n 'breathe',\n 'try',\n 'smile',\n 'death',\n 'finger',\n 'pulse',\n 'time',\n 'time',\n 'rest',\n 'simple',\n 'proposal',\n 'walk',\n 'line',\n 'try',\n 'borrow',\n 'time',\n 'moral',\n 'story',\n 'limit',\n 'crew',\n 'gonna',\n 'choose',\n 'tomorrow',\n 'scene',\n 'easy',\n 'turn',\n 'money',\n 'green',\n 'promise',\n 'future',\n 'wrong',\n 'line',\n 'walk',\n 'keep',\n 'take',\n 'long',\n 'rainbow',\n 'morning',\n 'follow',\n 'gonna',\n 'live',\n 'tomorrow',\n 'like',\n 'live',\n 'today',\n 'borrow',\n 'look',\n 'stay']\n\n\n\ndef yield_tokens(data_iter):\n    for target, text, _ in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"&lt;unk&gt;\"], min_freq = 50)\nvocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\n\nimport torch\n\nmax_len = 30\nnum_tokens = len(vocab.get_itos())\ndef text_pipeline(x):\n    tokens = vocab(tokenizer(x))\n    y = torch.zeros(max_len, dtype=torch.int64) + num_tokens\n    if len(tokens) &gt; max_len:\n        tokens = tokens[0:max_len]\n    y[0:len(tokens)] = torch.tensor(tokens,dtype=torch.int64)\n    return y\n\nlabel_pipeline = lambda x: int(x)\n\n\ntext_pipeline(\"we cant believe\")\n\ntensor([   0,    0,   44, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897,\n        2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897,\n        2897, 2897, 2897, 2897, 2897, 2897])\n\n\n\nimport numpy as np\n\ndef collate_batch(batch):\n    target_label_list = []\n    lyric_text_list = []\n    features_list = []  \n\n    for (_targets, _lyrics, _features) in batch:\n        # process targets\n        target_label_list.append(label_pipeline(_targets))\n\n        # process lyrics\n        processed_lyrics = text_pipeline(_lyrics)\n        lyric_text_list.append(processed_lyrics)\n\n        # process features\n        features_list.append(_features.to_numpy())\n\n    target_label_list = torch.tensor(target_label_list, dtype=(torch.int64))\n    lyric_text_list = torch.stack(lyric_text_list)\n    features_list = torch.tensor((features_list), dtype=torch.float64)\n\n    return target_label_list, lyric_text_list, features_list\n\n\ntrain_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\n\n\nnext(iter(train_loader))\n\n(tensor([0, 0, 2, 0, 1, 6, 4, 2]),\n tensor([[  35,    1,  108,   92,    1,   62,    0,  295,  604,    2,  481,   35,\n            77,    9,   14,  228,  246,  276,   35,   35,   58,   58,   58,   58,\n           127,   96,   35,   81,   32,   25],\n         [1264, 2779,   17,    0,    0,    0,  281,  830,  218,    0,   74,  167,\n           862, 1878, 1762, 1117, 1454,  256,    0,   55,  154,  154,    0, 1739,\n             0,    0,    0, 1473,  392,  496],\n         [ 815,  439,  717,   29, 1252, 2876,  147,    0,  140,   84, 1222,  311,\n          1669, 1295,  815,  439,  717,   29, 1252, 2876,  147,    0,  140,   84,\n          1222,  311, 1669, 1295, 2897, 2897],\n         [   0,  382,    0,    0,    0, 1963,  139,    0, 1942,    0, 1097, 2897,\n          2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897, 2897,\n          2897, 2897, 2897, 2897, 2897, 2897],\n         [   0,   33,    0,    0,   82, 2469,    0,   81,  160,  170,  257,    0,\n           259,   33,   77, 2091,  527, 1901, 1174,  148,  684,   19,  261,    0,\n             0,    0,   95,  100,   45,  783],\n         [  38,  331,  298,    0,  780,    0,   35,   35,   81,  174,    0,    0,\n           516,    0,  142,    1,    0,   79,  951, 2374,   95,  806,   15,    0,\n           212,   17,   12,    0,  557,    0],\n         [ 716,    0,   68,  511,   28, 2314,  924,   29, 1915, 1551,  120,  583,\n            21, 1227,   68,  193,  885, 2234, 1232,   83, 2897, 2897, 2897, 2897,\n          2897, 2897, 2897, 2897, 2897, 2897],\n         [  12,  183,   41,  197,  150,   55,  459,   11,   94,  197,  150,   55,\n          2000,  262, 1011,    4, 1449,  140,  541,    5,  484,    4,  156,  183,\n           196,  197,  150,   55, 2000,  262]]),\n tensor([[8.4890e-04, 8.4890e-04, 8.4890e-04, 4.0088e-02, 8.4890e-04, 8.4890e-04,\n          3.6078e-02, 6.1065e-02, 2.2078e-02, 1.9050e-01, 8.4890e-04, 8.4890e-04,\n          8.4890e-04, 2.9738e-02, 4.1083e-01, 2.2231e-02, 4.1514e-01, 6.9546e-01,\n          6.9779e-01, 1.6194e-05, 1.1068e-01, 5.3752e-01],\n         [1.8797e-03, 2.0632e-01, 3.6854e-01, 1.8797e-03, 1.8797e-03, 1.8797e-03,\n          1.8797e-03, 1.8797e-03, 1.8797e-03, 2.3701e-01, 1.8797e-03, 8.1619e-02,\n          3.8763e-02, 1.8797e-03, 1.8797e-03, 4.3311e-02, 3.8915e-01, 7.0638e-01,\n          1.5261e-01, 4.4939e-01, 5.0330e-01, 6.6065e-01],\n         [4.0486e-03, 9.2713e-01, 4.0486e-03, 4.0486e-03, 4.0486e-03, 4.0486e-03,\n          4.0486e-03, 4.0486e-03, 4.0486e-03, 4.0486e-03, 4.0486e-03, 4.0486e-03,\n          4.0486e-03, 4.0486e-03, 4.0486e-03, 4.0486e-03, 3.6424e-01, 7.0989e-01,\n          7.6807e-05, 4.6964e-03, 4.5383e-01, 7.5975e-01],\n         [1.7544e-02, 3.5088e-01, 1.7544e-02, 1.7544e-02, 1.7544e-02, 1.7544e-02,\n          1.7544e-02, 1.7544e-02, 1.7544e-02, 1.7544e-02, 1.7544e-02, 1.7544e-02,\n          1.7544e-02, 3.5088e-01, 1.7544e-02, 1.7544e-02, 5.2345e-01, 7.4348e-01,\n          3.8252e-02, 0.0000e+00, 3.3739e-01, 4.6645e-01],\n         [1.1198e-03, 3.2208e-01, 1.1323e-01, 1.1198e-03, 1.1198e-03, 1.1198e-03,\n          2.7500e-02, 3.3270e-01, 9.0046e-02, 1.1198e-03, 1.1198e-03, 1.1198e-03,\n          5.5796e-02, 1.1198e-03, 1.1198e-03, 1.1198e-03, 4.1189e-01, 4.5812e-01,\n          9.1064e-01, 1.3360e-01, 3.8170e-01, 3.3531e-01],\n         [5.3706e-04, 4.7196e-02, 1.3929e-01, 5.3706e-04, 5.3706e-04, 5.3706e-04,\n          5.3706e-04, 6.2527e-02, 5.7175e-01, 5.6773e-02, 5.3706e-04, 5.3706e-04,\n          5.3706e-04, 5.3706e-04, 5.3706e-04, 1.7871e-02, 8.1263e-01, 6.4026e-01,\n          1.7469e-02, 2.0243e-06, 9.1138e-01, 6.2962e-01],\n         [4.7847e-03, 4.7847e-03, 4.7847e-03, 4.7847e-03, 4.7847e-03, 4.7847e-03,\n          4.7847e-03, 4.7847e-03, 4.7847e-03, 4.2536e-01, 4.7847e-03, 2.5726e-01,\n          2.4082e-01, 4.7847e-03, 4.7847e-03, 4.7847e-03, 7.3032e-01, 6.6374e-01,\n          5.1505e-02, 5.3441e-01, 6.1871e-01, 6.0760e-01],\n         [7.4699e-02, 4.7351e-01, 1.3495e-03, 1.3495e-03, 1.3495e-03, 1.3495e-03,\n          1.3495e-03, 1.3495e-03, 1.3495e-03, 1.2202e-01, 1.3495e-03, 1.3495e-03,\n          1.3495e-03, 1.3495e-03, 2.8153e-01, 1.3495e-03, 4.7796e-01, 7.9545e-01,\n          7.6305e-01, 0.0000e+00, 8.3203e-01, 4.6745e-01]], dtype=torch.float64))\n\n\n\nfrom torch import nn\n\nclass LyricClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, max_len, num_class, dropout_p):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc = nn.Linear(max_len*embedding_dim, num_class)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        x = self.dropout(x)\n        return(x)\n\n\nvocab_size = len(vocab)\nembedding_dim = 3\nnum_class = 7\ndropout_p = 0.2\nmodel = LyricClassificationModel(vocab_size, embedding_dim, max_len, num_class, dropout_p)\n\n\nimport time\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\nloss_fn = torch.nn.CrossEntropyLoss()\n\ndef train(dataloader, d_type):\n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n    log_interval = 300\n    start_time = time.time()\n\n    for idx, (label, text, features) in enumerate(dataloader):\n        # zero gradients\n        optimizer.zero_grad()\n\n        if d_type == 'lyrics':\n            input_data = text\n        elif d_type == 'features':\n            input_data = features[idx]\n        else:  # 'both'\n            input_data = (text, features[idx])\n\n        # form prediction on batch\n        predicted_label = model(input_data)\n\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, label)\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n\n        # for printing accuracy\n        total_acc   += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n        \n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n    \ndef evaluate(dataloader, d_type):\n\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (label, text, features) in enumerate(dataloader):\n\n            if d_type == 'lyrics':\n                input_data = text\n            elif d_type == 'features':\n                input_data = features[idx]\n            else:  # 'both'\n                input_data = (text, features[idx])\n\n            predicted_label = model(input_data)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc/total_count\n\n\nEPOCHS = 20\nfor epoch in range(1, EPOCHS + 1):\n    train(train_loader, 'lyrics')\n\n| epoch   1 | train accuracy    0.204 | time: 44.38s\n| epoch   2 | train accuracy    0.231 | time: 45.90s\n| epoch   3 | train accuracy    0.261 | time: 48.69s\n| epoch   4 | train accuracy    0.276 | time: 44.80s\n| epoch   5 | train accuracy    0.281 | time: 47.53s\n| epoch   6 | train accuracy    0.292 | time: 44.49s\n| epoch   7 | train accuracy    0.290 | time: 42.61s\n| epoch   8 | train accuracy    0.300 | time: 42.15s\n| epoch   9 | train accuracy    0.296 | time: 47.53s\n| epoch  10 | train accuracy    0.301 | time: 50.76s\n| epoch  11 | train accuracy    0.305 | time: 51.45s\n| epoch  12 | train accuracy    0.300 | time: 49.81s\n| epoch  13 | train accuracy    0.302 | time: 49.77s\n| epoch  14 | train accuracy    0.300 | time: 49.35s\n| epoch  15 | train accuracy    0.302 | time: 48.89s\n| epoch  16 | train accuracy    0.300 | time: 45.99s\n| epoch  17 | train accuracy    0.299 | time: 44.35s\n| epoch  18 | train accuracy    0.303 | time: 44.38s\n| epoch  19 | train accuracy    0.303 | time: 44.09s\n| epoch  20 | train accuracy    0.298 | time: 45.52s\n\n\n\nevaluate(val_loader, 'lyrics')\n\n0.25127753303964756\n\n\n\nfrom torch import nn\n\nclass FeaturesClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, max_len, num_class, dropout_p):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc = nn.Linear(max_len*embedding_dim, num_class)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        x = self.dropout(x)\n        return(x)\n\n\nEPOCHS = 20\nfor epoch in range(1, EPOCHS + 1):\n    train(train_loader, 'features')\n\nRuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.DoubleTensor instead (while checking arguments for embedding)\n\n\n: \n\n\n\nevaluate(val_loader, 'features')\n\n\nEPOCHS = 20\nfor epoch in range(1, EPOCHS + 1):\n    train(train_loader, 'both')\n\n\nevaluate(val_loader, 'both')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog Post: Classifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 11: Deep Music Genre Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: ‘Optimal’ Decision Making\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Replication Study\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Women in Data Science\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Perceptron\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Newton’s Method for Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Some new text here"
  },
  {
    "objectID": "posts/BP1/BP1.html",
    "href": "posts/BP1/BP1.html",
    "title": "Blog Post: Classifying Palmer Penguins",
    "section": "",
    "text": "Blog Post: Classifying Palmer Penguins\nIn this blog post, I aim to gain familiarity with pandas tools used for machine learning. I will be exploring a data set about penguins, and attempting to classify them by species based on individual quantitative and qualitative attributes. First, I will take a look at the data and clean it so that it is ready for use. Then I will get familiar with the data through graphical representations, in an effort to make the best decision about which features to use in modelling. Using a reproducible process, I will determine which 3 attributes (1 qualitative and 2 quantitative) should be used for the model. Cross-validation on the training data will be tested with several different models to determine the best model before moving on to the test data. Once the best model is determined, it will be run on the test data, with a goal of 100% accuracy of classification. To analyze the work, I will plot the species regions for both test and training data, and look at a confusion matrix before summing up my findings.\nBefore beginning analysis, the training data must be accessed.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nBelow is a sample of what it looks like:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nClearly, this is a LOT of data. Furthermore, not all of it is super useful. For example, the studyName and Sample Number columns aren’t meaningful to this analysis. Some other columns, like Sex and Island need to be reformatted to allow for analysis. Now, I will clean the data.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.fit_transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\n\n\nExplore\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = X_train['Flipper Length (mm)']\ny = X_train['Body Mass (g)']\n\nax = sns.scatterplot(data = X_train, x = 'Flipper Length (mm)', y = 'Body Mass (g)', hue = y_train)\n\nh,l = ax.get_legend_handles_labels()\n\nl = [\"Gentoo Penguin\", \"Chinstrap Penguin\", \"Adelie Penguin\"]\n\nax.legend(h, l)\nplt.show()\n\n\n\n\n\n\n\n\nTo be honest, I started out with some quantitative attributes that seemed interesting. Tossing them onto a graph organized by species seems to be a good way to get a feel of whether they could successfully be used as classifiers. While the combination of body mass and flipper length looks like it may be able to identify (2) penguins, it would be extremely difficult to differentiate penguins of species (0) and (1). Thus, I determined I should continue searching for more ideal attributes prior to modeling.\n\nx = X_train['Delta 15 N (o/oo)']\ny = X_train['Delta 13 C (o/oo)']\n\nax = sns.scatterplot(data=X_train, x = 'Delta 15 N (o/oo)', y = 'Delta 13 C (o/oo)', hue = y_train)\n\nh,l = ax.get_legend_handles_labels()\n\nl = [\"Gentoo Penguin\", \"Chinstrap Penguin\", \"Adelie Penguin\"]\n\nax.legend(h, l)\nplt.show()\n\n\n\n\n\n\n\n\nI threw a few new quantitative attributes on a graph to get an idea of how they compared. Once again, looking at the graph, I don’t have super high hopes that these would be successful as classifiers. It does appear that (1) penguins have a higher Delta 13 C (o/oo) than other species. (0) penguins overlapped with (1) penguins heavily for Delta 15 N (o/oo). (0) penguins also overlap heavily for Delta 13 C (o/oo) with (2) penguins, leading to a prediction that classification would be difficult.\n\nX_train.groupby(y_train)[['Island_Biscoe', 'Island_Dream', 'Island_Torgersen']].mean()\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\n\n\n\n\n0\n0.305556\n0.37963\n0.314815\n\n\n1\n0.000000\n1.00000\n0.000000\n\n\n2\n1.000000\n0.00000\n0.000000\n\n\n\n\n\n\n\nHere, I felt like I hit the jackpot. It seems that (1) penguins were only found on Island_Dream, and (2) pengiuns were only found on Island_Biscoe. Although (0) penguins are spread equally among all three islands, this simplifies the problem significantly. However, it is still important to run some tests and be aware of any possible overfitting.\nWith a few interesting graphics, and a summary table, it’s time to start modeling.\n\n\nModeling\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\n\nx_new = SelectKBest(mutual_info_classif, k = 3)\nx_new.fit_transform(X_train, y_train)\nx_new.get_feature_names_out()\n\narray(['Culmen Depth (mm)', 'Flipper Length (mm)', 'Delta 13 C (o/oo)'],\n      dtype=object)\n\n\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.001, penalty=\"l1\", dual=False).fit(X_train, y_train)\nmodel = SelectFromModel(lsvc, prefit=True).fit(X_train, y_train)\nx_new = model.transform(X_train)\nmodel.get_feature_names_out()\n\nc:\\Users\\Zoe Greenwald\\anaconda3\\envs\\ml-0451-2\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\narray(['Culmen Length (mm)', 'Flipper Length (mm)', 'Body Mass (g)'],\n      dtype=object)\n\n\nAfter trying two methods of features selection and getting only quantitative features, I’m going to give the brute force method a shot.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nqual_cols = [\"Island\", \"Clutch Completion\", \"Stage_adult\", \"Sex\"]\nquant_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nbest_score = 0.0\nbest_cols = []\n\nfor qual in qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression(solver='lbfgs', max_iter = 10000)\n    LR.fit(X_train[cols], y_train)\n    score = LR.score(X_train[cols], y_train)\n\n    if score &gt; best_score:\n      best_score = score\n      best_cols = cols\n\n# puts the qualitative columns at the end \nbest_cols = best_cols[::-1]\n\nprint(best_cols)\nprint(best_score)\n\n['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Torgersen', 'Island_Dream', 'Island_Biscoe']\n0.99609375\n\n\nSuccess! I may have forced it by looking only at groups of 3 that included 1 qualitative column, but I’ve managed to reproducibly find the columns I want to use for my modelling. Culmen depth, culmen length, and island seem to have a high success rate for classifying the three penguin species.\n\ncv_scores_LR = cross_val_score(LR, X_train[best_cols], y_train, cv=5).mean()\nprint(cv_scores_LR)\n\n0.9883107088989442\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\ndepths = [*range(3, 20)]\ncv_scores_dtc = []\n\nfor val in depths:\n    dtc = DecisionTreeClassifier(criterion=\"gini\", max_depth=val).fit(X_train[best_cols], y_train)\n    cv_scores_dtc.append(cross_val_score(dtc, X_train[best_cols], y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores_dtc)\nbest_depth = depths[np.argmax(cv_scores_dtc)]\nprint(best_score)\nprint(best_depth)\n\n0.9765460030165913\n7\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrees = [*range(10, 100)]\ncv_scores_rf = []\n\nfor val in trees:\n    rf = RandomForestClassifier(n_estimators = val).fit(X_train[best_cols], y_train)\n    cv_scores_rf.append(cross_val_score(rf, X_train[best_cols], y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores_rf)\nbest_depth = trees[np.argmax(cv_scores_rf)]\nprint(best_score)\nprint(best_depth)\n\n0.9843891402714933\n11\n\n\n\n\nTesting\nLooking at the scores of the best fits of all the tested models, it looks like LR pulls ahead by just a fraction of a percent, so I’ll go ahead and try that on the test data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR.fit(X_train[best_cols], y_train)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nWe did it! With enough modeling and praying, we got 100% accuracy on the test data! Woohoo!!!\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"upper right\", bbox_to_anchor = (2.5, 0.75))\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[best_cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\nTo finish up, lets take a look at the confusion matrix to see what kind of mistakes our model made (even though our model didn’t make any mistakes on the test data).\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[best_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]], dtype=int64)\n\n\nGiven that our model had 100% accuracy on the test set, this was the confusion matrix I was expecting. The only non-zero numbers are along the diagonal, signifying that every penguin was correctly classified as its own species.\nTo close out this first blog post, I will first discuss my results, and then my learnings. I found that I was able to use machine learning on training data to prepare an group of features and model to successfully classify penguins into their respective species group. The optimal features ended up being culmen length, culmen depth, and what island the penguin was found on, and I determined the ideal model to be linear regression after using cross-validation on several different models before applying the algorithm to the test data. I was excited to meet Phil’s request for 100% accuracy on the test data. Some further analysis was done to compare the decision regions and their boundaries between the train and test data sets. I learned a lot about pandas feature selections and models in this assignment, along with the basic flow of using machine learning in a data science project."
  },
  {
    "objectID": "posts/BP2/BP2.html",
    "href": "posts/BP2/BP2.html",
    "title": "Blog Post: ‘Optimal’ Decision Making",
    "section": "",
    "text": "In this blog post, I aim to analyze data regarding the likelihood that the bank will grant a loan based on a group of personal attributes. First, I will find explore the data set to observe trends, and then I will determine the best attributes for successfully predicted whether a borrower will default on a loan. Then, I will build a model and determine an ideal threshold before analyzing the test data. Finally, I will observe the accuracy and profit with which my model performs on the test data from the point of view of the bank and the borrower.\nOnce again, to begin, the training data must be accessed.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nHere’s a look at the data:\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nTime to clean the data and take another look. I won’t one hot encode the columns yet, because I’m going to use some of them to explore the data set.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nle.fit(df_train['loan_status'])\n\ndef prepare_data(df):\n    df = df.drop(['loan_grade'], axis=1)\n    df = df.dropna()\n    y = le.fit_transform(df['loan_status'])\n    df = df.drop(['loan_status'], axis = 1)\n\n    return df, y\n\nX_train, y_train = prepare_data(df_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\n11750\n13.47\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\n10000\n7.51\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\n1325\n12.87\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\n15000\n9.63\n0.28\nN\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\n5500\n14.91\n0.25\nN\n2\n\n\n\n\n\n\n\n\n\nLet’s get familiar with some of the attributes and decide which ones might be helpful to predict whether someone defaulted on a loan or not.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nx = X_train['person_home_ownership']\ny = X_train['loan_percent_income']\n\nax = sns.barplot(data=X_train, x = 'person_home_ownership', y = 'loan_percent_income', hue = y_train.astype(str))\n\nh,l = ax.get_legend_handles_labels()\nl = [\"Repaid Loan\", \"Defaulted\"]\nax.legend(h, l)\n\nax.set_title(\"Percent Income vs. Home Ownership\")\nax.set_xlabel(\"Home Ownership\")\nax.set_ylabel(\"Loan Percent of Income\")\n\nplt.show()\n\n\n\n\n\n\n\n\nLooking at different types of home ownership, it looks like the percent income of the loan plays a really big role in whether or not it gets defaulted.\n\nx = X_train['loan_percent_income']\ny = X_train['loan_int_rate']\n\nax = sns.scatterplot(data=X_train, x = 'loan_percent_income', y = 'loan_int_rate', hue = y_train.astype(str), alpha = 0.5)\n\nh,l = ax.get_legend_handles_labels()\nl = [\"Repaid Loan\", \"Defaulted\"]\nax.legend(h, l)\n\nax.set_title(\"Interest Rate vs. Percent Income\")\nax.set_xlabel(\"Loan Percent of Income\")\nax.set_ylabel(\"Loan Interest Rate\")\n\nplt.show()\n\n\n\n\n\n\n\n\nWell, unsurprisingly, it looks like the chance that a loan is fully repaid is increased as the interest rate falls, and as the percentage of a person’s income falls.\n\nX_train.groupby([y_train, 'cb_person_default_on_file'])[['person_emp_length', 'cb_person_cred_hist_length']].mean()\n\n\n\n\n\n\n\n\n\nperson_emp_length\ncb_person_cred_hist_length\n\n\n\ncb_person_default_on_file\n\n\n\n\n\n\n0\nN\n4.987118\n5.809296\n\n\nY\n4.782471\n5.928938\n\n\n1\nN\n4.161868\n5.603114\n\n\nY\n4.166886\n5.735217\n\n\n\n\n\n\n\nPeople who repaid the loan fully have generally been employed for a bit longer. I was surprised by how little difference it made to have a previous default on file. Also, while credit history length decreased slightly for loans that were defaulted, it didn’t seem to be by a significant amount.\n\n\n\nNow that I’m ready to start modeling, I’ll go ahead an one hot encode any qualitative columns.\n\nX_train = pd.get_dummies(X_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\nI’m going to try recursive feature elimination to assign weights to different features in the training data and select the best ones.\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nestimator = LogisticRegression()\n\ncols = RFE(estimator, n_features_to_select=3, step=1).fit(X_train, y_train)\ncols_rfe = cols.get_feature_names_out()\ncols_rfe\n\narray(['person_income', 'loan_amnt', 'loan_int_rate'], dtype=object)\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\nlr_rfe = LogisticRegression().fit(X_train[cols_rfe], y_train)\n\ncv_scores_rfe = cross_val_score(lr_rfe, X_train[cols_rfe], y_train, cv=5).mean()\ncv_scores_rfe\n\n0.8117172718507574\n\n\nRFE uses recursion consider smaller and smaller sets of features and selects the best (3 in this case) for a model. Based on this model, the best features are income, loan amount, and loan interest rate, and the model has an accuracy of about 81%. I’m going to try a few other models before I choose what features to use for my final model.\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\n\ncols = SelectKBest(mutual_info_classif, k = 3)\ncols.fit_transform(X_train, y_train)\ncols_skb = cols.get_feature_names_out()\ncols_skb\n\narray(['person_income', 'loan_int_rate', 'loan_percent_income'],\n      dtype=object)\n\n\n\nlr_skb = LogisticRegression().fit(X_train[cols_skb], y_train)\n\ncv_scores_skb = cross_val_score(lr_skb, X_train[cols_skb], y_train, cv=5).mean()\ncv_scores_skb\n\n0.8021563455835603\n\n\nSelectKBest uses statistical tests to rank features based on their relationship with the outcome variables. It selected income, interest rate, and loan percent income as the best features with an accuracy of 80% (just a bit lower than RFE). I note that 2/3 of these features agree with RFE.\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.0001, penalty=\"l1\", dual=False).fit(X_train, y_train)\nmodel = SelectFromModel(lsvc, prefit=True).fit(X_train, y_train)\nx_new = model.transform(X_train)\ncols_svc = model.get_feature_names_out()\ncols_svc\n\narray(['person_age', 'loan_amnt', 'loan_int_rate'], dtype=object)\n\n\n\nlr_svc = LogisticRegression().fit(X_train[cols_svc], y_train)\n\ncv_scores_svc = cross_val_score(lr_svc, X_train[cols_svc], y_train, cv=5).mean()\ncv_scores_svc\n\n0.7949095723125648\n\n\nLinearSVC uses a random number generator to select features - this makes sense as we’re seeing less feature overlap, and slightly lower accuracy. Since it looks like RFE worked the best to select features so I’ll stick with person_income, loan_int_rate, and loan_percent_income as the attributes for the remainder of the project.\n\nw = lr_rfe.coef_.T\nw\n\narray([[-4.05735976e-05],\n       [ 1.06558819e-04],\n       [ 9.49045880e-08]])\n\n\n\ndef calc_score(X, w):\n    return X@w\n\n\nX_train['score'] = calc_score(X_train[cols_rfe], w)\n\n\n\n\n\nimport numpy as np\n\n# create columns for the profit of the bank if the loan is not defaulted and the cost if it is\nX_train['profit'] = X_train[\"loan_amnt\"]*(1 + 0.25*X_train[\"loan_int_rate\"] * 0.01)**10 - X_train[\"loan_amnt\"]\nX_train['cost'] = X_train[\"loan_amnt\"]*(1 + 0.25*X_train[\"loan_int_rate\"] * 0.01)*3 - 1.7*X_train[\"loan_amnt\"]\n\nprofit = []\nacc = []\n\nfor t in np.linspace(-2, 0, 41):\n    #if score &lt;= t, they get the loan\n    y_pred = X_train['score'] &gt; t\n\n    # when y_pred and y_train equal one, the bank correctly predicted a default and profit is 0\n    # when y_pred and y_train equal zero, the bank correctly predicted a repaid loan and profit +X_train['profit']\n    # when y_pred = 1, y_train = 0 the bank predicted a default and did not give the loan, so profit is 0\n    # when y_pred = 0, y_train = 1 the bank incorrectly predicted the loan would be repaid, so profit is -X_train['cost']\n\n    profit.append((((y_pred != y_train) * (y_pred == 0) * (-X_train['cost'])) + ((y_pred == y_train) * (y_pred == 0) * X_train['profit'])).mean())\n    acc.append((y_pred == y_train).mean())\n\n# making a plot of the threshold data from above loop\nplt.plot(np.linspace(-2, 0, 41), profit)\nplt.title(\"Profit vs. Threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Profit per Borrower\")\nplt.show()\n\nt_best = -2 + np.argmax(np.array(profit)) * 0.05\nprint(f\"A threshold of {t_best:.2f} gives the maximum profit of ${np.array(profit).max():.2f} per borrower and an accuracy of {acc[np.argmax(np.array(profit))]*100:.0f}%.\")\n\n\n\n\n\n\n\n\nA threshold of -0.70 gives the maximum profit of $608.31 per borrower and an accuracy of 77%.\n\n\nLooking at the plot, we see that from the smallest threshold of -2, the profit rises slowly until it reaches a peak of just over $600 at t = -0.70 and then plummets steeply.\n\n\n\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\n\nX_test, y_test = prepare_data(df_test)\n\nX_test['score'] = calc_score(X_test[cols_rfe], w)\nX_test['profit'] = X_test[\"loan_amnt\"]*(1 + 0.25*X_test[\"loan_int_rate\"] * 0.01)**10 - X_test[\"loan_amnt\"]\nX_test['cost'] = X_test[\"loan_amnt\"]*(1 + 0.25*X_test[\"loan_int_rate\"] * 0.01)*3 - 1.7*X_test[\"loan_amnt\"]\n\n\ny_pred_test = X_test['score'] &gt; t_best\navg_test_profit = ((((y_pred_test != y_test) * (y_pred_test == 0) * (-X_test['cost'])) + ((y_pred_test == y_test) * (y_pred_test == 0) * X_test['profit'])).mean())\ntest_acc = ((y_pred_test == y_test).mean())\n\nprint(f\"A threshold of {t_best:.2f} on the test data set gives an average profit of ${avg_test_profit:.2f} per borrower and an accuracy of {test_acc*100:.0f}%.\")\n\nA threshold of -0.70 on the test data set gives an average profit of $472.68 per borrower and an accuracy of 77%.\n\n\nHonestly, I feel pretty good about this. The expected profit for the test set is a bit over $150 less per person than that of the training data set. Additionally, the accuracy is exactly the same - yay!\n\n\n\n\nages = X_test['person_age'].unique()\nages.sort()\n\npred_d_rates = []\nactual_d_rates = []\n\nfor a in ages:\n    pred_d_rates.append((y_pred_test * (a == X_test['person_age'])).mean() * 100)\n    actual_d_rates.append((y_test * (a == X_test['person_age'])).mean() * 100)\n\nfig, ax = plt.subplots()\nax.plot(ages, pred_d_rates, label=\"Predicted\")\nax.plot(ages, actual_d_rates, label='Actual')\nplt.ylabel(\"Percent Default Rate\")\nplt.xlabel(\"Age\")\nax.legend()\n\n\n\n\n\n\n\n\nIt looks like the bank predicts a much higher rate of default for younger folks. This has a particularly strong impact on people in their 20s, but also seems to have some impact on people in their 30. As seen by the visual above, this does correlate reasonably well with an increased chance of an actual default among these ages.\n\nX_test['y_test'] = y_test\nX_test['y_pred_test'] = y_pred_test\n\nmean_values = X_test.groupby('loan_intent').agg({'y_test': 'mean', 'y_pred_test': 'mean'})\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nbar_width = 0.35\nindex = np.arange(len(mean_values))\n\nax.bar(index, mean_values['y_test'], bar_width, label='Actual')\nax.bar(index + bar_width, mean_values['y_pred_test'], bar_width, label='Predicted')\n\nax.set_xlabel('Loan Intent')\nax.set_ylabel('Mean Values')\nax.set_title('Mean of y_test and y_pred_test Values for Each Loan Intent')\nax.set_xticks(index + bar_width / 2)\nax.set_xticklabels(mean_values.index)\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the actual and predicted mean rates of default based on loan intent. The higher a column is means the higher the chance of default, or the more data points where a default occurred (blue) or was predicted (orange), as defaulting is represented by 1 in this data set. We can see that medical loans have the highest predicted rate of default among all types of loan intent, which means medical loans are the least likely to be granted of any type of loan, or in other words, the most difficult to attain. In this category, its just a bit of an over-prediction of the actual rate of default. On the other hand, the rate of default for business ventures and education loans is steeply over predicted by the model compared to the actual rate.\n\nX_test['inc_percentile'] = round(X_test['person_income'].rank(pct = True), 2) * 100\ngrouped_data = X_test.groupby(['inc_percentile'])['y_pred_test'].mean()\ngrouped_data = pd.DataFrame(grouped_data)\n\nax = sns.scatterplot(data=grouped_data, x='y_pred_test', y='inc_percentile')\nax.set(xlabel = \"Predicted Default Rate\")\nax.set(ylabel = \"Income Percentile\")\n\n\n\n\n\n\n\n\nThis is… well, more or less the trend I expected. As the income percentile increases, so does the chance that the bank grants them a loan. This is seen by the more or less inverse relationship on the plot above.\nAs mentioned earlier, it getting a medical loan can be challenging. In my opinion (supported by the middle view of fairness of Barocas, Hardt, and Narayanan), this is unfair. As they stated, “this view holds that the decision makers have an obligation to avoid perpetuating injustice.” Often times, individuals are born with medical conditions. This is completely out of their control, and doesn’t necessarily guarantee that they possess the economic capital to pay for continuous treatment. In fact, I’d assume that there is a higher chance of being born into a high-risk medical group in lower income brackets, and by this logic, the middle view of fairness is blatantly violated by the difficulty to access medical loans regardless of whether they are paid back.\nIn this blog post, I learned a lot about how difficult it can be to manipulate data with groupby functions to create effective plots. Keeping the number of points to a minimum can help make a clear point and avoid diluting with substantial amounts of noise. The technique I used was splitting data up into percentiles. Then I explored the data, and with recursive feature elimination to determined which features to model (income, loan amount, and loan interest rate) and based off this model, I found a good threshold. After running the model on the test set, I observed that younger folks have more difficulty getting a loan (which makes some sense given their increased risk of default), that medical loans are the most difficult to get, and that business venture and education loans are the categories in which the bank most over-predicts the chance of default. Also, a higher income is a strong contributing factor to being granted a loan."
  },
  {
    "objectID": "posts/BP2/BP2.html#evaluating-the-model-from-the-banks-perspective",
    "href": "posts/BP2/BP2.html#evaluating-the-model-from-the-banks-perspective",
    "title": "Blog Post: ‘Optimal’ Decision Making",
    "section": "",
    "text": "url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\n\nX_test, y_test = prepare_data(df_test)\n\nX_test['score'] = calc_score(X_test[cols_rfe], w)\nX_test['profit'] = X_test[\"loan_amnt\"]*(1 + 0.25*X_test[\"loan_int_rate\"] * 0.01)**10 - X_test[\"loan_amnt\"]\nX_test['cost'] = X_test[\"loan_amnt\"]*(1 + 0.25*X_test[\"loan_int_rate\"] * 0.01)*3 - 1.7*X_test[\"loan_amnt\"]\n\n\ny_pred_test = X_test['score'] &gt; t_best\navg_test_profit = ((((y_pred_test != y_test) * (y_pred_test == 0) * (-X_test['cost'])) + ((y_pred_test == y_test) * (y_pred_test == 0) * X_test['profit'])).mean())\ntest_acc = ((y_pred_test == y_test).mean())\n\nprint(f\"A threshold of {t_best:.2f} on the test data set gives an average profit of ${avg_test_profit:.2f} per borrower and an accuracy of {test_acc*100:.0f}%.\")\n\nA threshold of -0.70 on the test data set gives an average profit of $472.68 per borrower and an accuracy of 77%.\n\n\nHonestly, I feel pretty good about this. The expected profit for the test set is a bit over $150 less per person than that of the training data set. Additionally, the accuracy is exactly the same - yay!"
  },
  {
    "objectID": "posts/BP2/BP2.html#and-the-borrowers-perspective",
    "href": "posts/BP2/BP2.html#and-the-borrowers-perspective",
    "title": "Blog Post: ‘Optimal’ Decision Making",
    "section": "",
    "text": "ages = X_test['person_age'].unique()\nages.sort()\n\npred_d_rates = []\nactual_d_rates = []\n\nfor a in ages:\n    pred_d_rates.append((y_pred_test * (a == X_test['person_age'])).mean() * 100)\n    actual_d_rates.append((y_test * (a == X_test['person_age'])).mean() * 100)\n\nfig, ax = plt.subplots()\nax.plot(ages, pred_d_rates, label=\"Predicted\")\nax.plot(ages, actual_d_rates, label='Actual')\nplt.ylabel(\"Percent Default Rate\")\nplt.xlabel(\"Age\")\nax.legend()\n\n\n\n\n\n\n\n\nIt looks like the bank predicts a much higher rate of default for younger folks. This has a particularly strong impact on people in their 20s, but also seems to have some impact on people in their 30. As seen by the visual above, this does correlate reasonably well with an increased chance of an actual default among these ages.\n\nX_test['y_test'] = y_test\nX_test['y_pred_test'] = y_pred_test\n\nmean_values = X_test.groupby('loan_intent').agg({'y_test': 'mean', 'y_pred_test': 'mean'})\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nbar_width = 0.35\nindex = np.arange(len(mean_values))\n\nax.bar(index, mean_values['y_test'], bar_width, label='Actual')\nax.bar(index + bar_width, mean_values['y_pred_test'], bar_width, label='Predicted')\n\nax.set_xlabel('Loan Intent')\nax.set_ylabel('Mean Values')\nax.set_title('Mean of y_test and y_pred_test Values for Each Loan Intent')\nax.set_xticks(index + bar_width / 2)\nax.set_xticklabels(mean_values.index)\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the actual and predicted mean rates of default based on loan intent. The higher a column is means the higher the chance of default, or the more data points where a default occurred (blue) or was predicted (orange), as defaulting is represented by 1 in this data set. We can see that medical loans have the highest predicted rate of default among all types of loan intent, which means medical loans are the least likely to be granted of any type of loan, or in other words, the most difficult to attain. In this category, its just a bit of an over-prediction of the actual rate of default. On the other hand, the rate of default for business ventures and education loans is steeply over predicted by the model compared to the actual rate.\n\nX_test['inc_percentile'] = round(X_test['person_income'].rank(pct = True), 2) * 100\ngrouped_data = X_test.groupby(['inc_percentile'])['y_pred_test'].mean()\ngrouped_data = pd.DataFrame(grouped_data)\n\nax = sns.scatterplot(data=grouped_data, x='y_pred_test', y='inc_percentile')\nax.set(xlabel = \"Predicted Default Rate\")\nax.set(ylabel = \"Income Percentile\")\n\n\n\n\n\n\n\n\nThis is… well, more or less the trend I expected. As the income percentile increases, so does the chance that the bank grants them a loan. This is seen by the more or less inverse relationship on the plot above.\nAs mentioned earlier, it getting a medical loan can be challenging. In my opinion (supported by the middle view of fairness of Barocas, Hardt, and Narayanan), this is unfair. As they stated, “this view holds that the decision makers have an obligation to avoid perpetuating injustice.” Often times, individuals are born with medical conditions. This is completely out of their control, and doesn’t necessarily guarantee that they possess the economic capital to pay for continuous treatment. In fact, I’d assume that there is a higher chance of being born into a high-risk medical group in lower income brackets, and by this logic, the middle view of fairness is blatantly violated by the difficulty to access medical loans regardless of whether they are paid back.\nIn this blog post, I learned a lot about how difficult it can be to manipulate data with groupby functions to create effective plots. Keeping the number of points to a minimum can help make a clear point and avoid diluting with substantial amounts of noise. The technique I used was splitting data up into percentiles. Then I explored the data, and with recursive feature elimination to determined which features to model (income, loan amount, and loan interest rate) and based off this model, I found a good threshold. After running the model on the test set, I observed that younger folks have more difficulty getting a loan (which makes some sense given their increased risk of default), that medical loans are the most difficult to get, and that business venture and education loans are the categories in which the bank most over-predicts the chance of default. Also, a higher income is a strong contributing factor to being granted a loan."
  },
  {
    "objectID": "posts/BP4/BP4.html",
    "href": "posts/BP4/BP4.html",
    "title": "Blog Post: Women in Data Science",
    "section": "",
    "text": "Blog Post: Women in Data Science\nIn this blog post, I will first seek to better understand the under-representation of women in engineering and the computing sciences, the harms that this causes for everyone, why this happens, and some steps towards improving it. Then, I will discuss the Women in Data Science Conference at Middlebury, one such step taken to help give women a sense of belonging in fields that are more stereotypically dominated by men. Each speaker shared important lessons about their time in data science, and how data science is used in the real world. Then I will summarize my learnings from the readings regarding women in data science, attending the conference, and writing this blog post!\nThere are a few reasons why it is problematic that women are underrepresented in engineering and computing sciences. First off, and potentially most obviously, this is a problem for women. When women cannot work in fields, their needs may be overlooked by their male counterparts. However, this under-representation is not just a problem for women - its a problem for everyone. Research has found that innovation soars with greater diversity in the work force. Further research has shown that low-performing men are frequently hired over high-performing women. This combination of factors leaves a large talent pool untapped, and many breakthroughs on the bench. Increased diversity has also been found to increase productivity.\nMany fields that have found themselves under-representing women historically have made huge gains with respect to representation since the 2000s. Computing represents the unique exception to the rule - while almost 40% of the field was women in 1985, that percent has declined continuously and in 2013, women only made up 18% of the workforce. This percent is reminiscent of the 1970s. One reason that could be behind this is the narrow focus of many engineering jobs; there is rarely a drive to discuss and consider social and ethical responsibilities of ones’ job. As women have a higher preference for attaining a clear social purpose, this leads to imbalance. Other issues include isolation, stereotyping, and challenges with work life balance that are culturally expected of many women like marriage and children. Additionally, women are statistically more likely to be the victim of sexual assault.\nDespite these challenges, progress is being made towards change. One idea is to include components of ethical and community values in both college courses and the work place. This can help women feel as if they are making a contribution to society. Feeling welcome and wanted can improve motivation, perseverance, and commitment to computing, boosting interest, and retention of women in the field. Anti-harassment and anti-assault policies and trainings can feel safe in the workplace. Introducing women to computing and engineering from a young age and exposing students to female and non-binary role models who have successful careers can provide a sense of belonging. One example of that is the Women in Data Science conference hosted annually at Middlebury College.\nThe conference opened with Professor Amy Yuen’s lightning talk. As a political science professor, one of her recent areas of study has been representation in the UN Security council - more specifically whether the representation was equal. She explained how it seemed unlikely; the council consists of only 15 member countries at a time. Moreover, 1/3 of these countries always remain on the council with veto privileges while the other 10 seats are campaigned for by countries based on region. Professor Yuen determined that a successful member would have a high output discovered that wealthy countries were not necessarily more productive when on the council. Rather, sponsorship was a significant influencing factor. Somewhat surprisingly, she also determined that the council has somewhat equal representation among the seats that are campaigned for.\nThe keynote presenter, Professor Sarah Brown, spoke about the ethical implications of involving machine learning in our daily life. As usage of such algorithms increases, so does the need to ensure that they are capable of fairly analyzing data. To frame her talk, she shared three keys, or epiphanies that she has had, and how they apply to data science. Each key related understanding the context of data to its proper usage.\nThe first key was that Professor Brown discovered was that context is necessary to understand primary sources. The following example in particular resonated with me. Her project involved involved diagnosing patients with PTSD. The formerly used method applied a threshold, below which patients were no longer assessed. Professor Brown, after gaining a better understanding of how the data was interpreted, was able to make a minor adjustment which drastically decreased misclassification of patients who had PTSD as not having PTSD. As someone who is deeply interested in the healthcare system, how access can be increased, and the role data science is and will play in it, I found this insightful.\nContinuing on, she shared her discovery that disciplines are communities. This was an essential learning; most people who work with data science are, well, computer scientists, and don’t have the expertise to interpret the data in context. Involving others from the community, and even other communities from relevant fields can lessen bias and bolster information gained from the results.\nThe final key was to meet people where they are. While she was on the board of the National Society for Black Engineers, she discovered a startling breakdown of how information was passed on. The national board received positive feedback from individual chapter heads, the policies still weren’t being implemented. Eventually, they realized that student organizations didn’t have the resources or motivation to make these policies happen. This translated to machine learning when Professor Brown noticed that once an algorithm is accurate, producers will hesitate to make it fair, not sure if it is worth the trade-off in accuracy. Her proposed solution was to indicate whether a model was fair before fitting it, to reduce hesitance to make a model fair.\nThe second lightning talk was shared by Professor Jessica L’Roe highlighted the importance of context in research. She went into detail about how she collected quantitative and qualitative data during her work with deforestation directly from local communities that were impacted. By doing so, she discovered that the tree planting was carried out primarily by foreigners, and they were not planting local species of trees. As a result of the increased interest in tree planting by foreigners, much of the farming land was being purchased, and mothers local to the area had begun prioritizing education over agriculture as a future for their children due to concerns of insufficient land to be farmed. Without gathering data from locals directly impacted, Professor L’Roe never would have discovered the subtle intricacies of her data, and how it was changing life for future generations of locals.\nProfessor Biester (one of our own Middlebury Computer Science Professors!) shared a talk about her research on mental health and social media presence. To collect data, she took on the impressive feat of scraping a massive amount of data from reddit. Then, she searched the data for instances of specific first person declarations of having been diagnosed with depression. The assumption that makes this work is that although a few users may lie, the percentage of users who claim to have been diagnosed and but have not been is negligible compared to those who claim to have been diagnosed and actually have been diagnosed, and vice versa for those who did not claim to be diagnosed. I learned that looking at the data and thinking about what must be accounted for, and what factors are negligible is an important step in cleaning and preparing the data.\nFirst and foremost, writing this blog post forced me to sit with how tall of a mountain we will have to climb to have equal representation, opportunity, and treatment of women working in STEM. I was previously aware of the under-representation of women in these fields, I didn’t quite realize how deep the roots were of this issue, or how difficult it would be to unravel them. From stereotype threat to a sense of belonging, it is challenging to be a successful woman in STEM. However, I was pleased to see the deep thought that went into some steps that have been taken (by, for example, Harvey Mudd College), and even just an increased awareness that this is, indeed an issue (thanks to big tech companies for publishing percentages of women in the workforce). While this mountain might be a tall one, it’s definitely worth climbing. I hope to learn more about steps I can take to reduce all kinds of bias in algorithms that I implement by considering what biases I have beforehand and asking experts for help to reduce them."
  },
  {
    "objectID": "posts/BP7/BP7.html",
    "href": "posts/BP7/BP7.html",
    "title": "Blog Post: Logistic Regression",
    "section": "",
    "text": "First of all, here is a link to my LogisticRegression source code: https://github.com/ShadowedSpace/ShadowedSpace.github.io/blob/main/posts/BP7/LogisticRegression.py\nIn this blog post, I aim to perform several experiments on the above posted logistic regression source code. The first experiment will be vanilla gradient descent, to get a better understanding of whether my source code works, and what it means for it to work. Then, I will implement momentum, and compare the efficiency of the first two experiments. Finally, I will do an experiment to show the danger of overfitting and that it doesn’t generalize well.\n\n%load_ext autoreload\n%autoreload 2\nfrom LogisticRegression import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\n\n# generates data for classification problem\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nw = torch.linspace(-1, 1, X.shape[1])\nw_prev = torch.linspace(-1, 1, X.shape[1])\n\n\n# plots data and boundary decision based on w\n# only works for dimension = 2\n\ndef plot_data_and_boundary(X, y, w, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the data\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    colors = [\"red\", \"blue\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = colors[i], cmap = \"BrBG\", marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n# calculates the accuracy of a model\n\ndef accuracy(y_pred, y_true):\n    correct = (y_pred == y_true).sum()\n    total = y_true.shape[0]\n    print(correct)\n    return correct / total\n\n\n\nThis first experiment is to implement “vanilla gradient descent.” Its main purpose is to ensure that the LogisticRegression and GradientDescentOptimizer classes are properly implemented. The intent is to show that w looks visually correct, and the loss decreases monotonically.\n\n# vanilla gradient descent\n\nLR = LogisticRegression(w)\nopt = GradientDescentOptimizer(LR, w, w_prev)\n\nlosses = []\n\nfor _ in range(100):\n    losses.append(LR.loss(X, y))\n    opt.step(X, y, alpha = 1.5, beta = 0)\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nOk nice! It looks like the losses indeed converge, so that’s exciting to see. It looks like it takes around 70 steps to even out.\n\nplot_data_and_boundary(X, y, LR.w)\n\n\n\n\n\n\n\n\nThat’s a solid decision boundary. Probably pretty close to what my human eye could make out as the best linear decision boundary.\n\n# Predict the labels\ny_pred = LR.predict(X)\n\n# Calculate the accuracy\nacc = accuracy(y_pred, y)\nprint(f'Accuracy: {acc}')\n\ntensor(273)\nAccuracy: 0.9100000262260437\n\n\n\n\n\nThe second experiment is to better learn how momentum impacts logistic regression. With a proper selection of beta, it should converge even faster than vanilla gradient descent.\n\n# momentum\n\nw2 = torch.linspace(-1, 1, X.shape[1])\nw_prev2 = torch.linspace(-1, 1, X.shape[1])\n\nLR2 = LogisticRegression(w)\nopt2 = GradientDescentOptimizer(LR2, w2, w_prev2)\n\nlosses_mom = []\n\nfor _ in range(50):\n    losses_mom.append(LR2.loss(X, y))\n    opt2.step(X, y, alpha = 0.45, beta = 0.9)\n\nplt.plot(losses_mom, label = \"momentum\")\nplt.plot(losses, label = \"vanilla\")\nplt.xlim(0, 50)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nWoah - LR with momentum is totally worth the extra term. This converged after only about 15 steps (in comparison to ~70 without momentum).\n\n# Predict the labels\ny_pred2 = LR2.predict(X)\n\n# Calculate the accuracy\nacc = accuracy(y_pred2, y)\nprint(f'Accuracy: {acc}')\n\ntensor(273)\nAccuracy: 0.9100000262260437\n\n\n\nplot_data_and_boundary(X, y, LR2.w)\n\n\n\n\n\n\n\n\n\n\n\nThe third and final experiment is to look into the effects of overfitting. To do this, I will generate two data sets through exactly the same method, and fit the training set on an absurdly high number of dimensions to see what impact this has on the test set. I would expecet the accuracy to decrease noticeably.\n\n# overfitting\n\nX_train, y_train = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\nX_val, y_val = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\n\nw3 = torch.linspace(-1, 1, X_train.shape[1])\nw_prev3 = torch.linspace(-1, 1, X_train.shape[1])\n\nLR3 = LogisticRegression(w3)\nopt = GradientDescentOptimizer(LR3, w3, w_prev3)\n\nlosses = []\nlosses_val = []\n\nfor _ in range(500):\n    losses.append(LR3.loss(X_train, y_train))\n    losses_val.append(LR3.loss(X_val, y_val))\n    opt.step(X_train, y_train, alpha = 0.15, beta = 0.2)\n\nplt.plot(losses, label = \"training\")\nplt.plot(losses_val, label = \"validation\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nHere, I plot the losses of the train set (blue) and the losses of the validation set (orange) as the logistic regression algorithm steps. As predicted, because of the high number of dimensions, while the losses for the train set converge rather quickly, while the losses of the validation set do not converge.\n\n# Predict the labels\ny_pred_train = LR3.predict(X_train)\n\n# Calculate the accuracy\nacc = accuracy(y_pred_train, y_train)\nprint(f'Accuracy: {acc}')\n\ntensor(75)\nAccuracy: 1.0\n\n\nAn astounding accuracy of 100% on the training test. I sure would hope so with more dimensions than data points.\n\n# Predict the labels\ny_pred_test = LR3.predict(X_test)\n\n# Calculate the accuracy\nacc = accuracy(y_pred_test, y_test)\nprint(f'Accuracy: {acc}')\n\ntensor(67)\nAccuracy: 0.8933333158493042\n\n\nLooks like we (successfully) overfit from the less than satisfactory 92% accuracy on the test set. While more dimensions than necessary can get an impressive result on the training set, it doesn’t generalize so well.\n\n\n\n\nWell, I agree with Phil — that was a pretty cool blog post. I definitely struggled some of the more challenging bits (especially plots and vectorized operations), but looking back at my accomplishment, I am proud of the learning I did. I have a much better understanding of how logistic regression works as a classifier, and how to show this in the form of loss graphs and data visualization. I also learned about the use of a validation set to track loss on a “test” set without touching the test set, which is a pretty neat comment. My first experiment (vanilla gradient descent) showed that the loss monotonically decreases. The momentum experiment showed that logistic regression can be further optimized. Finally, the overfitting experiment showed that too many dimensions can decrease accuracy when generalizing to unknown data."
  },
  {
    "objectID": "posts/BP7/BP7.html#introduction",
    "href": "posts/BP7/BP7.html#introduction",
    "title": "Blog Post: Logistic Regression",
    "section": "",
    "text": "First of all, here is a link to my LogisticRegression source code: https://github.com/ShadowedSpace/ShadowedSpace.github.io/blob/main/posts/BP7/LogisticRegression.py\nIn this blog post, I aim to perform several experiments on the above posted logistic regression source code. The first experiment will be vanilla gradient descent, to get a better understanding of whether my source code works, and what it means for it to work. Then, I will implement momentum, and compare the efficiency of the first two experiments. Finally, I will do an experiment to show the danger of overfitting and that it doesn’t generalize well.\n\n%load_ext autoreload\n%autoreload 2\nfrom LogisticRegression import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\n\n# generates data for classification problem\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nw = torch.linspace(-1, 1, X.shape[1])\nw_prev = torch.linspace(-1, 1, X.shape[1])\n\n\n# plots data and boundary decision based on w\n# only works for dimension = 2\n\ndef plot_data_and_boundary(X, y, w, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Plot the data\n    targets = [0, 1]\n    markers = [\"o\", \",\"]\n    colors = [\"red\", \"blue\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = colors[i], cmap = \"BrBG\", marker = markers[i])\n\n    # Draw the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x_values = torch.linspace(x_min, x_max, 100)\n    y_values = -(w[0]*x_values + w[2])/w[1]\n    ax.plot(x_values, y_values, color='black')\n\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    plt.show()\n\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n# calculates the accuracy of a model\n\ndef accuracy(y_pred, y_true):\n    correct = (y_pred == y_true).sum()\n    total = y_true.shape[0]\n    print(correct)\n    return correct / total\n\n\n\nThis first experiment is to implement “vanilla gradient descent.” Its main purpose is to ensure that the LogisticRegression and GradientDescentOptimizer classes are properly implemented. The intent is to show that w looks visually correct, and the loss decreases monotonically.\n\n# vanilla gradient descent\n\nLR = LogisticRegression(w)\nopt = GradientDescentOptimizer(LR, w, w_prev)\n\nlosses = []\n\nfor _ in range(100):\n    losses.append(LR.loss(X, y))\n    opt.step(X, y, alpha = 1.5, beta = 0)\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nOk nice! It looks like the losses indeed converge, so that’s exciting to see. It looks like it takes around 70 steps to even out.\n\nplot_data_and_boundary(X, y, LR.w)\n\n\n\n\n\n\n\n\nThat’s a solid decision boundary. Probably pretty close to what my human eye could make out as the best linear decision boundary.\n\n# Predict the labels\ny_pred = LR.predict(X)\n\n# Calculate the accuracy\nacc = accuracy(y_pred, y)\nprint(f'Accuracy: {acc}')\n\ntensor(273)\nAccuracy: 0.9100000262260437\n\n\n\n\n\nThe second experiment is to better learn how momentum impacts logistic regression. With a proper selection of beta, it should converge even faster than vanilla gradient descent.\n\n# momentum\n\nw2 = torch.linspace(-1, 1, X.shape[1])\nw_prev2 = torch.linspace(-1, 1, X.shape[1])\n\nLR2 = LogisticRegression(w)\nopt2 = GradientDescentOptimizer(LR2, w2, w_prev2)\n\nlosses_mom = []\n\nfor _ in range(50):\n    losses_mom.append(LR2.loss(X, y))\n    opt2.step(X, y, alpha = 0.45, beta = 0.9)\n\nplt.plot(losses_mom, label = \"momentum\")\nplt.plot(losses, label = \"vanilla\")\nplt.xlim(0, 50)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nWoah - LR with momentum is totally worth the extra term. This converged after only about 15 steps (in comparison to ~70 without momentum).\n\n# Predict the labels\ny_pred2 = LR2.predict(X)\n\n# Calculate the accuracy\nacc = accuracy(y_pred2, y)\nprint(f'Accuracy: {acc}')\n\ntensor(273)\nAccuracy: 0.9100000262260437\n\n\n\nplot_data_and_boundary(X, y, LR2.w)\n\n\n\n\n\n\n\n\n\n\n\nThe third and final experiment is to look into the effects of overfitting. To do this, I will generate two data sets through exactly the same method, and fit the training set on an absurdly high number of dimensions to see what impact this has on the test set. I would expecet the accuracy to decrease noticeably.\n\n# overfitting\n\nX_train, y_train = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\nX_val, y_val = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 75, noise = 0.5, p_dims = 100)\n\nw3 = torch.linspace(-1, 1, X_train.shape[1])\nw_prev3 = torch.linspace(-1, 1, X_train.shape[1])\n\nLR3 = LogisticRegression(w3)\nopt = GradientDescentOptimizer(LR3, w3, w_prev3)\n\nlosses = []\nlosses_val = []\n\nfor _ in range(500):\n    losses.append(LR3.loss(X_train, y_train))\n    losses_val.append(LR3.loss(X_val, y_val))\n    opt.step(X_train, y_train, alpha = 0.15, beta = 0.2)\n\nplt.plot(losses, label = \"training\")\nplt.plot(losses_val, label = \"validation\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nHere, I plot the losses of the train set (blue) and the losses of the validation set (orange) as the logistic regression algorithm steps. As predicted, because of the high number of dimensions, while the losses for the train set converge rather quickly, while the losses of the validation set do not converge.\n\n# Predict the labels\ny_pred_train = LR3.predict(X_train)\n\n# Calculate the accuracy\nacc = accuracy(y_pred_train, y_train)\nprint(f'Accuracy: {acc}')\n\ntensor(75)\nAccuracy: 1.0\n\n\nAn astounding accuracy of 100% on the training test. I sure would hope so with more dimensions than data points.\n\n# Predict the labels\ny_pred_test = LR3.predict(X_test)\n\n# Calculate the accuracy\nacc = accuracy(y_pred_test, y_test)\nprint(f'Accuracy: {acc}')\n\ntensor(67)\nAccuracy: 0.8933333158493042\n\n\nLooks like we (successfully) overfit from the less than satisfactory 92% accuracy on the test set. While more dimensions than necessary can get an impressive result on the training set, it doesn’t generalize so well."
  },
  {
    "objectID": "posts/BP7/BP7.html#conclusion",
    "href": "posts/BP7/BP7.html#conclusion",
    "title": "Blog Post: Logistic Regression",
    "section": "",
    "text": "Well, I agree with Phil — that was a pretty cool blog post. I definitely struggled some of the more challenging bits (especially plots and vectorized operations), but looking back at my accomplishment, I am proud of the learning I did. I have a much better understanding of how logistic regression works as a classifier, and how to show this in the form of loss graphs and data visualization. I also learned about the use of a validation set to track loss on a “test” set without touching the test set, which is a pretty neat comment. My first experiment (vanilla gradient descent) showed that the loss monotonically decreases. The momentum experiment showed that logistic regression can be further optimized. Finally, the overfitting experiment showed that too many dimensions can decrease accuracy when generalizing to unknown data."
  }
]