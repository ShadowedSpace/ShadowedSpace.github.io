[
  {
    "objectID": "posts/BP4/BP4.html",
    "href": "posts/BP4/BP4.html",
    "title": "Blog Post: Women in Data Science",
    "section": "",
    "text": "Blog Post: Women in Data Science\nIn this blog post, I will first seek to better understand the under-representation of women in engineering and the computing sciences, the harms that this causes for everyone, why this happens, and some steps towards improving it. Then, I will discuss the Women in Data Science Conference at Middlebury, one such step taken to help give women a sense of belonging in fields that are more stereotypically dominated by men. Each speaker shared important lessons about their time in data science, and how data science is used in the real world. Then I will summarize my learnings from the readings regarding women in data science, attending the conference, and writing this blog post!\nThere are a few reasons why it is problematic that women are underrepresented in engineering and computing sciences. First off, and potentially most obviously, this is a problem for women. When women cannot work in fields, their needs may be overlooked by their male counterparts. However, this under-representation is not just a problem for women - its a problem for everyone. Research has found that innovation soars with greater diversity in the work force. Further research has shown that low-performing men are frequently hired over high-performing women. This combination of factors leaves a large talent pool untapped, and many breakthroughs on the bench. Increased diversity has also been found to increase productivity.\nMany fields that have found themselves under-representing women historically have made huge gains with respect to representation since the 2000s. Computing represents the unique exception to the rule - while almost 40% of the field was women in 1985, that percent has declined continuously and in 2013, women only made up 18% of the workforce. This percent is reminiscent of the 1970s. One reason that could be behind this is the narrow focus of many engineering jobs; there is rarely a drive to discuss and consider social and ethical responsibilities of ones’ job. As women have a higher preference for attaining a clear social purpose, this leads to imbalance. Other issues include isolation, stereotyping, and challenges with work life balance that are culturally expected of many women like marriage and children. Additionally, women are statistically more likely to be the victim of sexual assault.\nDespite these challenges, progress is being made towards change. One idea is to include components of ethical and community values in both college courses and the work place. This can help women feel as if they are making a contribution to society. Feeling welcome and wanted can improve motivation, perseverance, and commitment to computing, boosting interest, and retention of women in the field. Anti-harassment and anti-assault policies and trainings can feel safe in the workplace. Introducing women to computing and engineering from a young age and exposing students to female and non-binary role models who have successful careers can provide a sense of belonging. One example of that is the Women in Data Science conference hosted annually at Middlebury College.\nThe conference opened with Professor Amy Yuen’s lightning talk. As a political science professor, one of her recent areas of study has been representation in the UN Security council - more specifically whether the representation was equal. She explained how it seemed unlikely; the council consists of only 15 member countries at a time. Moreover, 1/3 of these countries always remain on the council with veto privileges while the other 10 seats are campaigned for by countries based on region. Professor Yuen determined that a successful member would have a high output discovered that wealthy countries were not necessarily more productive when on the council. Rather, sponsorship was a significant influencing factor. Somewhat surprisingly, she also determined that the council has somewhat equal representation among the seats that are campaigned for.\nThe keynote presenter, Professor Sarah Brown, spoke about the ethical implications of involving machine learning in our daily life. As usage of such algorithms increases, so does the need to ensure that they are capable of fairly analyzing data. To frame her talk, she shared three keys, or epiphanies that she has had, and how they apply to data science. Each key related understanding the context of data to its proper usage.\nThe first key was that Professor Brown discovered was that context is necessary to understand primary sources. The following example in particular resonated with me. Her project involved involved diagnosing patients with PTSD. The formerly used method applied a threshold, below which patients were no longer assessed. Professor Brown, after gaining a better understanding of how the data was interpreted, was able to make a minor adjustment which drastically decreased misclassification of patients who had PTSD as not having PTSD. As someone who is deeply interested in the healthcare system, how access can be increased, and the role data science is and will play in it, I found this insightful.\nContinuing on, she shared her discovery that disciplines are communities. This was an essential learning; most people who work with data science are, well, computer scientists, and don’t have the expertise to interpret the data in context. Involving others from the community, and even other communities from relevant fields can lessen bias and bolster information gained from the results.\nThe final key was to meet people where they are. While she was on the board of the National Society for Black Engineers, she discovered a startling breakdown of how information was passed on. The national board received positive feedback from individual chapter heads, the policies still weren’t being implemented. Eventually, they realized that student organizations didn’t have the resources or motivation to make these policies happen. This translated to machine learning when Professor Brown noticed that once an algorithm is accurate, producers will hesitate to make it fair, not sure if it is worth the trade-off in accuracy. Her proposed solution was to indicate whether a model was fair before fitting it, to reduce hesitance to make a model fair.\nThe second lightning talk was shared by Professor Jessica L’Roe highlighted the importance of context in research. She went into detail about how she collected quantitative and qualitative data during her work with deforestation directly from local communities that were impacted. By doing so, she discovered that the tree planting was carried out primarily by foreigners, and they were not planting local species of trees. As a result of the increased interest in tree planting by foreigners, much of the farming land was being purchased, and mothers local to the area had begun prioritizing education over agriculture as a future for their children due to concerns of insufficient land to be farmed. Without gathering data from locals directly impacted, Professor L’Roe never would have discovered the subtle intricacies of her data, and how it was changing life for future generations of locals.\nProfessor Biester (one of our own Middlebury Computer Science Professors!) shared a talk about her research on mental health and social media presence. To collect data, she took on the impressive feat of scraping a massive amount of data from reddit. Then, she searched the data for instances of specific first person declarations of having been diagnosed with depression. The assumption that makes this work is that although a few users may lie, the percentage of users who claim to have been diagnosed and but have not been is negligible compared to those who claim to have been diagnosed and actually have been diagnosed, and vice versa for those who did not claim to be diagnosed. I learned that looking at the data and thinking about what must be accounted for, and what factors are negligible is an important step in cleaning and preparing the data.\nFirst and foremost, writing this blog post forced me to sit with how tall of a mountain we will have to climb to have equal representation, opportunity, and treatment of women working in STEM. I was previously aware of the under-representation of women in these fields, I didn’t quite realize how deep the roots were of this issue, or how difficult it would be to unravel them. From stereotype threat to a sense of belonging, it is challenging to be a successful woman in STEM. However, I was pleased to see the deep thought that went into some steps that have been taken (by, for example, Harvey Mudd College), and even just an increased awareness that this is, indeed an issue (thanks to big tech companies for publishing percentages of women in the workforce). While this mountain might be a tall one, it’s definitely worth climbing. I hope to learn more about steps I can take to reduce all kinds of bias in algorithms that I implement by considering what biases I have beforehand and asking experts for help to reduce them."
  },
  {
    "objectID": "posts/BP2/BP2.html",
    "href": "posts/BP2/BP2.html",
    "title": "Blog Post: ‘Optimal’ Decision Making",
    "section": "",
    "text": "Once again, to begin, the training data must be accessed.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nHere’s a look at the data:\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\nTime to clean the data and take another look. I won’t one hot encode the columns yet, because I’m going to use some of them to explore the data set.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train['loan_status'])\n\ndef prepare_data(df):\n    df = df.drop(['loan_grade'], axis=1)\n    df = df.dropna()\n    y = le.fit_transform(df['loan_status'])\n    df = df.drop(['loan_status'], axis = 1)\n\n    return df, y\n\nX_train, y_train = prepare_data(df_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\n11750\n13.47\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\n10000\n7.51\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\n1325\n12.87\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\n15000\n9.63\n0.28\nN\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\n5500\n14.91\n0.25\nN\n2"
  },
  {
    "objectID": "posts/BP2/BP2.html#exploring-the-data",
    "href": "posts/BP2/BP2.html#exploring-the-data",
    "title": "Blog Post: ‘Optimal’ Decision Making",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nLet’s get familiar with some of the attributes and decide which ones might be helpful to predict whether someone defaulted on a loan or not.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nx = X_train['person_home_ownership']\ny = X_train['loan_percent_income']\n\nax = sns.barplot(data=X_train, x = 'person_home_ownership', y = 'loan_percent_income', hue = y_train.astype(str))\n\nh,l = ax.get_legend_handles_labels()\nl = [\"Repaid Loan\", \"Defaulted\"]\nax.legend(h, l)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nx = X_train['loan_percent_income']\ny = X_train['loan_int_rate']\n\nax = sns.scatterplot(data=X_train, x = 'loan_percent_income', y = 'loan_int_rate', hue = y_train.astype(str))\n\nh,l = ax.get_legend_handles_labels()\nl = [\"Repaid Loan\", \"Defaulted\"]\nax.legend(h, l)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nX_train.groupby([y_train, 'cb_person_default_on_file'])[['person_emp_length', 'cb_person_cred_hist_length']].mean()\n\n\n\n\n\n\n\n\n\nperson_emp_length\ncb_person_cred_hist_length\n\n\n\ncb_person_default_on_file\n\n\n\n\n\n\n0\nN\n4.987118\n5.809296\n\n\nY\n4.782471\n5.928938\n\n\n1\nN\n4.161868\n5.603114\n\n\nY\n4.166886\n5.735217\n\n\n\n\n\n\n\n\nBuilding a Model\nNow that I’m ready to start modeling, I’ll go ahead an one hot encode any qualitative columns.\n\nX_train = pd.get_dummies(X_train)\nX_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\nI’m going to try recursive feature elimination to assign weights to different features in the training data and select the best ones.\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nestimator = LogisticRegression()\n\ncols = RFE(estimator, n_features_to_select=3, step=1).fit(X_train, y_train)\ncols_rfe = cols.get_feature_names_out()\ncols_rfe\n\narray(['person_income', 'loan_amnt', 'loan_int_rate'], dtype=object)\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\nlr_rfe = LogisticRegression().fit(X_train[cols_rfe], y_train)\n\ncv_scores_rfe = cross_val_score(lr_rfe, X_train[cols_rfe], y_train, cv=5).mean()\ncv_scores_rfe\n\n0.8117172718507574\n\n\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\n\ncols = SelectKBest(mutual_info_classif, k = 3)\ncols.fit_transform(X_train, y_train)\ncols_skb = cols.get_feature_names_out()\ncols_skb\n\narray(['person_income', 'loan_int_rate', 'loan_percent_income'],\n      dtype=object)\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\nlr_skb = LogisticRegression().fit(X_train[cols_skb], y_train)\n\ncv_scores_skb = cross_val_score(lr_skb, X_train[cols_skb], y_train, cv=5).mean()\ncv_scores_skb\n\n0.8021563455835603\n\n\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.0001, penalty=\"l1\", dual=False).fit(X_train, y_train)\nmodel = SelectFromModel(lsvc, prefit=True).fit(X_train, y_train)\nx_new = model.transform(X_train)\ncols_svc = model.get_feature_names_out()\ncols_svc\n\narray(['person_age', 'loan_amnt', 'loan_int_rate'], dtype=object)\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\nlr_svc = LogisticRegression().fit(X_train[cols_svc], y_train)\n\ncv_scores_svc = cross_val_score(lr_svc, X_train[cols_svc], y_train, cv=5).mean()\ncv_scores_svc\n\n0.7949095723125648\n\n\nLooks like RFE worked the best to select features so I’ll stick with person_income, loan_int_rate, and loan_percent_income as the attributes for the remainder of the project.\n\nw = lr_rfe.coef_.T\nw\n\narray([[-4.05735976e-05],\n       [ 1.06558819e-04],\n       [ 9.49045880e-08]])\n\n\n\ndef calc_score(X, w):\n    return X@w\n\n\nX_train['score'] = calc_score(X_train[cols_rfe], w)\nX_train['score'].head(20)\n\n1    -2.724145\n2    -0.435472\n3    -0.913722\n4    -0.552180\n6    -0.294372\n7    -1.078671\n8    -1.328416\n9    -0.866993\n10   -0.086189\n11   -0.080296\n12   -0.207142\n13   -4.709531\n14   -1.492597\n15   -0.059797\n16   -2.525174\n17   -2.025183\n18   -1.236856\n19   -2.458977\n20   -0.508184\n21   -1.859404\nName: score, dtype: float64\n\n\n\nimport numpy as np\n\nX_train['profit'] = X_train[\"loan_amnt\"]*(1 + 0.25*X_train[\"loan_int_rate\"] * 0.01)**10 - X_train[\"loan_amnt\"]\nX_train['cost'] = X_train[\"loan_amnt\"]*(1 + 0.25*X_train[\"loan_int_rate\"] * 0.01)*3 - 1.7*X_train[\"loan_amnt\"]\n\n# both equal to 1 is 0\n# both equal to 0 is +profit\n# y_pred = 1, y_train = 0 is 0\n# y_pred = 0, y_train = 1 is -cost\n\nfor t in np.linspace(0, 1, 21):\n    y_pred = 1.0*(X_train['score'] &gt; t)\n    print(y_pred)\n    #if score &lt;= t, they get the loan\n    profit = (((y_pred != y_train) * (y_pred == 0) * -X_train['cost']) + ((y_pred == y_train) * (y_pred == 0) * X_train['profit'])).sum()\n    acc = (y_pred == y_train).mean()\n    print(f\"A threshold of {t:.2f} gives an accuracy of {acc:.2f} and an average profit of {profit:.0f}.\")\n\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.00 gives an accuracy of 0.81 and an average profit of -7866664.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.05 gives an accuracy of 0.80 and an average profit of -9650146.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.10 gives an accuracy of 0.80 and an average profit of -11659056.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.15 gives an accuracy of 0.80 and an average profit of -13955409.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.20 gives an accuracy of 0.80 and an average profit of -15162766.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.25 gives an accuracy of 0.79 and an average profit of -16754399.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.30 gives an accuracy of 0.79 and an average profit of -17931599.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.35 gives an accuracy of 0.79 and an average profit of -19022470.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.40 gives an accuracy of 0.79 and an average profit of -19671379.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.45 gives an accuracy of 0.79 and an average profit of -20259539.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.50 gives an accuracy of 0.79 and an average profit of -20751030.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.55 gives an accuracy of 0.79 and an average profit of -21333027.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.60 gives an accuracy of 0.79 and an average profit of -22013949.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.65 gives an accuracy of 0.79 and an average profit of -22568274.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.70 gives an accuracy of 0.79 and an average profit of -22936382.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.75 gives an accuracy of 0.79 and an average profit of -23200133.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.80 gives an accuracy of 0.79 and an average profit of -23639568.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.85 gives an accuracy of 0.79 and an average profit of -23760256.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.90 gives an accuracy of 0.79 and an average profit of -23903334.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 0.95 gives an accuracy of 0.79 and an average profit of -24068266.\n1        0.0\n2        0.0\n3        0.0\n4        0.0\n6        0.0\n        ... \n26059    0.0\n26060    0.0\n26061    0.0\n26062    0.0\n26063    0.0\nName: score, Length: 22907, dtype: float64\nA threshold of 1.00 gives an accuracy of 0.79 and an average profit of -24102760."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Blog Post: Classifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: ‘Optimal’ Decision Making\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Replication Study - Dissecting racial bias an in algorithm used to manage the health of populations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post: Women in Data Science\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Some new text here"
  },
  {
    "objectID": "posts/BP1/BP1.html",
    "href": "posts/BP1/BP1.html",
    "title": "Blog Post: Classifying Palmer Penguins",
    "section": "",
    "text": "Blog Post: Classifying Palmer Penguins\nIn this blog post, I aim to gain familiarity with pandas tools used for machine learning. I will be exploring a data set about penguins, and attempting to classify them by species based on individual quantitative and qualitative attributes. First, I will take a look at the data and clean it so that it is ready for use. Then I will get familiar with the data through graphical representations, in an effort to make the best decision about which features to use in modelling. Using a reproducible process, I will determine which 3 attributes (1 qualitative and 2 quantitative) should be used for the model. Cross-validation on the training data will be tested with several different models to determine the best model before moving on to the test data. Once the best model is determined, it will be run on the test data, with a goal of 100% accuracy of classification. To analyze the work, I will plot the species regions for both test and training data, and look at a confusion matrix before summing up my findings.\nBefore beginning analysis, the training data must be accessed.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nBelow is a sample of what it looks like:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nClearly, this is a LOT of data. Furthermore, not all of it is super useful. For example, the studyName and Sample Number columns aren’t meaningful to this analysis. Some other columns, like Sex and Island need to be reformatted to allow for analysis. Now, I will clean the data.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.fit_transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\n1\n0\n0\n1\n0\n1\n0\n1\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n\n\n\n\n\n\n\nExplore\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = X_train['Flipper Length (mm)']\ny = X_train['Body Mass (g)']\n\nax = sns.scatterplot(data = X_train, x = 'Flipper Length (mm)', y = 'Body Mass (g)', hue = y_train)\n\nh,l = ax.get_legend_handles_labels()\n\nl = [\"Gentoo Penguin\", \"Chinstrap Penguin\", \"Adelie Penguin\"]\n\nax.legend(h, l)\nplt.show()\n\n\n\n\n\n\n\n\nTo be honest, I started out with some quantitative attributes that seemed interesting. Tossing them onto a graph organized by species seems to be a good way to get a feel of whether they could successfully be used as classifiers. While the combination of body mass and flipper length looks like it may be able to identify (2) penguins, it would be extremely difficult to differentiate penguins of species (0) and (1). Thus, I determined I should continue searching for more ideal attributes prior to modeling.\n\nx = X_train['Delta 15 N (o/oo)']\ny = X_train['Delta 13 C (o/oo)']\n\nax = sns.scatterplot(data=X_train, x = 'Delta 15 N (o/oo)', y = 'Delta 13 C (o/oo)', hue = y_train)\n\nh,l = ax.get_legend_handles_labels()\n\nl = [\"Gentoo Penguin\", \"Chinstrap Penguin\", \"Adelie Penguin\"]\n\nax.legend(h, l)\nplt.show()\n\n\n\n\n\n\n\n\nI threw a few new quantitative attributes on a graph to get an idea of how they compared. Once again, looking at the graph, I don’t have super high hopes that these would be successful as classifiers. It does appear that (1) penguins have a higher Delta 13 C (o/oo) than other species. (0) penguins overlapped with (1) penguins heavily for Delta 15 N (o/oo). (0) penguins also overlap heavily for Delta 13 C (o/oo) with (2) penguins, leading to a prediction that classification would be difficult.\n\nX_train.groupby(y_train)[['Island_Biscoe', 'Island_Dream', 'Island_Torgersen']].mean()\n\n\n\n\n\n\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\n\n\n\n\n0\n0.305556\n0.37963\n0.314815\n\n\n1\n0.000000\n1.00000\n0.000000\n\n\n2\n1.000000\n0.00000\n0.000000\n\n\n\n\n\n\n\nHere, I felt like I hit the jackpot. It seems that (1) penguins were only found on Island_Dream, and (2) pengiuns were only found on Island_Biscoe. Although (0) penguins are spread equally among all three islands, this simplifies the problem significantly. However, it is still important to run some tests and be aware of any possible overfitting.\nWith a few interesting graphics, and a summary table, it’s time to start modeling.\n\n\nModeling\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\n\nx_new = SelectKBest(mutual_info_classif, k = 3)\nx_new.fit_transform(X_train, y_train)\nx_new.get_feature_names_out()\n\narray(['Culmen Depth (mm)', 'Flipper Length (mm)', 'Delta 13 C (o/oo)'],\n      dtype=object)\n\n\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.001, penalty=\"l1\", dual=False).fit(X_train, y_train)\nmodel = SelectFromModel(lsvc, prefit=True).fit(X_train, y_train)\nx_new = model.transform(X_train)\nmodel.get_feature_names_out()\n\nc:\\Users\\Zoe Greenwald\\anaconda3\\envs\\ml-0451-2\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\narray(['Culmen Length (mm)', 'Flipper Length (mm)', 'Body Mass (g)'],\n      dtype=object)\n\n\nAfter trying two methods of features selection and getting only quantitative features, I’m going to give the brute force method a shot.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nqual_cols = [\"Island\", \"Clutch Completion\", \"Stage_adult\", \"Sex\"]\nquant_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nbest_score = 0.0\nbest_cols = []\n\nfor qual in qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(quant_cols, 2):\n    cols = qual_cols + list(pair) \n\n    LR = LogisticRegression(solver='lbfgs', max_iter = 10000)\n    LR.fit(X_train[cols], y_train)\n    score = LR.score(X_train[cols], y_train)\n\n    if score &gt; best_score:\n      best_score = score\n      best_cols = cols\n\n# puts the qualitative columns at the end \nbest_cols = best_cols[::-1]\n\nprint(best_cols)\nprint(best_score)\n\n['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Torgersen', 'Island_Dream', 'Island_Biscoe']\n0.99609375\n\n\nSuccess! I may have forced it by looking only at groups of 3 that included 1 qualitative column, but I’ve managed to reproducibly find the columns I want to use for my modelling. Culmen depth, culmen length, and island seem to have a high success rate for classifying the three penguin species.\n\ncv_scores_LR = cross_val_score(LR, X_train[best_cols], y_train, cv=5).mean()\nprint(cv_scores_LR)\n\n0.9883107088989442\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\ndepths = [*range(3, 20)]\ncv_scores_dtc = []\n\nfor val in depths:\n    dtc = DecisionTreeClassifier(criterion=\"gini\", max_depth=val).fit(X_train[best_cols], y_train)\n    cv_scores_dtc.append(cross_val_score(dtc, X_train[best_cols], y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores_dtc)\nbest_depth = depths[np.argmax(cv_scores_dtc)]\nprint(best_score)\nprint(best_depth)\n\n0.9765460030165913\n7\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrees = [*range(10, 100)]\ncv_scores_rf = []\n\nfor val in trees:\n    rf = RandomForestClassifier(n_estimators = val).fit(X_train[best_cols], y_train)\n    cv_scores_rf.append(cross_val_score(rf, X_train[best_cols], y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores_rf)\nbest_depth = trees[np.argmax(cv_scores_rf)]\nprint(best_score)\nprint(best_depth)\n\n0.9843891402714933\n11\n\n\n\n\nTesting\nLooking at the scores of the best fits of all the tested models, it looks like LR pulls ahead by just a fraction of a percent, so I’ll go ahead and try that on the test data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\nLR.fit(X_train[best_cols], y_train)\nLR.score(X_test[best_cols], y_test)\n\n1.0\n\n\nWe did it! With enough modeling and praying, we got 100% accuracy on the test data! Woohoo!!!\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"upper right\", bbox_to_anchor = (2.5, 0.75))\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[best_cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(LR, X_test[best_cols], y_test)\n\n\n\n\n\n\n\n\nTo finish up, lets take a look at the confusion matrix to see what kind of mistakes our model made (even though our model didn’t make any mistakes on the test data).\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[best_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]], dtype=int64)\n\n\nGiven that our model had 100% accuracy on the test set, this was the confusion matrix I was expecting. The only non-zero numbers are along the diagonal, signifying that every penguin was correctly classified as its own species.\nTo close out this first blog post, I will first discuss my results, and then my learnings. I found that I was able to use machine learning on training data to prepare an group of features and model to successfully classify penguins into their respective species group. The optimal features ended up being culmen length, culmen depth, and what island the penguin was found on, and I determined the ideal model to be linear regression after using cross-validation on several different models before applying the algorithm to the test data. I was excited to meet Phil’s request for 100% accuracy on the test data. Some further analysis was done to compare the decision regions and their boundaries between the train and test data sets. I learned a lot about pandas feature selections and models in this assignment, along with the basic flow of using machine learning in a data science project."
  },
  {
    "objectID": "posts/BP3/BP3.html",
    "href": "posts/BP3/BP3.html",
    "title": "Blog Post: Replication Study - Dissecting racial bias an in algorithm used to manage the health of populations",
    "section": "",
    "text": "Blog Post: Replication Study - Dissecting racial bias an in algorithm used to manage the health of populations\nIn this blog post, I seek to replicate the findings of Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” They give into a study of whether medical data used to qualify high-risk patients for additional automatic support from the health care system is biased. I am to reproduce several of their figures showing the discrepancies between black and white patients with regards to who qualifies as “high-risk” thus being considered or automatically entered into the aforementioned program, and costs between black and white patients for the same risk-score as well as number of chronic conditions. Finally, I will perform a linear regression to determine how much less the average black patient incurs in health costs, and discuss why this is problematic, and which principle of fairness is violated.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\ndf_imp = df[[\"risk_score_t\", \"cost_t\", \"race\", \"gagne_sum_t\"]].copy()\ndf_imp.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n\n\n1\n7.677934\n2600.0\nwhite\n3\n\n\n2\n0.407678\n500.0\nwhite\n0\n\n\n3\n0.798369\n1300.0\nwhite\n0\n\n\n4\n17.513165\n1100.0\nwhite\n1\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_imp['risk_percentile'] = round(df['risk_score_t'].rank(pct = True), 2) * 100\n\ngrouped_data = df_imp.groupby(['risk_percentile', 'race'])['gagne_sum_t'].mean()\n\ngrouped_data = pd.DataFrame(grouped_data)\n\nsns.scatterplot(data=grouped_data, x='gagne_sum_t', y='risk_percentile', hue='race')\n\nplt.xlabel('Mean Number of Chronic Illnesses')\nplt.ylabel('Percentile Risk Score')\nplt.title('Mean number of chronic illnesses versus algorithm predicted risk by race')\n\nplt.legend(title='Race')\nplt.show()\n\n\n\n\n\n\n\n\nThis plot begins to unravel some underlying racial bias that could be present in the algorithm. As seen in the figure, the orange dots representing white patients lie on a higher curve of percentile risk score. This suggests that less sick white patients are more likely to be referred to a high-risk program than sicker black patients.\n\nfig, axes = plt.subplots(1, 2, sharey = True)\n\ngrouped_cost_data = pd.DataFrame(df_imp.groupby(['risk_percentile', 'race'])['cost_t'].mean())\ngrouped_illness_data = pd.DataFrame(df_imp.groupby(['gagne_sum_t', 'race'])['cost_t'].mean())\n\nsns.scatterplot(ax = axes[0], data=grouped_cost_data, x='risk_percentile', y='cost_t', hue='race')\nsns.scatterplot(ax = axes[1], data=grouped_illness_data, x='gagne_sum_t', y='cost_t', hue='race')\n\naxes[0].set_ylabel('Total Medical Expenditure')\naxes[0].set_xlabel('Percentile Risk Score')\naxes[1].set_xlabel('Number of Chronic Illnesses')\n\nplt.ylim(0, 80000)\n\n\n\n\n\n\n\n\nFrom this chart, we can see that holding either percentile risk score or number of chronic illnesses constant, black patients have lower costs than white patients. Further to the right on each graph, the amount of data available drops off steeply, which explains the more erratic correlation.\n\ngreater_than_5 = round((df_imp['gagne_sum_t'] &gt;= 5).mean() * 100, 2)\ngreater_than_5\n\n7.0\n\n\nGiven that 93% of the patients in this data set have 5 or less chronic conditions, it seems reasonable to focus on them, at least as a starting point. It is still important to analyze trends for patients with more chronic illnesses, but in the scope of this assignment, it could downplay trends, and without sufficient data, could display incorrect trends.\n\nimport numpy as np\n\ndf_not_0 = df_imp.drop(df_imp[df_imp['cost_t'] == 0].index)\ndf_not_0['log_cost'] = np.log(df_not_0['cost_t'])\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\nrisk_percentile\nlog_cost\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n35.0\n7.090077\n\n\n1\n7.677934\n2600.0\nwhite\n3\n86.0\n7.863267\n\n\n2\n0.407678\n500.0\nwhite\n0\n4.0\n6.214608\n\n\n3\n0.798369\n1300.0\nwhite\n0\n11.0\n7.170120\n\n\n4\n17.513165\n1100.0\nwhite\n1\n98.0\n7.003065\n\n\n\n\n\n\n\n\ndf_not_0 = pd.get_dummies(df_not_0)\ndf_not_0.head()\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_black\nrace_white\n\n\n\n\n0\n1.987430\n1200.0\n0\n35.0\n7.090077\n0\n1\n\n\n1\n7.677934\n2600.0\n3\n86.0\n7.863267\n0\n1\n\n\n2\n0.407678\n500.0\n0\n4.0\n6.214608\n0\n1\n\n\n3\n0.798369\n1300.0\n0\n11.0\n7.170120\n0\n1\n\n\n4\n17.513165\n1100.0\n1\n98.0\n7.003065\n0\n1\n\n\n\n\n\n\n\n\nX_train = df_not_0[['gagne_sum_t', 'race_black']].rename(columns={'race_black' : 'race'})\ny_train = df_not_0[['log_cost']]\nX_train.head(10)\n\n\n\n\n\n\n\n\ngagne_sum_t\nrace\n\n\n\n\n0\n0\n0\n\n\n1\n3\n0\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n1\n0\n\n\n5\n1\n0\n\n\n6\n1\n0\n\n\n7\n0\n0\n\n\n8\n1\n1\n\n\n9\n0\n0\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nLR = LinearRegression()\nLR.fit(X_train, y_train)\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\n\ncv_scores = []\n\nfor i in range(1, 10):\n    X_deg = add_polynomial_features(X_train, i)\n    cv_scores.append(cross_val_score(LR.fit(X_deg, y_train), X_deg, y_train, cv=5).mean())\n\nbest_score = np.max(cv_scores)\nbest_deg = np.argmax(cv_scores)\n\nprint(cv_scores)\nprint(best_score)\nprint(best_deg)\n\n[0.14538846793594323, 0.14538846793594318, 0.14700305368206446, 0.1469209985546572, 0.14733406563857981, 0.1477609417478441, 0.1480739986118528, 0.1481166054857896, 0.14820529981684197]\n0.14820529981684197\n8\n\n\n\nbest = add_polynomial_features(X_train, best_deg)\n\nLR.fit(best, y_train)\nw = LR.coef_\nprint(w)\n\n[[ 3.36049611e-01 -2.66862126e-01  3.36049611e-01 -3.67776150e-01\n   1.30350450e-01 -2.24509569e-02  1.99855281e-03 -8.86256261e-05\n   1.54525291e-06]]\n\n\n\nb_coef = w[0, 1]\nb_coef\n\n-0.26686212567281653\n\n\n\ncost_incurred = np.exp(b_coef)\ncost_incurred\n\n0.7657786454027453\n\n\nSo, it looks like black patients, on average, pay about 3/4 of what white patients pay for health care. This is more or less following the argument of Obermeyer et al. (2019). I note that my percentage shows significantly larger disparity than what was found in the paper, but but show that black patients generate lower costs than white patients.\nIn this blog post, I found that preparing, processing, and cleaning data can take a lot of work to achieve the desired result. From the recreated figure 1, I found that on average, white patients are rated as having a higher risk score than black patients with the same amount of chronic illnesses. This puts black patients at risk because they are less likely to be referred to a high-risk program that can provide additional health care support. In figure 3, it was clear that, particularly for lower numbers of chronic illnesses (for which there was a lot more data), white patients typically spent more money than black patients. This was further emphasized by the analysis done of the performed linear regression, as described above. In my opinion, separation is the model of fairness that most clearly demonstrates the bias in this data set; the finding that sicker black patients don’t qualify as frequently as less sick white patients suggests a higher false negative rate for black patients. This discrepancy in error rates leads to more black patients not receiving additional care, despite being qualified for it, and can lead to them falling through holes in the health care system without necessary support."
  }
]